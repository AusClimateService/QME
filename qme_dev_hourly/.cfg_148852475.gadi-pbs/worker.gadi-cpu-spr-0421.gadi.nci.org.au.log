Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-05 09:11:47,372 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:36175'
2025-09-05 09:11:47,381 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:37885'
2025-09-05 09:11:47,385 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:37507'
2025-09-05 09:11:47,390 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:42637'
2025-09-05 09:11:47,393 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:38439'
2025-09-05 09:11:47,397 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:38383'
2025-09-05 09:11:47,402 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:42023'
2025-09-05 09:11:47,406 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:34033'
2025-09-05 09:11:47,410 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:33533'
2025-09-05 09:11:47,416 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:39467'
2025-09-05 09:11:47,421 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:43849'
2025-09-05 09:11:47,433 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:39775'
2025-09-05 09:11:47,436 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:45105'
2025-09-05 09:11:47,440 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:36511'
2025-09-05 09:11:47,444 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:38965'
2025-09-05 09:11:47,448 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:39987'
2025-09-05 09:11:47,452 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:34385'
2025-09-05 09:11:47,456 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:40747'
2025-09-05 09:11:47,461 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:33519'
2025-09-05 09:11:47,465 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:45345'
2025-09-05 09:11:47,600 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:42491'
2025-09-05 09:11:47,604 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:43261'
2025-09-05 09:11:47,609 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:33825'
2025-09-05 09:11:47,613 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:42873'
2025-09-05 09:11:47,617 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:39787'
2025-09-05 09:11:47,621 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:43771'
2025-09-05 09:11:47,627 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:45535'
2025-09-05 09:11:47,631 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:37051'
2025-09-05 09:11:47,635 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:37715'
2025-09-05 09:11:47,640 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:34435'
2025-09-05 09:11:47,645 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:45445'
2025-09-05 09:11:47,650 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:36125'
2025-09-05 09:11:47,654 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:41427'
2025-09-05 09:11:47,658 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:34205'
2025-09-05 09:11:47,662 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:33531'
2025-09-05 09:11:47,667 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:35419'
2025-09-05 09:11:47,672 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:43745'
2025-09-05 09:11:47,676 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:41301'
2025-09-05 09:11:47,680 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:41381'
2025-09-05 09:11:47,685 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:36153'
2025-09-05 09:11:47,688 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:39851'
2025-09-05 09:11:47,693 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:34877'
2025-09-05 09:11:47,696 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:38121'
2025-09-05 09:11:47,701 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:41587'
2025-09-05 09:11:47,706 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:37213'
2025-09-05 09:11:47,711 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:39983'
2025-09-05 09:11:47,713 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:44273'
2025-09-05 09:11:47,717 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:44237'
2025-09-05 09:11:47,722 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:46365'
2025-09-05 09:11:47,726 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:34453'
2025-09-05 09:11:47,730 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:41499'
2025-09-05 09:11:47,735 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.61:33781'
2025-09-05 09:11:48,563 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:46245
2025-09-05 09:11:48,563 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:46245
2025-09-05 09:11:48,563 - distributed.worker - INFO -          dashboard at:          10.6.101.61:37067
2025-09-05 09:11:48,563 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,563 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,563 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:45735
2025-09-05 09:11:48,563 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:39483
2025-09-05 09:11:48,563 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:48,563 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:45735
2025-09-05 09:11:48,563 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:48,563 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:39483
2025-09-05 09:11:48,563 - distributed.worker - INFO -          dashboard at:          10.6.101.61:39761
2025-09-05 09:11:48,563 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-qbvasimy
2025-09-05 09:11:48,563 - distributed.worker - INFO -          dashboard at:          10.6.101.61:35525
2025-09-05 09:11:48,563 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,563 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,563 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,563 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,563 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:48,563 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,563 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:36715
2025-09-05 09:11:48,563 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:48,563 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:48,563 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:46199
2025-09-05 09:11:48,563 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:36715
2025-09-05 09:11:48,563 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:48,563 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-pbvu7btu
2025-09-05 09:11:48,563 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:46199
2025-09-05 09:11:48,563 - distributed.worker - INFO -          dashboard at:          10.6.101.61:37821
2025-09-05 09:11:48,563 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-v8493wx9
2025-09-05 09:11:48,563 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,563 - distributed.worker - INFO -          dashboard at:          10.6.101.61:41429
2025-09-05 09:11:48,563 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,563 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,563 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,563 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,563 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:48,563 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,564 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:48,564 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:48,564 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:48,564 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-wg3me8h2
2025-09-05 09:11:48,564 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-l9cec3e1
2025-09-05 09:11:48,564 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,564 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,571 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:42805
2025-09-05 09:11:48,571 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:42805
2025-09-05 09:11:48,571 - distributed.worker - INFO -          dashboard at:          10.6.101.61:42333
2025-09-05 09:11:48,571 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,571 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,571 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:48,571 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:48,571 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-4ji026p4
2025-09-05 09:11:48,571 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,594 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:48,594 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,594 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,595 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:48,603 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:48,603 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:39867
2025-09-05 09:11:48,603 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:39867
2025-09-05 09:11:48,603 - distributed.worker - INFO -          dashboard at:          10.6.101.61:45079
2025-09-05 09:11:48,603 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,603 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,603 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:48,603 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:48,603 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-z71o0yg3
2025-09-05 09:11:48,603 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,604 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,604 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,605 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:48,608 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:34493
2025-09-05 09:11:48,608 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:34493
2025-09-05 09:11:48,608 - distributed.worker - INFO -          dashboard at:          10.6.101.61:45527
2025-09-05 09:11:48,608 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,608 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,608 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:48,608 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:48,608 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-6_zgjnci
2025-09-05 09:11:48,608 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,609 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:48,609 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:32973
2025-09-05 09:11:48,609 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:32973
2025-09-05 09:11:48,609 - distributed.worker - INFO -          dashboard at:          10.6.101.61:36211
2025-09-05 09:11:48,609 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,609 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,609 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:48,609 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:48,609 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-qw7hwrhq
2025-09-05 09:11:48,609 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,610 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,610 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,612 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:48,614 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:48,615 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,615 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,617 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:48,618 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:48,619 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,619 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,620 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:48,622 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:48,623 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,623 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,625 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:48,629 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:40929
2025-09-05 09:11:48,629 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:40929
2025-09-05 09:11:48,629 - distributed.worker - INFO -          dashboard at:          10.6.101.61:35603
2025-09-05 09:11:48,629 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,629 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,629 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:48,629 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:48,629 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-7jwxnj1o
2025-09-05 09:11:48,630 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,642 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:48,643 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,643 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,644 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:45909
2025-09-05 09:11:48,644 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:45909
2025-09-05 09:11:48,644 - distributed.worker - INFO -          dashboard at:          10.6.101.61:33447
2025-09-05 09:11:48,644 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,644 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,644 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:48,644 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:48,644 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-j5j6g3dt
2025-09-05 09:11:48,644 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,645 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:48,647 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:48,648 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,648 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,650 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:48,650 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:37255
2025-09-05 09:11:48,650 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:37255
2025-09-05 09:11:48,650 - distributed.worker - INFO -          dashboard at:          10.6.101.61:46593
2025-09-05 09:11:48,650 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,650 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,650 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:48,650 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:48,650 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-yhc1n8ng
2025-09-05 09:11:48,651 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,651 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:48,651 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:35615
2025-09-05 09:11:48,651 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:35615
2025-09-05 09:11:48,651 - distributed.worker - INFO -          dashboard at:          10.6.101.61:38995
2025-09-05 09:11:48,651 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,651 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,651 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:48,651 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:48,651 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-ccpdpze4
2025-09-05 09:11:48,651 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,652 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,652 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,653 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:33069
2025-09-05 09:11:48,653 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:33069
2025-09-05 09:11:48,653 - distributed.worker - INFO -          dashboard at:          10.6.101.61:46649
2025-09-05 09:11:48,653 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,653 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,653 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:48,653 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:48,653 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-8siru140
2025-09-05 09:11:48,653 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,653 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:41101
2025-09-05 09:11:48,654 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:41101
2025-09-05 09:11:48,654 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:48,654 - distributed.worker - INFO -          dashboard at:          10.6.101.61:40649
2025-09-05 09:11:48,654 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,654 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,654 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:48,654 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:48,654 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-l32sg4v5
2025-09-05 09:11:48,654 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,654 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:48,655 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,655 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,658 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:34469
2025-09-05 09:11:48,658 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:34469
2025-09-05 09:11:48,658 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:48,658 - distributed.worker - INFO -          dashboard at:          10.6.101.61:42407
2025-09-05 09:11:48,658 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,658 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,658 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:48,658 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:48,658 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-ne2vpj72
2025-09-05 09:11:48,658 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,659 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:37995
2025-09-05 09:11:48,659 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:37995
2025-09-05 09:11:48,659 - distributed.worker - INFO -          dashboard at:          10.6.101.61:40149
2025-09-05 09:11:48,659 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,659 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,659 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:48,659 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:48,659 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-mz0dtzii
2025-09-05 09:11:48,659 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,666 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:48,666 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,666 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,667 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:48,671 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:39033
2025-09-05 09:11:48,671 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:39033
2025-09-05 09:11:48,671 - distributed.worker - INFO -          dashboard at:          10.6.101.61:45961
2025-09-05 09:11:48,671 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,671 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,671 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:48,671 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:48,671 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-uu9x8_5h
2025-09-05 09:11:48,671 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,676 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:48,677 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,677 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,678 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:48,678 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:48,679 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,679 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,680 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:48,691 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:48,693 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,693 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,693 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:48,694 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,694 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,694 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:48,696 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:48,696 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:48,697 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,697 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,698 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:48,699 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:48,699 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,699 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,701 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:48,706 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:48,707 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,707 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,708 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:48,807 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:43631
2025-09-05 09:11:48,807 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:43631
2025-09-05 09:11:48,808 - distributed.worker - INFO -          dashboard at:          10.6.101.61:36831
2025-09-05 09:11:48,808 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,808 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,808 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:48,808 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:48,808 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-6unxwfh7
2025-09-05 09:11:48,808 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,812 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:38189
2025-09-05 09:11:48,812 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:38189
2025-09-05 09:11:48,812 - distributed.worker - INFO -          dashboard at:          10.6.101.61:44379
2025-09-05 09:11:48,812 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,812 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,812 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:48,812 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:48,812 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-l7r8r3cz
2025-09-05 09:11:48,812 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,815 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:34083
2025-09-05 09:11:48,815 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:34083
2025-09-05 09:11:48,816 - distributed.worker - INFO -          dashboard at:          10.6.101.61:34159
2025-09-05 09:11:48,816 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,816 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,816 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:48,816 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:48,816 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-tr2jafe4
2025-09-05 09:11:48,816 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,834 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:48,835 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,835 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,837 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:48,841 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:48,843 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,843 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,844 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:48,847 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:48,848 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:48,849 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:48,850 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,043 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:34747
2025-09-05 09:11:49,043 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:34747
2025-09-05 09:11:49,043 - distributed.worker - INFO -          dashboard at:          10.6.101.61:43663
2025-09-05 09:11:49,043 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,043 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,043 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,043 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,043 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-atcytd0o
2025-09-05 09:11:49,043 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,044 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:41379
2025-09-05 09:11:49,044 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:41379
2025-09-05 09:11:49,044 - distributed.worker - INFO -          dashboard at:          10.6.101.61:42579
2025-09-05 09:11:49,044 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,044 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,044 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,044 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,044 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-jhguclng
2025-09-05 09:11:49,044 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,044 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:33343
2025-09-05 09:11:49,045 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:33343
2025-09-05 09:11:49,045 - distributed.worker - INFO -          dashboard at:          10.6.101.61:44475
2025-09-05 09:11:49,045 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,045 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,045 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,045 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,045 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-42ud0241
2025-09-05 09:11:49,045 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,048 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:36565
2025-09-05 09:11:49,048 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:36565
2025-09-05 09:11:49,048 - distributed.worker - INFO -          dashboard at:          10.6.101.61:41239
2025-09-05 09:11:49,048 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,048 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,048 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,048 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,048 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-bvfmk97b
2025-09-05 09:11:49,048 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,055 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,055 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,055 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,056 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,058 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,059 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,059 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,059 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,062 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,062 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,062 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,063 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,069 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,070 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,070 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,072 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,085 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:46149
2025-09-05 09:11:49,085 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:46149
2025-09-05 09:11:49,085 - distributed.worker - INFO -          dashboard at:          10.6.101.61:34881
2025-09-05 09:11:49,085 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,085 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,085 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,085 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,085 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-6lhdvfno
2025-09-05 09:11:49,085 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,087 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:37469
2025-09-05 09:11:49,087 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:37469
2025-09-05 09:11:49,087 - distributed.worker - INFO -          dashboard at:          10.6.101.61:45377
2025-09-05 09:11:49,087 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,087 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,087 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,087 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,087 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-ttzvlrwl
2025-09-05 09:11:49,087 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,089 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:34285
2025-09-05 09:11:49,090 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:34285
2025-09-05 09:11:49,090 - distributed.worker - INFO -          dashboard at:          10.6.101.61:34287
2025-09-05 09:11:49,090 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,090 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,090 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,090 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,090 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-lf5w8z__
2025-09-05 09:11:49,090 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,091 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:33283
2025-09-05 09:11:49,091 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:33283
2025-09-05 09:11:49,091 - distributed.worker - INFO -          dashboard at:          10.6.101.61:40151
2025-09-05 09:11:49,091 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,091 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,091 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,091 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,092 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-0fg1wlkc
2025-09-05 09:11:49,092 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,094 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:45331
2025-09-05 09:11:49,094 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:45331
2025-09-05 09:11:49,094 - distributed.worker - INFO -          dashboard at:          10.6.101.61:35475
2025-09-05 09:11:49,094 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,094 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,094 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,094 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,094 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-9wvt3ze5
2025-09-05 09:11:49,094 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,097 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,097 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,097 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,098 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,100 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:42823
2025-09-05 09:11:49,100 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:42823
2025-09-05 09:11:49,100 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:45973
2025-09-05 09:11:49,100 - distributed.worker - INFO -          dashboard at:          10.6.101.61:34763
2025-09-05 09:11:49,100 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:45973
2025-09-05 09:11:49,100 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,100 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,100 - distributed.worker - INFO -          dashboard at:          10.6.101.61:38863
2025-09-05 09:11:49,100 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,100 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,100 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,100 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,100 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,100 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-2h4xgb_5
2025-09-05 09:11:49,100 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,100 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,100 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-lm_untcd
2025-09-05 09:11:49,100 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,103 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:33805
2025-09-05 09:11:49,103 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:33805
2025-09-05 09:11:49,103 - distributed.worker - INFO -          dashboard at:          10.6.101.61:40231
2025-09-05 09:11:49,103 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,103 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,103 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,103 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,103 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-rn8gqh54
2025-09-05 09:11:49,103 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,105 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:46481
2025-09-05 09:11:49,105 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:46481
2025-09-05 09:11:49,105 - distributed.worker - INFO -          dashboard at:          10.6.101.61:40459
2025-09-05 09:11:49,105 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,105 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,106 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,106 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,106 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-y8y7tqv1
2025-09-05 09:11:49,106 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,106 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:46205
2025-09-05 09:11:49,106 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:46205
2025-09-05 09:11:49,106 - distributed.worker - INFO -          dashboard at:          10.6.101.61:33309
2025-09-05 09:11:49,106 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,106 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,106 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,106 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,106 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-n4fe_hon
2025-09-05 09:11:49,106 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,107 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:35213
2025-09-05 09:11:49,107 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:35213
2025-09-05 09:11:49,107 - distributed.worker - INFO -          dashboard at:          10.6.101.61:37719
2025-09-05 09:11:49,107 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,107 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,107 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,107 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,107 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-dg1wt6vr
2025-09-05 09:11:49,107 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,108 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:45389
2025-09-05 09:11:49,108 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:45389
2025-09-05 09:11:49,108 - distributed.worker - INFO -          dashboard at:          10.6.101.61:45157
2025-09-05 09:11:49,108 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,108 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,108 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,108 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,108 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-ij4hxszl
2025-09-05 09:11:49,108 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,108 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,109 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,109 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,111 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:41579
2025-09-05 09:11:49,111 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:41579
2025-09-05 09:11:49,111 - distributed.worker - INFO -          dashboard at:          10.6.101.61:37341
2025-09-05 09:11:49,111 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,111 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,111 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,111 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,111 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,111 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-m7q1gryw
2025-09-05 09:11:49,111 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,112 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,113 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,113 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,115 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,115 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,116 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:46183
2025-09-05 09:11:49,116 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:46183
2025-09-05 09:11:49,116 - distributed.worker - INFO -          dashboard at:          10.6.101.61:46575
2025-09-05 09:11:49,116 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,116 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,116 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,116 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,116 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-ku3vvkuj
2025-09-05 09:11:49,116 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,116 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,116 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,118 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,119 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:34531
2025-09-05 09:11:49,119 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:34531
2025-09-05 09:11:49,119 - distributed.worker - INFO -          dashboard at:          10.6.101.61:33197
2025-09-05 09:11:49,119 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,119 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,119 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,119 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,119 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-zmt9eksy
2025-09-05 09:11:49,119 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,121 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,122 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,122 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,123 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:40915
2025-09-05 09:11:49,123 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:40915
2025-09-05 09:11:49,123 - distributed.worker - INFO -          dashboard at:          10.6.101.61:35221
2025-09-05 09:11:49,123 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,123 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,123 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,123 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,123 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-8j8fbv9p
2025-09-05 09:11:49,123 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,123 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,123 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:45537
2025-09-05 09:11:49,123 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:44465
2025-09-05 09:11:49,123 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:45537
2025-09-05 09:11:49,124 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:44465
2025-09-05 09:11:49,124 - distributed.worker - INFO -          dashboard at:          10.6.101.61:45097
2025-09-05 09:11:49,124 - distributed.worker - INFO -          dashboard at:          10.6.101.61:42007
2025-09-05 09:11:49,124 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,124 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,124 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,124 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,124 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,124 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,124 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,124 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,124 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-3k8u7t23
2025-09-05 09:11:49,124 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-ceu0jdj7
2025-09-05 09:11:49,124 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,124 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,124 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,125 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,125 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,126 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,126 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,126 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,127 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,127 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,127 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,128 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,128 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,129 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,129 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,130 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,130 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,130 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:37251
2025-09-05 09:11:49,130 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:37251
2025-09-05 09:11:49,130 - distributed.worker - INFO -          dashboard at:          10.6.101.61:42183
2025-09-05 09:11:49,130 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,130 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,130 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,131 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,131 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-zdp49g2y
2025-09-05 09:11:49,131 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,131 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,132 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,133 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:33339
2025-09-05 09:11:49,133 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:33339
2025-09-05 09:11:49,133 - distributed.worker - INFO -          dashboard at:          10.6.101.61:39725
2025-09-05 09:11:49,133 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,133 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,133 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,133 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,133 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-elvti5pa
2025-09-05 09:11:49,133 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,133 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,133 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,134 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,135 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,135 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,135 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,137 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,137 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,138 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,138 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,139 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:40651
2025-09-05 09:11:49,139 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:40651
2025-09-05 09:11:49,139 - distributed.worker - INFO -          dashboard at:          10.6.101.61:42941
2025-09-05 09:11:49,139 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,139 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,139 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,139 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,139 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,139 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-4xcx6j9_
2025-09-05 09:11:49,139 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,139 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,140 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,140 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,141 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:41291
2025-09-05 09:11:49,141 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:41291
2025-09-05 09:11:49,141 - distributed.worker - INFO -          dashboard at:          10.6.101.61:38353
2025-09-05 09:11:49,141 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,141 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,141 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,141 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,141 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-h2m5hov5
2025-09-05 09:11:49,141 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,141 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,142 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,142 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:37817
2025-09-05 09:11:49,142 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:37817
2025-09-05 09:11:49,142 - distributed.worker - INFO -          dashboard at:          10.6.101.61:46291
2025-09-05 09:11:49,142 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,142 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,142 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,142 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,142 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-1lbmkw9e
2025-09-05 09:11:49,143 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,143 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,143 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,143 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:34199
2025-09-05 09:11:49,144 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:34199
2025-09-05 09:11:49,143 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,144 - distributed.worker - INFO -          dashboard at:          10.6.101.61:41627
2025-09-05 09:11:49,144 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,144 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,144 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,144 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,144 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-bythvluo
2025-09-05 09:11:49,144 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,144 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,144 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,144 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,146 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,149 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:38853
2025-09-05 09:11:49,149 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:38853
2025-09-05 09:11:49,149 - distributed.worker - INFO -          dashboard at:          10.6.101.61:39225
2025-09-05 09:11:49,149 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,149 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,149 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,149 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,149 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-25x1wa58
2025-09-05 09:11:49,150 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,150 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:43737
2025-09-05 09:11:49,150 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:43737
2025-09-05 09:11:49,150 - distributed.worker - INFO -          dashboard at:          10.6.101.61:33649
2025-09-05 09:11:49,150 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,150 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,150 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,150 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,150 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-k1b7auhr
2025-09-05 09:11:49,150 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,150 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,151 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,151 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,152 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,153 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.61:38533
2025-09-05 09:11:49,153 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.61:38533
2025-09-05 09:11:49,153 - distributed.worker - INFO -          dashboard at:          10.6.101.61:40241
2025-09-05 09:11:49,153 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,153 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,153 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,153 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,153 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-x93ax6rb
2025-09-05 09:11:49,153 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,153 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,154 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,154 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,155 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,156 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,156 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,156 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,158 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,158 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,159 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,159 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,161 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,161 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,162 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,162 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,163 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,165 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,166 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,166 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,167 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,167 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,168 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,168 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,168 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,169 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,169 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,169 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,170 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,170 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,171 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,171 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,172 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,172 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,173 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,173 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,173 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,174 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,174 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,174 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,175 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,177 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,178 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,178 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,180 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:12:06,693 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,693 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,693 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,693 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,693 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,693 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,693 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,694 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,694 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,694 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,695 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,695 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,695 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,695 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,695 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,695 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,696 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,696 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,695 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,696 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,696 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,696 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,696 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,696 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,696 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,696 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,696 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,696 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,696 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,696 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,697 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,697 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,697 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,696 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,697 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,697 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,697 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,697 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,697 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,697 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,697 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,697 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,697 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,697 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,697 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,698 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,698 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,698 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,698 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,696 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,698 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,698 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,698 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,698 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,698 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,698 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,698 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,699 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,699 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,699 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,697 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,699 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,697 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,697 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,699 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,699 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,700 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,697 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,700 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,700 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,700 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,698 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,700 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,698 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,700 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,701 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,701 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,701 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,699 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,701 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,699 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,700 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,700 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,702 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,702 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,702 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,702 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,705 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,706 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,707 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,707 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,708 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,708 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,708 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,708 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,709 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,709 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,710 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,711 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,709 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,713 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,713 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,713 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,715 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:09,495 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,495 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,495 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,495 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,495 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,495 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,495 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,495 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,495 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,496 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,496 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,496 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,496 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,496 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,496 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,496 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,496 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,497 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,496 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,497 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,497 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,497 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,497 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,497 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,497 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,497 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,497 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,497 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,497 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,497 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,497 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,497 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,497 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,498 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,498 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,498 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,497 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,498 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,498 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,497 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,498 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,498 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,498 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,498 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,498 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,498 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,498 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,498 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,498 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,498 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,498 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,498 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,498 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,498 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,499 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,498 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,499 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,499 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,499 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,499 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,499 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,499 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,499 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,499 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,500 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,500 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,499 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,499 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,500 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,500 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,500 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,499 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,500 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,500 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,500 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,500 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,500 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,500 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,501 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,501 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,501 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,501 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,501 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,501 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,501 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,501 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,501 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,502 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,502 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,500 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,502 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,502 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,502 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,500 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,502 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,502 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,502 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,502 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,502 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,501 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,503 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,504 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,505 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,507 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,879 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,879 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,880 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,880 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,880 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,880 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,880 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,880 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,880 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,880 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,880 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,881 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,881 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,881 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,881 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,881 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,881 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,881 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,881 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,882 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,882 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,882 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,882 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,882 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,882 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,882 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,883 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,883 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,883 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,883 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,883 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,883 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,883 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,883 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,883 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,883 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,883 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,884 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,883 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,884 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,884 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,884 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,883 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,884 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,884 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,884 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,884 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,884 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,884 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,884 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,884 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,884 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,884 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,885 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,885 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,885 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,885 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,885 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,885 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,885 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,885 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,885 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,885 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,885 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,885 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,885 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,885 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,885 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,885 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,885 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,886 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,886 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,886 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,886 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,886 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,886 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,886 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,886 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,886 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,886 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,886 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,887 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,887 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,887 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,887 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,887 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,887 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,887 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,887 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,887 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,888 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,888 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,888 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,888 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,888 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,889 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,889 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,889 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,889 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,889 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,889 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,889 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,889 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,889 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:10,337 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,337 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,337 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,338 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,338 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,338 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,338 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,338 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,338 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,338 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,338 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,339 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,339 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,339 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,339 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,339 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,339 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,339 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,339 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,340 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,340 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,340 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,340 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,340 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,340 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,340 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,340 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,340 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,340 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,340 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,340 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,340 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,340 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,340 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,341 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,340 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,340 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,340 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,340 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,341 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,341 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,341 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,341 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,341 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,341 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,341 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,341 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,341 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,341 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,341 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,342 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,341 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,342 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,342 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,342 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,342 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,342 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,342 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,342 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,342 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,342 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,342 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,342 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,342 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,342 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,342 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,342 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,342 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,342 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,342 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,342 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,343 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,343 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,343 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,343 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,343 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,343 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,343 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,343 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,343 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,343 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,343 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,344 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,344 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,344 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,344 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,344 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,344 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,345 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,345 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,345 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,345 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,345 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,345 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,345 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,345 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,346 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,346 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,346 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,346 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,346 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,345 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,346 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,351 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:16:17,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,501 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,501 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,501 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,501 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,501 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,511 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,520 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:48,434 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:48,692 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:50,623 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:50,624 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:50,837 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:51,166 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:51,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,102 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:53,654 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:53,753 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:53,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:53,895 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:53,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:53,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:53,942 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:53,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:53,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:54,755 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:55,040 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:55,323 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:55,821 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:56,447 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:56,543 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:56,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:56,899 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:56,899 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:56,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:57,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:57,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:57,522 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:57,866 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:57,868 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,207 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,665 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,771 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,772 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,774 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,777 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:59,005 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:59,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:01,460 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:03,467 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:09,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:09,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:10,376 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:10,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:12,066 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:12,071 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:12,082 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:12,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:12,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:14,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:14,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:14,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:14,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:14,460 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:14,855 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:14,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:14,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,761 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,698 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,703 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,924 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,982 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,225 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,695 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:19,234 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:19,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:19,599 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:20,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:20,757 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:21,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:22,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:22,823 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:23,723 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:23,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:23,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:25,629 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:29,093 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:29,403 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:30,179 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:30,179 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:30,181 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:30,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:31,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:32,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:32,273 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:32,527 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:32,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:32,530 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:32,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:32,708 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:32,709 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:32,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,426 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,293 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,296 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,573 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,900 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:37,253 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:37,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:37,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:37,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,507 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,514 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,661 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:39,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:39,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:39,494 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:39,496 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:39,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:41,610 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:41,611 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:42,305 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:44,618 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:44,622 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:44,622 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:50,555 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:50,556 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:50,558 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:50,560 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:50,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:50,649 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:51,159 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:51,510 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,554 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,628 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,734 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,948 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,460 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,105 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,361 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,363 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,728 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:55,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:58,507 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:58,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:03,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,296 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,297 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,005 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,389 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,394 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,396 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,528 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,588 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,588 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,592 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,598 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,745 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,916 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,946 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,950 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,080 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,534 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,690 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,785 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,854 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,866 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,929 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:18,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:18,034 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:18,132 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:18,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:18,246 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:18,247 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:18,282 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:18,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:18,287 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:18,287 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:18,288 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,454 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,569 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,899 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,902 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,871 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,885 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,954 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,265 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,317 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,317 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,703 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,365 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,022 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,139 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,615 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,617 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,623 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,981 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:39,580 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:42,288 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:42,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:43,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:43,574 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,668 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,398 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,399 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,401 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,402 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,194 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,195 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,806 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,815 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,025 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,524 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,525 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,885 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,172 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:49,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:50,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:50,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:50,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:50,967 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:53,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:53,907 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:53,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:54,766 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:08,846 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,120 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,345 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,357 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,196 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,047 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,389 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,753 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:12,151 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:12,562 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:12,963 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:12,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:12,967 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,296 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,414 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,415 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,415 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,415 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,468 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:15,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:15,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:16,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:16,562 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:16,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:17,440 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:17,444 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:17,444 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:17,449 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:18,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:20,812 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:25,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:25,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:26,742 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:33,786 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:33,787 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:34,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:34,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:35,312 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:35,675 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:35,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:36,072 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:36,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:36,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:37,028 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:37,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:37,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:37,904 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,127 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,520 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,520 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,524 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,627 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,737 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,063 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,085 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,090 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,329 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,849 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,321 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,847 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,869 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,494 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,742 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,865 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,868 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,868 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,873 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,908 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,922 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,952 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,958 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:45,137 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:47,466 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:47,943 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,354 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,742 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,749 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,750 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,783 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,096 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,148 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,357 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,718 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,902 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,826 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:52,307 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:52,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:54,788 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:54,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:55,533 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:55,534 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:57,651 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:57,652 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:57,652 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:57,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:57,654 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:57,657 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:03,757 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:04,585 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:04,585 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:04,720 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:05,311 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:05,311 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:05,319 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:05,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:11,269 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,269 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,270 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,271 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,272 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:40915. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,272 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:44465. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,270 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,272 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:41579. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,270 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,270 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,270 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,272 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:45537. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,270 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,270 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,270 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,271 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,272 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:41101. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,270 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,273 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:35615. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,273 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:46199. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,273 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:41291. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,271 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,273 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:34493. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,271 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,273 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:39867. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,270 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,271 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,273 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:42805. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,271 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,273 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:39483. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,271 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,271 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,273 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:32973. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,272 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,272 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,272 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,271 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,274 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:37995. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,274 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:42823. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,271 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,271 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,274 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:45973. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,270 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,274 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:36715. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,272 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,274 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:46183. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,272 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,274 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:33283. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,274 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:45331. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,272 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,274 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:40929. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,272 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,273 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,275 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:45909. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,271 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,275 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:38533. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,270 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,273 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,273 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,273 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,275 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:34531. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,273 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,275 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:45389. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,275 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:33339. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,273 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,273 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,275 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:33805. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,275 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:46245. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,275 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:46481. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,276 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:34285. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,276 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:37251. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,276 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:36565. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,272 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,276 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:35213. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,276 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:41379. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,276 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:39033. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,276 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:46149. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,276 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:33343. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,277 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:34083. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,277 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:37469. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,277 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:34747. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,277 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:46205. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,283 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:39851'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,286 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1098, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,287 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1111, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,287 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,287 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,288 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,288 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,288 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,291 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:41499'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,292 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:33781'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,292 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:36153'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,293 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1889, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,293 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:34385'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,293 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 838, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,293 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.50:33413, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,293 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:39775'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,293 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3259, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,293 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,293 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.61:38853, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,294 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:38439'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,294 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3271, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,294 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,294 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.58:39351, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,294 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2177, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,294 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,294 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.61:37817, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,294 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:33533'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,294 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,294 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,294 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2208, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,294 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,294 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1828, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,294 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:37885'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,294 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1799, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,294 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,295 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,295 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,295 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,295 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:42637'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,295 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,295 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,295 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,295 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1443, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,295 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,295 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,295 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,295 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:38383'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,295 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,295 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,295 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.61:38189, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,295 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,295 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,295 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,296 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,296 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 5246, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,296 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,296 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1353, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,296 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,296 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,296 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3722, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,296 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,296 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.38:35731, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,296 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3802, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,296 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.37:36545, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,296 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,296 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2207, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,296 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,296 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 840, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,296 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3840, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,297 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1610, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,301 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:43745'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,301 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:39467'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,301 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:34453'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,301 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:36511'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,301 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:43261'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,301 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:38121'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,302 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:36125'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,302 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:43849'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,302 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:34205'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,302 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:37507'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,302 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:40747'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,302 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:45445'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,302 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1370, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,302 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:34435'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,302 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2394, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:41381'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3803, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1090, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:41587'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.41:33917, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2195, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3867, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:42873'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.40:44565, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.61:34469, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3637, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.53:37759, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:45535'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.61:43631, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3897, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1499, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.47:37089, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1749, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1363, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2687, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,304 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:37715'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 548, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.37:46225, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2312, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.37:32841, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2990, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.61:40651, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3560, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1384, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,304 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:41427'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.63:42593, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1754, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.66:42545, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3888, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1773, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1094, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1899, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 846, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,305 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:41301'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.64:38415, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2988, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.53:45293, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.61:40651, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2204, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3552, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1912, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:37213'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.54:37473, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.43:33649, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,306 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 5391, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,306 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1969, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,306 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:33825'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,306 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.39:37235, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,306 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2205, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,306 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2209, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,306 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 8746, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,307 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1797, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,306 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:44237'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,307 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:33519'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,307 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1096, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,307 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3513, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,307 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:39983'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,307 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1568, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,308 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2690, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,308 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1893, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,308 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:42023'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,308 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3636, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,308 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:46365'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,309 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3279, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,309 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1250, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,309 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:39987'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,309 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3738, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,309 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.51:38829, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,309 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:39787'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,310 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:37051'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,310 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1971, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,310 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1878, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,310 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2133, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,310 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1891, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,311 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1611, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,311 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1168, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,312 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1243, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,313 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1189, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,313 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.58:42873, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,313 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 8155, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,315 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,315 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 6984, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,315 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,316 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,316 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,316 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,316 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,316 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,317 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:11,319 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,320 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,320 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,320 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,320 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,328 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:11,330 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:11,468 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:13,154 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:13,156 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:38189. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:13,164 - distributed.worker - ERROR - failed during get data with tcp://10.6.101.61:38189 -> tcp://10.6.101.61:46199
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.101.61:38189 remote=tcp://10.6.101.61:39664>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-05 09:20:13,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:13,173 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.61:59486 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:13,184 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:45345'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:13,185 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:13,185 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:13,185 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:13,185 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:13,185 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:13,188 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x151dd9327510>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:13,195 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:13,321 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:13,331 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:13,334 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:13,472 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:13,872 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:33781'. Reason: nanny-close-gracefully
2025-09-05 09:20:13,876 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:40747'. Reason: nanny-close-gracefully
2025-09-05 09:20:13,878 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:33781' closed.
2025-09-05 09:20:13,879 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:40747' closed.
2025-09-05 09:20:13,882 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:38121'. Reason: nanny-close-gracefully
2025-09-05 09:20:13,883 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:38121' closed.
2025-09-05 09:20:13,913 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:41301'. Reason: nanny-close-gracefully
2025-09-05 09:20:13,915 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:41301' closed.
2025-09-05 09:20:13,996 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:14,437 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:14,440 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:14,441 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:14,726 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:14,902 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:14,905 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:38853. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:14,915 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:14,923 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.61:59772 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:14,927 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:35419'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:14,928 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:14,928 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:14,928 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:14,928 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:14,928 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:14,931 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14ed408cac10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:14,937 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,020 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,198 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:15,587 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,604 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:45345'. Reason: nanny-close-gracefully
2025-09-05 09:20:15,609 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:45345' closed.
2025-09-05 09:20:16,000 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:16,250 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,254 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,270 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,269 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:16,271 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:33069. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:16,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:16,285 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.61:59430 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:16,290 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:34033'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:16,291 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2651, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:16,291 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:16,292 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:16,292 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:16,292 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:16,292 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:16,294 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14bc225e9b50>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:16,406 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:39467'. Reason: nanny-close-gracefully
2025-09-05 09:20:16,408 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:39467' closed.
2025-09-05 09:20:16,441 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:16,445 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:16,451 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,489 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:16,492 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:37817. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:16,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:16,513 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.61:59756 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:16,516 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:34877'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:16,517 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:16,517 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:16,517 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:16,517 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:16,518 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:16,519 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14f1636b5990>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:16,524 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,731 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:16,819 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:38383'. Reason: nanny-close-gracefully
2025-09-05 09:20:16,821 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:38383' closed.
2025-09-05 09:20:16,831 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:42873'. Reason: nanny-close-gracefully
2025-09-05 09:20:16,833 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:42637'. Reason: nanny-close-gracefully
2025-09-05 09:20:16,834 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:42873' closed.
2025-09-05 09:20:16,834 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:42637' closed.
2025-09-05 09:20:16,863 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:16,865 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:45735. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:16,864 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:16,866 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:40651. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:16,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:16,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:16,881 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.61:59738 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:16,885 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:43771'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:16,886 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1755, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:16,885 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.61:59332 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:16,887 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:16,887 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:16,887 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:16,887 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:16,887 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:16,889 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:36175'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:16,890 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:16,890 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:16,890 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:16,890 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:16,890 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:16,889 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x150af14e1ad0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:16,894 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,893 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x152c24cd3a50>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:16,895 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,898 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,898 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:16,900 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:37255. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:16,910 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:16,916 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.61:59404 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:16,920 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:38965'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:16,921 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:16,921 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:16,921 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:16,922 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:16,922 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:16,924 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14bf79743650>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:16,929 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,940 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,024 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,133 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:39987'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,139 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:39987' closed.
2025-09-05 09:20:17,335 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:35419'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,337 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:35419' closed.
2025-09-05 09:20:17,350 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,421 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:34205'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,423 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:34205' closed.
2025-09-05 09:20:17,591 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,694 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:17,696 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:34469. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:17,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:17,712 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.61:59446 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:17,716 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:45105'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:17,717 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:17,717 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:17,717 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:17,717 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:17,717 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:17,720 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x15274c6453d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:17,725 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,837 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,844 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:17,845 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:43631. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:17,850 - distributed.worker - ERROR - failed during get data with tcp://10.6.101.61:43631 -> tcp://10.6.101.44:41641
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.101.61:43631 remote=tcp://10.6.101.44:33060>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-05 09:20:17,855 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:17,860 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.61:59474 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.101.61:59474 remote=tcp://10.6.101.37:8753>: Stream is closed
2025-09-05 09:20:17,863 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:42491'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:17,864 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:17,864 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:17,864 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:17,864 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:17,864 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:17,866 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14ff0bd75250>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:17,871 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,990 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:37885'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,992 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:37885' closed.
2025-09-05 09:20:18,050 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,051 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:18,054 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:34199. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,054 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,064 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:18,070 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.61:59770 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:18,073 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:44273'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,074 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:18,075 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:18,075 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:18,075 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:18,075 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:18,076 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14e76b3bb190>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:18,082 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,254 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,274 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,455 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,527 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,649 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:33533'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,650 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:33533' closed.
2025-09-05 09:20:18,664 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,670 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:34385'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,671 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:41381'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,674 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:34385' closed.
2025-09-05 09:20:18,674 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:41381' closed.
2025-09-05 09:20:18,842 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:39983'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,843 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:39983' closed.
2025-09-05 09:20:18,897 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,899 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,901 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,907 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:34877'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,908 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:34877' closed.
2025-09-05 09:20:18,932 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,042 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,107 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,107 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,200 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,205 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,289 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:38439'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,293 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:36175'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,294 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:38439' closed.
2025-09-05 09:20:19,295 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:36125'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,296 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:36175' closed.
2025-09-05 09:20:19,296 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:36125' closed.
2025-09-05 09:20:19,317 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,320 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:38965'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,321 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:38965' closed.
2025-09-05 09:20:19,355 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,403 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,466 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,726 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,727 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,728 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,728 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,727 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:19,729 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.61:43737. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,739 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:19,745 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.61:59786 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:19,749 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.61:33531'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,750 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2112, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:19,750 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:19,750 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:19,750 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:19,750 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:19,750 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:19,754 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:43771'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,752 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14c7a46559d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:19,755 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:43771' closed.
2025-09-05 09:20:19,794 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,803 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,814 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,822 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,841 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,845 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,873 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,877 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,055 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,058 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,085 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,109 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:45105'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,110 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,110 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:45105' closed.
2025-09-05 09:20:20,188 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,190 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,210 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,224 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:37715'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,225 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:37715' closed.
2025-09-05 09:20:20,242 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,271 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:42491'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,272 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:42491' closed.
2025-09-05 09:20:20,459 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:37507'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,460 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:37507' closed.
2025-09-05 09:20:20,474 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:45535'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,475 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:45535' closed.
2025-09-05 09:20:20,572 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:44273'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,573 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:44273' closed.
2025-09-05 09:20:20,668 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,043 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:34453'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,044 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:34453' closed.
2025-09-05 09:20:21,111 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,112 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,204 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,209 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,321 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,408 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,447 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:37213'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,448 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:37213' closed.
2025-09-05 09:20:21,470 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,550 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:43745'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,551 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:43745' closed.
2025-09-05 09:20:21,581 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:36511'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,582 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:36511' closed.
2025-09-05 09:20:21,598 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:37051'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,599 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:37051' closed.
2025-09-05 09:20:21,705 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:36153'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,708 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:36153' closed.
2025-09-05 09:20:21,717 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:34033'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,717 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:34033' closed.
2025-09-05 09:20:21,730 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,731 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,732 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,797 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,807 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,818 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,826 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,849 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,881 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,892 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:34435'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,893 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:34435' closed.
2025-09-05 09:20:21,901 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:39787'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,902 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:39787' closed.
2025-09-05 09:20:22,113 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:44237'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,114 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:44237' closed.
2025-09-05 09:20:22,114 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,182 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:41427'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,182 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:41427' closed.
2025-09-05 09:20:22,192 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:39775'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,192 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,194 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,200 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:39775' closed.
2025-09-05 09:20:22,211 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:33825'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,212 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:33825' closed.
2025-09-05 09:20:22,214 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,234 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:43261'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,235 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:43261' closed.
2025-09-05 09:20:22,246 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,275 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:33519'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,276 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:33519' closed.
2025-09-05 09:20:22,283 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:33531'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,284 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:33531' closed.
2025-09-05 09:20:22,292 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:43849'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,293 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:43849' closed.
2025-09-05 09:20:22,360 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:41587'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,360 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:41587' closed.
2025-09-05 09:20:22,589 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:41499'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,590 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:41499' closed.
2025-09-05 09:20:22,595 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:45445'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,596 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:45445' closed.
2025-09-05 09:20:22,613 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:46365'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,614 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:46365' closed.
2025-09-05 09:20:22,616 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:42023'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,617 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:42023' closed.
2025-09-05 09:20:22,644 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.61:39851'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,645 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.61:39851' closed.
2025-09-05 09:20:22,648 - distributed.dask_worker - INFO - End worker
