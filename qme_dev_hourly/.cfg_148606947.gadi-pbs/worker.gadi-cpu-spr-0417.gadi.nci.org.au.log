Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-03 10:31:17,934 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:44927'
2025-09-03 10:31:17,940 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:38531'
2025-09-03 10:31:17,947 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:37475'
2025-09-03 10:31:17,955 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:41439'
2025-09-03 10:31:17,958 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:33567'
2025-09-03 10:31:18,016 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:45965'
2025-09-03 10:31:18,019 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:33959'
2025-09-03 10:31:18,023 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:35859'
2025-09-03 10:31:18,027 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:40551'
2025-09-03 10:31:18,032 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:42089'
2025-09-03 10:31:18,036 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:39739'
2025-09-03 10:31:18,040 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:44329'
2025-09-03 10:31:18,044 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:38511'
2025-09-03 10:31:18,048 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:35829'
2025-09-03 10:31:18,053 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:41455'
2025-09-03 10:31:18,058 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:43609'
2025-09-03 10:31:18,062 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:34823'
2025-09-03 10:31:18,067 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:41075'
2025-09-03 10:31:18,072 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:35823'
2025-09-03 10:31:18,076 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:44555'
2025-09-03 10:31:18,080 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:36277'
2025-09-03 10:31:18,085 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:42765'
2025-09-03 10:31:18,090 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:41449'
2025-09-03 10:31:18,094 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:38899'
2025-09-03 10:31:18,099 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:35583'
2025-09-03 10:31:18,103 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:40443'
2025-09-03 10:31:18,108 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:45053'
2025-09-03 10:31:18,112 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:35577'
2025-09-03 10:31:18,117 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:37071'
2025-09-03 10:31:18,123 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:38533'
2025-09-03 10:31:18,127 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:39779'
2025-09-03 10:31:18,132 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:37439'
2025-09-03 10:31:18,136 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:44461'
2025-09-03 10:31:18,141 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:39265'
2025-09-03 10:31:18,145 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:34625'
2025-09-03 10:31:18,149 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:42675'
2025-09-03 10:31:18,154 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:37871'
2025-09-03 10:31:18,158 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:39693'
2025-09-03 10:31:18,162 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:33367'
2025-09-03 10:31:18,166 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:39625'
2025-09-03 10:31:18,171 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:39497'
2025-09-03 10:31:18,175 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:42953'
2025-09-03 10:31:18,257 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:36135'
2025-09-03 10:31:18,261 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:37907'
2025-09-03 10:31:18,267 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:34743'
2025-09-03 10:31:18,273 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:36363'
2025-09-03 10:31:18,277 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:45525'
2025-09-03 10:31:18,282 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:40725'
2025-09-03 10:31:18,288 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:42849'
2025-09-03 10:31:18,292 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:41453'
2025-09-03 10:31:18,297 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:38931'
2025-09-03 10:31:18,302 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:42807'
2025-09-03 10:31:18,307 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:39577'
2025-09-03 10:31:18,311 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:46025'
2025-09-03 10:31:18,316 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:41901'
2025-09-03 10:31:18,319 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:46223'
2025-09-03 10:31:18,322 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:37325'
2025-09-03 10:31:18,327 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:40713'
2025-09-03 10:31:18,332 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:45189'
2025-09-03 10:31:18,336 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:33541'
2025-09-03 10:31:18,341 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:33027'
2025-09-03 10:31:18,346 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:46187'
2025-09-03 10:31:18,348 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:34939'
2025-09-03 10:31:18,349 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:36389'
2025-09-03 10:31:18,353 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:38729'
2025-09-03 10:31:18,354 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:35769'
2025-09-03 10:31:18,360 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:34765'
2025-09-03 10:31:18,363 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:42649'
2025-09-03 10:31:18,368 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:33677'
2025-09-03 10:31:18,373 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:46365'
2025-09-03 10:31:18,377 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:35955'
2025-09-03 10:31:18,380 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:36659'
2025-09-03 10:31:18,384 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:46831'
2025-09-03 10:31:18,388 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:42149'
2025-09-03 10:31:18,394 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:38629'
2025-09-03 10:31:18,396 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:39319'
2025-09-03 10:31:18,401 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:42589'
2025-09-03 10:31:18,419 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:46863'
2025-09-03 10:31:18,422 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:41931'
2025-09-03 10:31:18,426 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:38553'
2025-09-03 10:31:18,428 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:43441'
2025-09-03 10:31:18,434 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:37233'
2025-09-03 10:31:18,437 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:45233'
2025-09-03 10:31:18,445 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:44099'
2025-09-03 10:31:18,452 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:33551'
2025-09-03 10:31:18,456 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:38517'
2025-09-03 10:31:18,461 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:35959'
2025-09-03 10:31:18,464 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:45283'
2025-09-03 10:31:18,469 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:45495'
2025-09-03 10:31:18,474 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:37627'
2025-09-03 10:31:18,479 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:39903'
2025-09-03 10:31:18,486 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:40675'
2025-09-03 10:31:18,490 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:41933'
2025-09-03 10:31:18,497 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:44415'
2025-09-03 10:31:18,502 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:42709'
2025-09-03 10:31:18,507 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:44601'
2025-09-03 10:31:18,512 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:38867'
2025-09-03 10:31:18,517 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:40467'
2025-09-03 10:31:18,522 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:34463'
2025-09-03 10:31:18,525 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:39343'
2025-09-03 10:31:18,536 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:42643'
2025-09-03 10:31:18,539 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:34041'
2025-09-03 10:31:18,544 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:39059'
2025-09-03 10:31:18,551 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.57:43283'
2025-09-03 10:31:20,244 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:42923
2025-09-03 10:31:20,244 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:43191
2025-09-03 10:31:20,244 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:41879
2025-09-03 10:31:20,244 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:34825
2025-09-03 10:31:20,244 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:37883
2025-09-03 10:31:20,244 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:42923
2025-09-03 10:31:20,244 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:38245
2025-09-03 10:31:20,244 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:45589
2025-09-03 10:31:20,244 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:43191
2025-09-03 10:31:20,244 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:36835
2025-09-03 10:31:20,244 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:41879
2025-09-03 10:31:20,244 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:34825
2025-09-03 10:31:20,244 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:41197
2025-09-03 10:31:20,244 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:37883
2025-09-03 10:31:20,244 - distributed.worker - INFO -          dashboard at:          10.6.101.57:44119
2025-09-03 10:31:20,244 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:38245
2025-09-03 10:31:20,245 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:45589
2025-09-03 10:31:20,244 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:33263
2025-09-03 10:31:20,245 - distributed.worker - INFO -          dashboard at:          10.6.101.57:41189
2025-09-03 10:31:20,244 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:40715
2025-09-03 10:31:20,245 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:33263
2025-09-03 10:31:20,245 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:36835
2025-09-03 10:31:20,245 - distributed.worker - INFO -          dashboard at:          10.6.101.57:40875
2025-09-03 10:31:20,245 - distributed.worker - INFO -          dashboard at:          10.6.101.57:37263
2025-09-03 10:31:20,245 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:41197
2025-09-03 10:31:20,245 - distributed.worker - INFO -          dashboard at:          10.6.101.57:33685
2025-09-03 10:31:20,245 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,245 - distributed.worker - INFO -          dashboard at:          10.6.101.57:44271
2025-09-03 10:31:20,245 - distributed.worker - INFO -          dashboard at:          10.6.101.57:42575
2025-09-03 10:31:20,245 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,245 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:40715
2025-09-03 10:31:20,245 - distributed.worker - INFO -          dashboard at:          10.6.101.57:40083
2025-09-03 10:31:20,245 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,245 - distributed.worker - INFO -          dashboard at:          10.6.101.57:43199
2025-09-03 10:31:20,245 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,245 - distributed.worker - INFO -          dashboard at:          10.6.101.57:45303
2025-09-03 10:31:20,245 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,245 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,245 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,245 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,245 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,245 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,245 - distributed.worker - INFO -          dashboard at:          10.6.101.57:35433
2025-09-03 10:31:20,245 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,245 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,245 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,245 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,245 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,245 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,245 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,245 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,245 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,245 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,245 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,245 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,245 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,245 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,245 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,245 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,245 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,245 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,245 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,245 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,245 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,245 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,245 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,245 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,245 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,245 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,245 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,245 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,245 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-3cebv9k7
2025-09-03 10:31:20,245 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,245 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-vvfuuhzk
2025-09-03 10:31:20,245 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,245 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ghwmnv02
2025-09-03 10:31:20,245 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,245 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,245 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-snkqfmvx
2025-09-03 10:31:20,245 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,245 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-3wu22ab2
2025-09-03 10:31:20,245 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-hqsw4glj
2025-09-03 10:31:20,245 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,245 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-4_goabd1
2025-09-03 10:31:20,245 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,245 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-cjxc_sb8
2025-09-03 10:31:20,245 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,245 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,245 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-t8vvfod7
2025-09-03 10:31:20,245 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,245 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-j1yqin7y
2025-09-03 10:31:20,245 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,245 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,245 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,245 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ulk027bo
2025-09-03 10:31:20,245 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,245 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,245 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,245 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,246 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:34219
2025-09-03 10:31:20,246 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:36409
2025-09-03 10:31:20,246 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:34219
2025-09-03 10:31:20,246 - distributed.worker - INFO -          dashboard at:          10.6.101.57:43037
2025-09-03 10:31:20,246 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:36409
2025-09-03 10:31:20,246 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,246 - distributed.worker - INFO -          dashboard at:          10.6.101.57:41527
2025-09-03 10:31:20,246 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,246 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,246 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,246 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,246 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,246 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,246 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-74zy00n1
2025-09-03 10:31:20,246 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,246 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:36931
2025-09-03 10:31:20,246 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-jy9ppd15
2025-09-03 10:31:20,246 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,246 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:36931
2025-09-03 10:31:20,246 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,246 - distributed.worker - INFO -          dashboard at:          10.6.101.57:42377
2025-09-03 10:31:20,246 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,246 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,246 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,247 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,247 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-900gze49
2025-09-03 10:31:20,247 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,250 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:36773
2025-09-03 10:31:20,251 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:36773
2025-09-03 10:31:20,251 - distributed.worker - INFO -          dashboard at:          10.6.101.57:34473
2025-09-03 10:31:20,251 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,251 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,251 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,251 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,251 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-qkcsypho
2025-09-03 10:31:20,251 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,269 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:34715
2025-09-03 10:31:20,269 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:34715
2025-09-03 10:31:20,269 - distributed.worker - INFO -          dashboard at:          10.6.101.57:39419
2025-09-03 10:31:20,269 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,269 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,269 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,269 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,269 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-771rusgu
2025-09-03 10:31:20,269 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,273 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:40295
2025-09-03 10:31:20,273 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:40295
2025-09-03 10:31:20,273 - distributed.worker - INFO -          dashboard at:          10.6.101.57:43971
2025-09-03 10:31:20,273 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,273 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,273 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,273 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,273 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-2o_nrn6x
2025-09-03 10:31:20,273 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,276 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:33941
2025-09-03 10:31:20,276 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:33941
2025-09-03 10:31:20,276 - distributed.worker - INFO -          dashboard at:          10.6.101.57:44187
2025-09-03 10:31:20,276 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,276 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,276 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,276 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,276 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-gq9mywvg
2025-09-03 10:31:20,276 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,277 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:40245
2025-09-03 10:31:20,278 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:40245
2025-09-03 10:31:20,278 - distributed.worker - INFO -          dashboard at:          10.6.101.57:36523
2025-09-03 10:31:20,278 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,278 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,278 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,278 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,278 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-_qqi4awv
2025-09-03 10:31:20,278 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,406 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:44125
2025-09-03 10:31:20,406 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:44125
2025-09-03 10:31:20,406 - distributed.worker - INFO -          dashboard at:          10.6.101.57:42271
2025-09-03 10:31:20,406 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,406 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,406 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,406 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,406 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-a40grknd
2025-09-03 10:31:20,407 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,421 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:39055
2025-09-03 10:31:20,421 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:39055
2025-09-03 10:31:20,421 - distributed.worker - INFO -          dashboard at:          10.6.101.57:37957
2025-09-03 10:31:20,421 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,421 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,421 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,421 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,421 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-_02_scp_
2025-09-03 10:31:20,422 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,425 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:35023
2025-09-03 10:31:20,425 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:35023
2025-09-03 10:31:20,425 - distributed.worker - INFO -          dashboard at:          10.6.101.57:44051
2025-09-03 10:31:20,425 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,425 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,425 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,425 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,425 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-0pb49vcb
2025-09-03 10:31:20,425 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,517 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:43557
2025-09-03 10:31:20,517 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:43557
2025-09-03 10:31:20,517 - distributed.worker - INFO -          dashboard at:          10.6.101.57:37035
2025-09-03 10:31:20,517 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,517 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,517 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,517 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,517 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-irh9enlr
2025-09-03 10:31:20,518 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,520 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:42891
2025-09-03 10:31:20,520 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:42891
2025-09-03 10:31:20,520 - distributed.worker - INFO -          dashboard at:          10.6.101.57:33979
2025-09-03 10:31:20,520 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,520 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,520 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,520 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,520 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-6s0yrmzh
2025-09-03 10:31:20,520 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,542 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:37001
2025-09-03 10:31:20,542 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:37001
2025-09-03 10:31:20,542 - distributed.worker - INFO -          dashboard at:          10.6.101.57:46693
2025-09-03 10:31:20,542 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,542 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,542 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,542 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,542 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-_i4v_qsy
2025-09-03 10:31:20,542 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,557 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:41723
2025-09-03 10:31:20,557 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:41723
2025-09-03 10:31:20,557 - distributed.worker - INFO -          dashboard at:          10.6.101.57:37241
2025-09-03 10:31:20,557 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,557 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,557 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,557 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,557 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-gy_2f6et
2025-09-03 10:31:20,557 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,570 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:43415
2025-09-03 10:31:20,570 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:43415
2025-09-03 10:31:20,570 - distributed.worker - INFO -          dashboard at:          10.6.101.57:45101
2025-09-03 10:31:20,570 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,570 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,570 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,570 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,570 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-1rfm2f7z
2025-09-03 10:31:20,570 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,579 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:41277
2025-09-03 10:31:20,579 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:41277
2025-09-03 10:31:20,579 - distributed.worker - INFO -          dashboard at:          10.6.101.57:39057
2025-09-03 10:31:20,579 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,579 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,579 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,579 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,579 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-jqyijocq
2025-09-03 10:31:20,579 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,585 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:20,586 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,586 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,587 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:20,631 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:46417
2025-09-03 10:31:20,631 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:46417
2025-09-03 10:31:20,631 - distributed.worker - INFO -          dashboard at:          10.6.101.57:34941
2025-09-03 10:31:20,631 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,631 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,631 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,631 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,631 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-yoqp3rtx
2025-09-03 10:31:20,631 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,639 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:20,640 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,640 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,642 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:20,646 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:20,647 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,647 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,649 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:20,841 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:36853
2025-09-03 10:31:20,841 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:36853
2025-09-03 10:31:20,841 - distributed.worker - INFO -          dashboard at:          10.6.101.57:38837
2025-09-03 10:31:20,841 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,841 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,841 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,841 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,841 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-oc1zs2qi
2025-09-03 10:31:20,841 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,845 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:34787
2025-09-03 10:31:20,845 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:34787
2025-09-03 10:31:20,845 - distributed.worker - INFO -          dashboard at:          10.6.101.57:46581
2025-09-03 10:31:20,845 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,845 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,845 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,845 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,845 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-cwpsatx_
2025-09-03 10:31:20,845 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,851 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:41087
2025-09-03 10:31:20,851 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:41087
2025-09-03 10:31:20,851 - distributed.worker - INFO -          dashboard at:          10.6.101.57:40645
2025-09-03 10:31:20,851 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:20,851 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:20,851 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:20,851 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:20,851 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-c74cu3_j
2025-09-03 10:31:20,851 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,072 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:21,073 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,074 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,075 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:21,185 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:21,187 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,187 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,189 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:21,268 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:34163
2025-09-03 10:31:21,268 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:34163
2025-09-03 10:31:21,268 - distributed.worker - INFO -          dashboard at:          10.6.101.57:42767
2025-09-03 10:31:21,268 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,268 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,268 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,268 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,268 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-w2onuhn8
2025-09-03 10:31:21,268 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,273 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:46555
2025-09-03 10:31:21,273 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:46555
2025-09-03 10:31:21,273 - distributed.worker - INFO -          dashboard at:          10.6.101.57:33653
2025-09-03 10:31:21,273 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,273 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,273 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,273 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,273 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-gnmm7e50
2025-09-03 10:31:21,273 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,470 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:21,472 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,472 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,474 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:21,506 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:21,507 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,509 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:21,564 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:45455
2025-09-03 10:31:21,564 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:45455
2025-09-03 10:31:21,564 - distributed.worker - INFO -          dashboard at:          10.6.101.57:44033
2025-09-03 10:31:21,564 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,564 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,565 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,565 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,565 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-l3n5ihc9
2025-09-03 10:31:21,565 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,567 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:39417
2025-09-03 10:31:21,568 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:39417
2025-09-03 10:31:21,568 - distributed.worker - INFO -          dashboard at:          10.6.101.57:39613
2025-09-03 10:31:21,568 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,568 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,568 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,568 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,568 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-hsj6ouf1
2025-09-03 10:31:21,568 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,599 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:33773
2025-09-03 10:31:21,599 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:33773
2025-09-03 10:31:21,599 - distributed.worker - INFO -          dashboard at:          10.6.101.57:37365
2025-09-03 10:31:21,599 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,599 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,599 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,599 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,599 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-jjh_9xjz
2025-09-03 10:31:21,599 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,600 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:38713
2025-09-03 10:31:21,600 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:38713
2025-09-03 10:31:21,600 - distributed.worker - INFO -          dashboard at:          10.6.101.57:41737
2025-09-03 10:31:21,600 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,600 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,600 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,600 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,600 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-k9g69ji5
2025-09-03 10:31:21,600 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,601 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:46485
2025-09-03 10:31:21,601 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:46485
2025-09-03 10:31:21,601 - distributed.worker - INFO -          dashboard at:          10.6.101.57:37549
2025-09-03 10:31:21,601 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,601 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,601 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,601 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,601 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-cp2wmq2x
2025-09-03 10:31:21,601 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,621 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:43311
2025-09-03 10:31:21,621 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:43311
2025-09-03 10:31:21,621 - distributed.worker - INFO -          dashboard at:          10.6.101.57:44061
2025-09-03 10:31:21,621 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,621 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,621 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,621 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,622 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-7_vikh_h
2025-09-03 10:31:21,622 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,631 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:44107
2025-09-03 10:31:21,631 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:44107
2025-09-03 10:31:21,631 - distributed.worker - INFO -          dashboard at:          10.6.101.57:33149
2025-09-03 10:31:21,631 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,631 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,631 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,631 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,631 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-cjrhkgs4
2025-09-03 10:31:21,631 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,635 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:36633
2025-09-03 10:31:21,636 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:36633
2025-09-03 10:31:21,636 - distributed.worker - INFO -          dashboard at:          10.6.101.57:36075
2025-09-03 10:31:21,636 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,636 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,636 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,636 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,636 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-6wd2ky57
2025-09-03 10:31:21,636 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,642 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:41411
2025-09-03 10:31:21,642 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:41411
2025-09-03 10:31:21,642 - distributed.worker - INFO -          dashboard at:          10.6.101.57:39171
2025-09-03 10:31:21,642 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,642 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,642 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,642 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,642 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-m54zk0x0
2025-09-03 10:31:21,642 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,646 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:35645
2025-09-03 10:31:21,646 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:35645
2025-09-03 10:31:21,646 - distributed.worker - INFO -          dashboard at:          10.6.101.57:42521
2025-09-03 10:31:21,646 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,646 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,646 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,646 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,646 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-kfq7z7wx
2025-09-03 10:31:21,646 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,652 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:43073
2025-09-03 10:31:21,652 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:43073
2025-09-03 10:31:21,652 - distributed.worker - INFO -          dashboard at:          10.6.101.57:39935
2025-09-03 10:31:21,652 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,652 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,652 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,652 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,652 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-m7g6teqm
2025-09-03 10:31:21,652 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,665 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:39941
2025-09-03 10:31:21,665 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:39941
2025-09-03 10:31:21,665 - distributed.worker - INFO -          dashboard at:          10.6.101.57:43247
2025-09-03 10:31:21,665 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,665 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,665 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,666 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,666 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-6hw2yb9c
2025-09-03 10:31:21,666 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,670 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:46491
2025-09-03 10:31:21,670 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:46491
2025-09-03 10:31:21,670 - distributed.worker - INFO -          dashboard at:          10.6.101.57:38527
2025-09-03 10:31:21,670 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,670 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,670 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,670 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,670 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-7w1styet
2025-09-03 10:31:21,670 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,670 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:46295
2025-09-03 10:31:21,670 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:46295
2025-09-03 10:31:21,670 - distributed.worker - INFO -          dashboard at:          10.6.101.57:38877
2025-09-03 10:31:21,670 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,670 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,670 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,670 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,670 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-7huvhzpn
2025-09-03 10:31:21,670 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,671 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:42423
2025-09-03 10:31:21,671 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:42423
2025-09-03 10:31:21,671 - distributed.worker - INFO -          dashboard at:          10.6.101.57:38335
2025-09-03 10:31:21,671 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,671 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,671 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,671 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,671 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-4ivlk1q2
2025-09-03 10:31:21,672 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,675 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:34809
2025-09-03 10:31:21,675 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:34809
2025-09-03 10:31:21,675 - distributed.worker - INFO -          dashboard at:          10.6.101.57:39215
2025-09-03 10:31:21,675 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,675 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,675 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,675 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,675 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-akk2j1cy
2025-09-03 10:31:21,675 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,678 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:45149
2025-09-03 10:31:21,678 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:45149
2025-09-03 10:31:21,678 - distributed.worker - INFO -          dashboard at:          10.6.101.57:39301
2025-09-03 10:31:21,678 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,678 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,678 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,678 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,678 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-u9sjr5jp
2025-09-03 10:31:21,678 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,688 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:44607
2025-09-03 10:31:21,688 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:44607
2025-09-03 10:31:21,688 - distributed.worker - INFO -          dashboard at:          10.6.101.57:38949
2025-09-03 10:31:21,688 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,688 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,688 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,688 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,688 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-q0703a32
2025-09-03 10:31:21,688 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,689 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:45009
2025-09-03 10:31:21,689 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:45009
2025-09-03 10:31:21,689 - distributed.worker - INFO -          dashboard at:          10.6.101.57:35879
2025-09-03 10:31:21,689 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,689 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,689 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,689 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,689 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-garybs1_
2025-09-03 10:31:21,689 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,701 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:40009
2025-09-03 10:31:21,702 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:40009
2025-09-03 10:31:21,702 - distributed.worker - INFO -          dashboard at:          10.6.101.57:38481
2025-09-03 10:31:21,702 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,702 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,702 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,702 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,702 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-xhde2eme
2025-09-03 10:31:21,702 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,702 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:40659
2025-09-03 10:31:21,702 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:40659
2025-09-03 10:31:21,702 - distributed.worker - INFO -          dashboard at:          10.6.101.57:44735
2025-09-03 10:31:21,702 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,702 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,702 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,702 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,702 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-2gu8jxn0
2025-09-03 10:31:21,702 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,704 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:36431
2025-09-03 10:31:21,704 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:36431
2025-09-03 10:31:21,704 - distributed.worker - INFO -          dashboard at:          10.6.101.57:38967
2025-09-03 10:31:21,704 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,704 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,704 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,704 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,704 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-kv4z15yc
2025-09-03 10:31:21,704 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,704 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:39339
2025-09-03 10:31:21,704 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:39339
2025-09-03 10:31:21,704 - distributed.worker - INFO -          dashboard at:          10.6.101.57:44425
2025-09-03 10:31:21,704 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,704 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,704 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,705 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,705 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-7hyqnfs_
2025-09-03 10:31:21,705 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,705 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:36819
2025-09-03 10:31:21,705 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:36819
2025-09-03 10:31:21,705 - distributed.worker - INFO -          dashboard at:          10.6.101.57:34619
2025-09-03 10:31:21,706 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,706 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,706 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,706 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,706 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-_onzmzxl
2025-09-03 10:31:21,706 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,716 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:37567
2025-09-03 10:31:21,717 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:37567
2025-09-03 10:31:21,717 - distributed.worker - INFO -          dashboard at:          10.6.101.57:42177
2025-09-03 10:31:21,717 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,717 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,717 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,717 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,717 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-2tx7ramf
2025-09-03 10:31:21,717 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,732 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:42397
2025-09-03 10:31:21,732 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:42397
2025-09-03 10:31:21,732 - distributed.worker - INFO -          dashboard at:          10.6.101.57:41259
2025-09-03 10:31:21,732 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,732 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,732 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,732 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,732 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ccgc_u_d
2025-09-03 10:31:21,732 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,744 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:32871
2025-09-03 10:31:21,744 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:32871
2025-09-03 10:31:21,744 - distributed.worker - INFO -          dashboard at:          10.6.101.57:40457
2025-09-03 10:31:21,744 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,744 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,744 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,744 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,744 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-tpejod1g
2025-09-03 10:31:21,744 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,784 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:45229
2025-09-03 10:31:21,784 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:45229
2025-09-03 10:31:21,784 - distributed.worker - INFO -          dashboard at:          10.6.101.57:39393
2025-09-03 10:31:21,784 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,784 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,784 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,784 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,784 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-9x3kq4rc
2025-09-03 10:31:21,784 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,811 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:46225
2025-09-03 10:31:21,811 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:46225
2025-09-03 10:31:21,811 - distributed.worker - INFO -          dashboard at:          10.6.101.57:38571
2025-09-03 10:31:21,811 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,811 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,811 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,811 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,811 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-a4vw4wnz
2025-09-03 10:31:21,811 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,863 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:40913
2025-09-03 10:31:21,863 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:40913
2025-09-03 10:31:21,863 - distributed.worker - INFO -          dashboard at:          10.6.101.57:37841
2025-09-03 10:31:21,863 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,863 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,863 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,864 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,864 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-2a2ybhzd
2025-09-03 10:31:21,864 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,878 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:41655
2025-09-03 10:31:21,878 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:41655
2025-09-03 10:31:21,878 - distributed.worker - INFO -          dashboard at:          10.6.101.57:33445
2025-09-03 10:31:21,878 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,878 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,878 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,879 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,879 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-wcpfrakj
2025-09-03 10:31:21,879 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,879 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:39513
2025-09-03 10:31:21,879 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:39513
2025-09-03 10:31:21,879 - distributed.worker - INFO -          dashboard at:          10.6.101.57:35021
2025-09-03 10:31:21,879 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,879 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,879 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,879 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,879 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-t7h3y10t
2025-09-03 10:31:21,879 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,902 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:45921
2025-09-03 10:31:21,902 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:45921
2025-09-03 10:31:21,902 - distributed.worker - INFO -          dashboard at:          10.6.101.57:38019
2025-09-03 10:31:21,902 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,902 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,902 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,902 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,902 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-viu_rrhg
2025-09-03 10:31:21,902 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,905 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:38083
2025-09-03 10:31:21,905 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:38083
2025-09-03 10:31:21,905 - distributed.worker - INFO -          dashboard at:          10.6.101.57:39275
2025-09-03 10:31:21,905 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,905 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,905 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,905 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,905 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-s93dxop4
2025-09-03 10:31:21,905 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,911 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:45471
2025-09-03 10:31:21,912 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:45471
2025-09-03 10:31:21,912 - distributed.worker - INFO -          dashboard at:          10.6.101.57:33039
2025-09-03 10:31:21,912 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,912 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,912 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,912 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,912 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-5eop1uis
2025-09-03 10:31:21,912 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,914 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:45765
2025-09-03 10:31:21,914 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:45765
2025-09-03 10:31:21,915 - distributed.worker - INFO -          dashboard at:          10.6.101.57:33513
2025-09-03 10:31:21,915 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,915 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,915 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,915 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,915 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-5z40e7fk
2025-09-03 10:31:21,915 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,917 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:38401
2025-09-03 10:31:21,917 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:38401
2025-09-03 10:31:21,917 - distributed.worker - INFO -          dashboard at:          10.6.101.57:43625
2025-09-03 10:31:21,917 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,917 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,917 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,917 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,917 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-w36jeyot
2025-09-03 10:31:21,917 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,925 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:41261
2025-09-03 10:31:21,925 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:41261
2025-09-03 10:31:21,925 - distributed.worker - INFO -          dashboard at:          10.6.101.57:37343
2025-09-03 10:31:21,925 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,925 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,925 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,925 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,925 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-jk9_cyv1
2025-09-03 10:31:21,925 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,927 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:44895
2025-09-03 10:31:21,927 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:44895
2025-09-03 10:31:21,927 - distributed.worker - INFO -          dashboard at:          10.6.101.57:40947
2025-09-03 10:31:21,927 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,927 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,927 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,927 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,927 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-sef3ja6j
2025-09-03 10:31:21,927 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,928 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:42733
2025-09-03 10:31:21,928 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:42733
2025-09-03 10:31:21,928 - distributed.worker - INFO -          dashboard at:          10.6.101.57:43269
2025-09-03 10:31:21,928 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,928 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,928 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,928 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,928 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-4b785e0w
2025-09-03 10:31:21,928 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,931 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:42803
2025-09-03 10:31:21,931 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:42803
2025-09-03 10:31:21,931 - distributed.worker - INFO -          dashboard at:          10.6.101.57:38045
2025-09-03 10:31:21,931 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,931 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,931 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,932 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,932 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-vphn5v2y
2025-09-03 10:31:21,932 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,932 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:36973
2025-09-03 10:31:21,932 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:36973
2025-09-03 10:31:21,932 - distributed.worker - INFO -          dashboard at:          10.6.101.57:43865
2025-09-03 10:31:21,932 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,932 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,932 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,932 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,932 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-arcmx5dy
2025-09-03 10:31:21,932 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,937 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:33681
2025-09-03 10:31:21,937 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:33681
2025-09-03 10:31:21,937 - distributed.worker - INFO -          dashboard at:          10.6.101.57:33273
2025-09-03 10:31:21,937 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,937 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,937 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,937 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,937 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-7y933f44
2025-09-03 10:31:21,937 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,942 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:34855
2025-09-03 10:31:21,942 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:34855
2025-09-03 10:31:21,942 - distributed.worker - INFO -          dashboard at:          10.6.101.57:45359
2025-09-03 10:31:21,942 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,942 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,943 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,943 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,943 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-y6d33_6m
2025-09-03 10:31:21,943 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,943 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:43367
2025-09-03 10:31:21,943 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:43367
2025-09-03 10:31:21,943 - distributed.worker - INFO -          dashboard at:          10.6.101.57:35901
2025-09-03 10:31:21,943 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,943 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,943 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,943 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,943 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-euzxwmqe
2025-09-03 10:31:21,943 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,949 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:34583
2025-09-03 10:31:21,949 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:34583
2025-09-03 10:31:21,949 - distributed.worker - INFO -          dashboard at:          10.6.101.57:42333
2025-09-03 10:31:21,949 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,949 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,949 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,949 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,949 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-5ljc9evz
2025-09-03 10:31:21,949 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,954 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:44671
2025-09-03 10:31:21,954 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:44671
2025-09-03 10:31:21,955 - distributed.worker - INFO -          dashboard at:          10.6.101.57:43699
2025-09-03 10:31:21,955 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,955 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,955 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,955 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,955 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-pp5r10ed
2025-09-03 10:31:21,955 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,956 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:42111
2025-09-03 10:31:21,956 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:42111
2025-09-03 10:31:21,956 - distributed.worker - INFO -          dashboard at:          10.6.101.57:35335
2025-09-03 10:31:21,956 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,956 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,956 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,956 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,956 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-hg7ray2h
2025-09-03 10:31:21,956 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,961 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:36713
2025-09-03 10:31:21,961 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:36713
2025-09-03 10:31:21,961 - distributed.worker - INFO -          dashboard at:          10.6.101.57:40719
2025-09-03 10:31:21,961 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,961 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,961 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,961 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,961 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-p2bmgw_c
2025-09-03 10:31:21,962 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,966 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:42231
2025-09-03 10:31:21,966 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:42231
2025-09-03 10:31:21,966 - distributed.worker - INFO -          dashboard at:          10.6.101.57:45515
2025-09-03 10:31:21,966 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,966 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,966 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,966 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,966 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-52cqitfr
2025-09-03 10:31:21,966 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,967 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:46849
2025-09-03 10:31:21,967 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:46849
2025-09-03 10:31:21,967 - distributed.worker - INFO -          dashboard at:          10.6.101.57:40743
2025-09-03 10:31:21,967 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,967 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,967 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,967 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,967 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-13odiq2j
2025-09-03 10:31:21,967 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,969 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:36187
2025-09-03 10:31:21,969 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:36187
2025-09-03 10:31:21,969 - distributed.worker - INFO -          dashboard at:          10.6.101.57:44573
2025-09-03 10:31:21,969 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,969 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,969 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,969 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,969 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-z6m_7tv8
2025-09-03 10:31:21,969 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,970 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:40651
2025-09-03 10:31:21,970 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:40651
2025-09-03 10:31:21,970 - distributed.worker - INFO -          dashboard at:          10.6.101.57:34519
2025-09-03 10:31:21,970 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,970 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,970 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,970 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,970 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ukc1uv16
2025-09-03 10:31:21,970 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,972 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:33875
2025-09-03 10:31:21,972 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:33875
2025-09-03 10:31:21,972 - distributed.worker - INFO -          dashboard at:          10.6.101.57:33443
2025-09-03 10:31:21,972 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,972 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,972 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,972 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,972 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-sop2vumi
2025-09-03 10:31:21,972 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,975 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:37367
2025-09-03 10:31:21,976 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:37367
2025-09-03 10:31:21,976 - distributed.worker - INFO -          dashboard at:          10.6.101.57:36019
2025-09-03 10:31:21,976 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,976 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,976 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,976 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,976 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-p3wp240d
2025-09-03 10:31:21,976 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,981 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:46873
2025-09-03 10:31:21,981 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:46873
2025-09-03 10:31:21,981 - distributed.worker - INFO -          dashboard at:          10.6.101.57:42135
2025-09-03 10:31:21,981 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,981 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,982 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,982 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,982 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-i36xvbmu
2025-09-03 10:31:21,982 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,984 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:41017
2025-09-03 10:31:21,984 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:41017
2025-09-03 10:31:21,984 - distributed.worker - INFO -          dashboard at:          10.6.101.57:33065
2025-09-03 10:31:21,984 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,984 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,984 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,984 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,984 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-poo1vskk
2025-09-03 10:31:21,984 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,985 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:40341
2025-09-03 10:31:21,985 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:40341
2025-09-03 10:31:21,985 - distributed.worker - INFO -          dashboard at:          10.6.101.57:43211
2025-09-03 10:31:21,985 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,985 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,985 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,985 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,985 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ktrop0xz
2025-09-03 10:31:21,985 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,987 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:44689
2025-09-03 10:31:21,987 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:44689
2025-09-03 10:31:21,987 - distributed.worker - INFO -          dashboard at:          10.6.101.57:33741
2025-09-03 10:31:21,987 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:21,987 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:21,987 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:21,987 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:21,987 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-cs1_267k
2025-09-03 10:31:21,987 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,008 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:33767
2025-09-03 10:31:22,008 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:33767
2025-09-03 10:31:22,009 - distributed.worker - INFO -          dashboard at:          10.6.101.57:42207
2025-09-03 10:31:22,009 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,009 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,009 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:22,009 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:22,009 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-a_wohh2i
2025-09-03 10:31:22,009 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,023 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,024 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,024 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,026 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,037 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,038 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,038 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,040 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,044 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,045 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,045 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,047 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,105 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,106 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,106 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,108 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,177 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:44699
2025-09-03 10:31:22,177 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:44699
2025-09-03 10:31:22,177 - distributed.worker - INFO -          dashboard at:          10.6.101.57:36395
2025-09-03 10:31:22,177 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,177 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,177 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:22,177 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:22,177 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-gp10o5e3
2025-09-03 10:31:22,177 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,331 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:44921
2025-09-03 10:31:22,331 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:44921
2025-09-03 10:31:22,331 - distributed.worker - INFO -          dashboard at:          10.6.101.57:35095
2025-09-03 10:31:22,331 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,331 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,331 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:22,331 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:22,331 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-sar0o0kw
2025-09-03 10:31:22,332 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,538 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,539 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,540 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,541 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,546 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,547 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,547 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,549 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,553 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,554 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,554 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,556 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,560 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,562 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,562 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,563 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,592 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:42549
2025-09-03 10:31:22,592 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:42549
2025-09-03 10:31:22,592 - distributed.worker - INFO -          dashboard at:          10.6.101.57:37737
2025-09-03 10:31:22,592 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,592 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,592 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:22,592 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:22,592 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-wsfz_dm1
2025-09-03 10:31:22,592 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,593 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:42911
2025-09-03 10:31:22,593 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:42911
2025-09-03 10:31:22,593 - distributed.worker - INFO -          dashboard at:          10.6.101.57:37753
2025-09-03 10:31:22,593 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,593 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,593 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:22,593 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:22,593 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-x809w5hx
2025-09-03 10:31:22,593 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,593 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:37819
2025-09-03 10:31:22,594 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:37819
2025-09-03 10:31:22,594 - distributed.worker - INFO -          dashboard at:          10.6.101.57:34367
2025-09-03 10:31:22,594 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,594 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,594 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:22,594 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:22,594 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-0eb0bvrz
2025-09-03 10:31:22,594 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,594 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:45191
2025-09-03 10:31:22,594 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:45191
2025-09-03 10:31:22,594 - distributed.worker - INFO -          dashboard at:          10.6.101.57:43943
2025-09-03 10:31:22,594 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,594 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,594 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:22,594 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:22,594 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-1l0afulf
2025-09-03 10:31:22,594 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,595 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:42393
2025-09-03 10:31:22,595 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:42393
2025-09-03 10:31:22,595 - distributed.worker - INFO -          dashboard at:          10.6.101.57:43429
2025-09-03 10:31:22,595 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,595 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,595 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:22,595 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:22,595 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-pdqxcf_5
2025-09-03 10:31:22,595 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,596 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:36193
2025-09-03 10:31:22,596 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:36193
2025-09-03 10:31:22,596 - distributed.worker - INFO -          dashboard at:          10.6.101.57:39011
2025-09-03 10:31:22,596 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,596 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,596 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:22,596 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:22,596 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-zoucpng_
2025-09-03 10:31:22,596 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,597 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:45479
2025-09-03 10:31:22,597 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:45479
2025-09-03 10:31:22,597 - distributed.worker - INFO -          dashboard at:          10.6.101.57:38055
2025-09-03 10:31:22,597 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,597 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,597 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:22,597 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:22,597 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-3sdjd_ji
2025-09-03 10:31:22,597 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,598 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.57:42485
2025-09-03 10:31:22,598 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.57:42485
2025-09-03 10:31:22,598 - distributed.worker - INFO -          dashboard at:          10.6.101.57:44625
2025-09-03 10:31:22,598 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,598 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,598 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:22,598 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:22,598 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-u1pg15mn
2025-09-03 10:31:22,598 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,602 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,603 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,603 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,605 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,618 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,619 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,619 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,621 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,632 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,633 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,633 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,635 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,640 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,641 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,641 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,643 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,648 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,649 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,649 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,651 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,655 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,656 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,656 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,658 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,663 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,664 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,664 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,666 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,670 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,672 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,672 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,673 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,678 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,679 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,679 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,681 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,685 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,686 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,686 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,688 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,693 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,694 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,694 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,695 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,700 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,701 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,701 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,703 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,708 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,709 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,709 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,711 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,715 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,717 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,717 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,718 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,724 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,725 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,725 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,727 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,734 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,735 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,735 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,737 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,742 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,744 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,744 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,745 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,903 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,904 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,905 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,906 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,911 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,912 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,913 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,914 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,919 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,920 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,920 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,921 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,927 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,928 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,928 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,930 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,934 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,935 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,935 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,937 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,941 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,942 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,943 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,944 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,949 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,950 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,950 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,952 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:22,957 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:22,958 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:22,958 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:22,960 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:23,417 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:23,419 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:23,419 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:23,421 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:23,425 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:23,426 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:23,426 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:23,428 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:23,433 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:23,434 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:23,435 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:23,437 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:23,440 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:23,441 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:23,442 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:23,444 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:23,448 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:23,449 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:23,449 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:23,451 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:23,852 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:23,853 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:23,854 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:23,855 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:23,923 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:23,924 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:23,924 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:23,926 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:23,938 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:23,939 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:23,939 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:23,941 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:24,555 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:24,556 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:24,556 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:24,558 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:24,595 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:24,596 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:24,596 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:24,598 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:24,603 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:24,605 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:24,605 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:24,607 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:24,611 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:24,612 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:24,613 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:24,614 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:24,620 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:24,621 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:24,621 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:24,623 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:24,628 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:24,629 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:24,629 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:24,631 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:24,636 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:24,637 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:24,637 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:24,639 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:24,644 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:24,645 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:24,645 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:24,647 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:24,652 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:24,653 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:24,654 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:24,655 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:24,660 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:24,661 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:24,661 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:24,663 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:24,669 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:24,670 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:24,670 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:24,672 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:24,676 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:24,677 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:24,678 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:24,679 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:24,685 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:24,686 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:24,686 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:24,688 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:24,693 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:24,694 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:24,694 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:24,696 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:24,701 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:24,702 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:24,702 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:24,704 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:24,709 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:24,710 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:24,710 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:24,712 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:24,718 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:24,719 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:24,719 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:24,721 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:24,726 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:24,727 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:24,727 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:24,729 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:24,734 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:24,735 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:24,735 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:24,737 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:24,742 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:24,744 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:24,744 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:24,746 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:24,750 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:24,751 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:24,752 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:24,754 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:24,759 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:24,760 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:24,760 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:24,762 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:24,767 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:24,768 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:24,768 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:24,770 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:26,295 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:26,296 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:26,297 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:26,298 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:26,313 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:26,315 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:26,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:26,317 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:26,325 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:26,326 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:26,326 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:26,328 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:27,086 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:27,087 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:27,088 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:27,089 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:27,100 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:27,101 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:27,102 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:27,103 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:27,111 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:27,112 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:27,112 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:27,114 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:27,120 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:27,121 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:27,121 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:27,123 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:27,128 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:27,130 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:27,130 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:27,131 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:27,138 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:27,141 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:27,141 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:27,145 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:28,151 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:28,153 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:28,153 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:28,155 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:28,159 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:28,161 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:28,161 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:28,163 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:28,168 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:28,169 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:28,170 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:28,171 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:28,176 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:28,177 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:28,177 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:28,179 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:28,184 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:28,186 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:28,186 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:28,188 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:28,192 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:28,194 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:28,194 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:28,195 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:28,327 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:28,330 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:28,330 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:28,334 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:28,336 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:28,338 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:28,339 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:28,342 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:28,344 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:28,347 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:28,347 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:28,351 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:28,505 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:28,507 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:28,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:28,509 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:35,384 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:35,385 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:35,386 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:35,387 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:35,423 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:35,425 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:35,425 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:35,427 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:35,433 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:35,435 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:35,435 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:35,436 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:35,453 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:35,455 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:35,455 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:35,457 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:35,463 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:35,465 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:35,465 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:35,467 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:35,473 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:35,475 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:35,475 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:35,477 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:35,483 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:35,485 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:35,485 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:35,486 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:35,493 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:35,495 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:35,495 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:35,497 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:35,503 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:35,505 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:35,505 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:35,507 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:35,513 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:35,514 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:35,514 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:35,516 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:35,523 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:35,525 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:35,525 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:35,527 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:35,533 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:35,534 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:35,535 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:35,536 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:37,710 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:37,714 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:37,714 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:37,720 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:37,732 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:37,736 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:37,736 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:37,741 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:59,157 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:31:59,165 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:31:59,174 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:31:59,182 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:31:59,190 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:31:59,197 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:06,389 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:06,429 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:06,440 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:06,458 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:06,468 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:06,480 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:06,488 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:06,499 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:06,509 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:06,517 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:06,529 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:06,539 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:08,719 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:30,158 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:30,166 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:30,174 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:30,182 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:30,189 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:30,198 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:37,529 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:37,538 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:35:51,112 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,117 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,121 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,123 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,338 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,343 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,688 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,690 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,804 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,809 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,885 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,887 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,888 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,888 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,891 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,892 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,957 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,959 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,049 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,052 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,058 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,059 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,095 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,096 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,101 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,102 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,118 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,121 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,087 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,090 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,113 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,115 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,130 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,131 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,193 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,194 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,368 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,374 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,379 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,380 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,388 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,389 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,406 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,408 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,540 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,541 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,542 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,543 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,618 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,625 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,846 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,847 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,949 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,954 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,007 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,010 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,236 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,237 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,240 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,241 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,242 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,243 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,248 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,248 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,260 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,262 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,286 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,288 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,355 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,361 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,796 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,798 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,928 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,929 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,964 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,970 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,091 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,095 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,103 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,108 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,244 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,245 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,247 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,249 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,254 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,257 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,305 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,311 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,448 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,451 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,659 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,661 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,736 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,742 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,770 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,773 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,998 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,000 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,092 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,092 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,094 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,098 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,127 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,133 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,179 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,187 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,208 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,209 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,246 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,252 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,318 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,324 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,373 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,379 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,644 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,646 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,919 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,925 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,109 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,117 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,469 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,469 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,471 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,471 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,487 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,489 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,674 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,677 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,706 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,708 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,779 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,787 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,815 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,818 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,887 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,894 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,941 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,946 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,977 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,982 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,003 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,004 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,010 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,011 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,065 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,071 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,115 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,121 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,619 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,620 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,732 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,734 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,742 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,743 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,795 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,800 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,896 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,901 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,082 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,086 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,228 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,234 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,329 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,334 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,349 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,351 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,744 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,746 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,778 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,781 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,789 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,791 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,948 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,955 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,867 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,872 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,930 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,932 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,938 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,940 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,260 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,262 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,732 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,737 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,977 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,979 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,144 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,146 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,938 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,939 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:03,015 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:03,016 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:03,060 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:03,061 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:03,230 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:03,232 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:03,428 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:03,431 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:03,722 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:03,727 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:04,303 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:04,305 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:04,318 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:04,320 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:06,250 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:06,251 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:06,798 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:06,799 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:22,894 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,896 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,896 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,898 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,899 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,900 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,901 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,901 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,906 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,907 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,908 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,909 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,919 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,921 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,932 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,936 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,938 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,939 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,940 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,940 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,940 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,942 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,393 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,398 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,405 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,408 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,515 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,515 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,517 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,517 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,528 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,530 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,556 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,559 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,558 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,562 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,783 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,785 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,805 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,807 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,830 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,835 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,853 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,855 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,859 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,861 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,877 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,880 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,912 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,914 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,015 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,015 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,015 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,017 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,018 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,018 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,020 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,022 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,033 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,035 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,039 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,041 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,068 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,074 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,077 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,079 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,079 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,085 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,094 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,096 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,115 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,117 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,116 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,118 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,347 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,349 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,384 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,385 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,386 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,388 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,390 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,392 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,394 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,396 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,404 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,409 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,433 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,435 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,438 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,440 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,442 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,445 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,450 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,455 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,457 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,460 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,460 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,462 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,464 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,467 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,498 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,501 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,512 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,512 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,514 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,514 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,581 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,583 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,590 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,593 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,593 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,595 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,596 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,597 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,598 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,599 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,628 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,629 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,630 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,631 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,635 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,636 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,637 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,638 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,666 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,666 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,668 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,668 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,676 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,678 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,686 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,689 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,744 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,745 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,752 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,754 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,758 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,759 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,759 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,761 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,788 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,790 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,852 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,854 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,856 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,861 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,861 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,862 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,864 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,864 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,891 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,893 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,894 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,896 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,897 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,898 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,922 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,924 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,927 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,929 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,928 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,931 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,933 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,934 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,944 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,945 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,947 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,947 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,052 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,054 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,277 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,282 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,418 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,421 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,479 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,484 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,556 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,559 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,570 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,572 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,629 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,632 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,641 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,643 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,711 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,713 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,823 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,826 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,923 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,926 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,021 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,024 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,085 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,087 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,327 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,329 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,338 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,341 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,409 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,411 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,566 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,569 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,575 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,577 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:27,275 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:27,277 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:28,109 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:28,111 - distributed.utils - INFO - Reload module qme_vars from .py file
