Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-03 10:52:52,226 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:40987'
2025-09-03 10:52:52,236 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:42827'
2025-09-03 10:52:52,241 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:37489'
2025-09-03 10:52:52,247 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:33839'
2025-09-03 10:52:52,252 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:33707'
2025-09-03 10:52:52,344 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:33139'
2025-09-03 10:52:52,348 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:38587'
2025-09-03 10:52:52,354 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:36139'
2025-09-03 10:52:52,358 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:37841'
2025-09-03 10:52:52,362 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:46507'
2025-09-03 10:52:52,367 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:40833'
2025-09-03 10:52:52,371 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:36485'
2025-09-03 10:52:52,375 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:36969'
2025-09-03 10:52:52,379 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:34635'
2025-09-03 10:52:52,388 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:46165'
2025-09-03 10:52:52,393 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:44415'
2025-09-03 10:52:52,398 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:39495'
2025-09-03 10:52:52,402 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:33911'
2025-09-03 10:52:52,407 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:37099'
2025-09-03 10:52:52,411 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:42239'
2025-09-03 10:52:52,415 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:37753'
2025-09-03 10:52:52,419 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:45561'
2025-09-03 10:52:52,423 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:37229'
2025-09-03 10:52:52,427 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:35725'
2025-09-03 10:52:52,432 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:45307'
2025-09-03 10:52:52,436 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:39779'
2025-09-03 10:52:52,439 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:43521'
2025-09-03 10:52:52,443 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:34321'
2025-09-03 10:52:52,447 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:46721'
2025-09-03 10:52:52,451 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:44813'
2025-09-03 10:52:52,454 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:44451'
2025-09-03 10:52:52,459 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:46375'
2025-09-03 10:52:52,464 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:37773'
2025-09-03 10:52:52,469 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:38491'
2025-09-03 10:52:52,472 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:33901'
2025-09-03 10:52:52,477 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:39479'
2025-09-03 10:52:52,482 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:44735'
2025-09-03 10:52:52,486 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:39143'
2025-09-03 10:52:52,491 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:33181'
2025-09-03 10:52:52,496 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:42981'
2025-09-03 10:52:52,499 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:33119'
2025-09-03 10:52:52,504 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:40721'
2025-09-03 10:52:52,582 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:34715'
2025-09-03 10:52:52,587 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:43223'
2025-09-03 10:52:52,591 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:35229'
2025-09-03 10:52:52,596 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:34537'
2025-09-03 10:52:52,600 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:36725'
2025-09-03 10:52:52,605 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:39355'
2025-09-03 10:52:52,610 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:34743'
2025-09-03 10:52:52,614 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:41645'
2025-09-03 10:52:52,620 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:40569'
2025-09-03 10:52:52,625 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:36735'
2025-09-03 10:52:52,630 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:46147'
2025-09-03 10:52:52,634 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:42773'
2025-09-03 10:52:52,639 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:41515'
2025-09-03 10:52:52,642 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:46483'
2025-09-03 10:52:52,645 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:40099'
2025-09-03 10:52:52,651 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:44311'
2025-09-03 10:52:52,655 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:35463'
2025-09-03 10:52:52,658 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:40555'
2025-09-03 10:52:52,663 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:42179'
2025-09-03 10:52:52,666 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:43087'
2025-09-03 10:52:52,671 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:46123'
2025-09-03 10:52:52,677 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:46305'
2025-09-03 10:52:52,683 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:45177'
2025-09-03 10:52:52,687 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:32913'
2025-09-03 10:52:52,692 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:39397'
2025-09-03 10:52:52,697 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:34929'
2025-09-03 10:52:52,701 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:44773'
2025-09-03 10:52:52,708 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:42707'
2025-09-03 10:52:52,712 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:44893'
2025-09-03 10:52:52,716 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:46363'
2025-09-03 10:52:52,722 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:37847'
2025-09-03 10:52:52,728 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:40053'
2025-09-03 10:52:52,733 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:46831'
2025-09-03 10:52:52,738 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:41021'
2025-09-03 10:52:52,742 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:41691'
2025-09-03 10:52:52,760 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:46829'
2025-09-03 10:52:52,765 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:36231'
2025-09-03 10:52:52,770 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:37509'
2025-09-03 10:52:52,774 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:35935'
2025-09-03 10:52:52,780 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:45397'
2025-09-03 10:52:52,784 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:41781'
2025-09-03 10:52:52,788 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:45825'
2025-09-03 10:52:52,792 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:41477'
2025-09-03 10:52:52,798 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:45051'
2025-09-03 10:52:52,803 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:42879'
2025-09-03 10:52:52,809 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:35729'
2025-09-03 10:52:52,812 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:39369'
2025-09-03 10:52:52,816 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:45723'
2025-09-03 10:52:52,820 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:35249'
2025-09-03 10:52:52,825 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:38199'
2025-09-03 10:52:52,830 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:36531'
2025-09-03 10:52:52,835 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:39087'
2025-09-03 10:52:52,840 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:33103'
2025-09-03 10:52:52,845 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:37729'
2025-09-03 10:52:52,849 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:32981'
2025-09-03 10:52:52,854 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:44475'
2025-09-03 10:52:54,154 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:40611'
2025-09-03 10:52:54,197 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:45203
2025-09-03 10:52:54,197 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:41431
2025-09-03 10:52:54,197 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:43755
2025-09-03 10:52:54,197 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:34187
2025-09-03 10:52:54,197 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:41359
2025-09-03 10:52:54,197 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:42505
2025-09-03 10:52:54,197 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:33657
2025-09-03 10:52:54,197 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:42703
2025-09-03 10:52:54,197 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:33849
2025-09-03 10:52:54,197 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:36459
2025-09-03 10:52:54,197 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:44535
2025-09-03 10:52:54,197 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:34805
2025-09-03 10:52:54,197 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:41825
2025-09-03 10:52:54,197 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:45203
2025-09-03 10:52:54,197 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:40531
2025-09-03 10:52:54,197 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:34637
2025-09-03 10:52:54,197 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:41431
2025-09-03 10:52:54,197 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:35101
2025-09-03 10:52:54,197 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:43755
2025-09-03 10:52:54,197 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:42253
2025-09-03 10:52:54,197 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:40967
2025-09-03 10:52:54,197 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:34187
2025-09-03 10:52:54,197 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:41359
2025-09-03 10:52:54,197 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:42505
2025-09-03 10:52:54,197 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:33657
2025-09-03 10:52:54,197 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:42703
2025-09-03 10:52:54,197 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:33849
2025-09-03 10:52:54,197 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:36459
2025-09-03 10:52:54,197 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:44535
2025-09-03 10:52:54,197 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:34805
2025-09-03 10:52:54,197 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:41825
2025-09-03 10:52:54,197 - distributed.worker - INFO -          dashboard at:          10.6.105.22:32865
2025-09-03 10:52:54,197 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:40531
2025-09-03 10:52:54,197 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:34637
2025-09-03 10:52:54,197 - distributed.worker - INFO -          dashboard at:          10.6.105.22:39089
2025-09-03 10:52:54,197 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:35101
2025-09-03 10:52:54,197 - distributed.worker - INFO -          dashboard at:          10.6.105.22:37351
2025-09-03 10:52:54,197 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:42253
2025-09-03 10:52:54,197 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:40967
2025-09-03 10:52:54,197 - distributed.worker - INFO -          dashboard at:          10.6.105.22:37005
2025-09-03 10:52:54,197 - distributed.worker - INFO -          dashboard at:          10.6.105.22:43697
2025-09-03 10:52:54,197 - distributed.worker - INFO -          dashboard at:          10.6.105.22:38623
2025-09-03 10:52:54,197 - distributed.worker - INFO -          dashboard at:          10.6.105.22:44627
2025-09-03 10:52:54,197 - distributed.worker - INFO -          dashboard at:          10.6.105.22:39093
2025-09-03 10:52:54,197 - distributed.worker - INFO -          dashboard at:          10.6.105.22:41043
2025-09-03 10:52:54,197 - distributed.worker - INFO -          dashboard at:          10.6.105.22:37963
2025-09-03 10:52:54,197 - distributed.worker - INFO -          dashboard at:          10.6.105.22:43577
2025-09-03 10:52:54,197 - distributed.worker - INFO -          dashboard at:          10.6.105.22:44163
2025-09-03 10:52:54,197 - distributed.worker - INFO -          dashboard at:          10.6.105.22:34105
2025-09-03 10:52:54,197 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,197 - distributed.worker - INFO -          dashboard at:          10.6.105.22:36239
2025-09-03 10:52:54,197 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,197 - distributed.worker - INFO -          dashboard at:          10.6.105.22:43449
2025-09-03 10:52:54,197 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,197 - distributed.worker - INFO -          dashboard at:          10.6.105.22:32929
2025-09-03 10:52:54,197 - distributed.worker - INFO -          dashboard at:          10.6.105.22:44947
2025-09-03 10:52:54,197 - distributed.worker - INFO -          dashboard at:          10.6.105.22:45043
2025-09-03 10:52:54,197 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,197 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,197 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,197 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,197 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,197 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,197 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,197 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,197 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,197 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,197 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,197 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,197 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,197 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,197 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,197 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,197 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,197 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,197 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,198 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,198 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,198 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,198 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,198 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,198 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,198 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,198 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,198 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,198 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,198 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,198 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,198 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,198 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,198 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,198 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,198 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,198 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,198 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ij8h1gsd
2025-09-03 10:52:54,198 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,198 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,198 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,198 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,198 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,198 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,198 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,198 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,198 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,198 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ahlz4yjl
2025-09-03 10:52:54,198 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,198 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-lwfc9ppq
2025-09-03 10:52:54,198 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,198 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,198 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,198 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,198 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-q_24r3so
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ssv5zvwb
2025-09-03 10:52:54,198 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-moecqk94
2025-09-03 10:52:54,198 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ag22pgv3
2025-09-03 10:52:54,198 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-76gzzvs6
2025-09-03 10:52:54,198 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-mboezk5e
2025-09-03 10:52:54,198 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-jqus6tse
2025-09-03 10:52:54,198 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-rzawcx1k
2025-09-03 10:52:54,198 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ya2saktg
2025-09-03 10:52:54,198 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-5tu4obcn
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-z_jjbjto
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-qqwyc7yj
2025-09-03 10:52:54,198 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-e0llzt5x
2025-09-03 10:52:54,198 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-5sdfdc0n
2025-09-03 10:52:54,198 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-_sm82xil
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,224 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:37933
2025-09-03 10:52:54,224 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:37933
2025-09-03 10:52:54,224 - distributed.worker - INFO -          dashboard at:          10.6.105.22:46139
2025-09-03 10:52:54,224 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,224 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,224 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,224 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,224 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-d7gwsj2d
2025-09-03 10:52:54,224 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,241 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:43813
2025-09-03 10:52:54,241 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:43813
2025-09-03 10:52:54,241 - distributed.worker - INFO -          dashboard at:          10.6.105.22:36989
2025-09-03 10:52:54,241 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,241 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,241 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,241 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,241 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ag7vqo_y
2025-09-03 10:52:54,241 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,258 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:37747
2025-09-03 10:52:54,259 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:37747
2025-09-03 10:52:54,259 - distributed.worker - INFO -          dashboard at:          10.6.105.22:34655
2025-09-03 10:52:54,259 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,259 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,259 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,259 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,259 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-4ozoitc2
2025-09-03 10:52:54,259 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,304 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:41285
2025-09-03 10:52:54,304 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:41285
2025-09-03 10:52:54,304 - distributed.worker - INFO -          dashboard at:          10.6.105.22:40029
2025-09-03 10:52:54,304 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,304 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,304 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,304 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,304 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-zxl0hbzo
2025-09-03 10:52:54,304 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,333 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:42479
2025-09-03 10:52:54,333 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:42479
2025-09-03 10:52:54,333 - distributed.worker - INFO -          dashboard at:          10.6.105.22:36495
2025-09-03 10:52:54,333 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,333 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,333 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,333 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,333 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-vg1vy1ep
2025-09-03 10:52:54,333 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,340 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:34657
2025-09-03 10:52:54,340 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:34657
2025-09-03 10:52:54,340 - distributed.worker - INFO -          dashboard at:          10.6.105.22:39401
2025-09-03 10:52:54,340 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,340 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,340 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,340 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,340 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-qcxlxsxd
2025-09-03 10:52:54,341 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,345 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:43713
2025-09-03 10:52:54,345 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:43713
2025-09-03 10:52:54,345 - distributed.worker - INFO -          dashboard at:          10.6.105.22:33767
2025-09-03 10:52:54,345 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,345 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,345 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,345 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,345 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ppyrdfb3
2025-09-03 10:52:54,345 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,360 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:40773
2025-09-03 10:52:54,360 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:40773
2025-09-03 10:52:54,360 - distributed.worker - INFO -          dashboard at:          10.6.105.22:45037
2025-09-03 10:52:54,360 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,360 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,360 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,360 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,360 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-m6opeem6
2025-09-03 10:52:54,360 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,438 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:33057
2025-09-03 10:52:54,439 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:33057
2025-09-03 10:52:54,439 - distributed.worker - INFO -          dashboard at:          10.6.105.22:38251
2025-09-03 10:52:54,439 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,439 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,439 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,439 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,439 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-g8efp1pw
2025-09-03 10:52:54,439 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,442 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:34369
2025-09-03 10:52:54,442 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:34369
2025-09-03 10:52:54,442 - distributed.worker - INFO -          dashboard at:          10.6.105.22:45781
2025-09-03 10:52:54,442 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,442 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,442 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,442 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,442 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-aflohkh3
2025-09-03 10:52:54,442 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,535 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:43783
2025-09-03 10:52:54,535 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:43783
2025-09-03 10:52:54,535 - distributed.worker - INFO -          dashboard at:          10.6.105.22:33725
2025-09-03 10:52:54,535 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,535 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,535 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,535 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,535 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ouxonow7
2025-09-03 10:52:54,536 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,537 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:38159
2025-09-03 10:52:54,537 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:38159
2025-09-03 10:52:54,537 - distributed.worker - INFO -          dashboard at:          10.6.105.22:32987
2025-09-03 10:52:54,537 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,538 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,538 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,538 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,538 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-de8g7nz3
2025-09-03 10:52:54,538 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,538 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:44513
2025-09-03 10:52:54,538 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:44513
2025-09-03 10:52:54,538 - distributed.worker - INFO -          dashboard at:          10.6.105.22:37269
2025-09-03 10:52:54,538 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,538 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,538 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,538 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,538 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-t8obmlqq
2025-09-03 10:52:54,538 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,613 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:44749
2025-09-03 10:52:54,613 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:44749
2025-09-03 10:52:54,613 - distributed.worker - INFO -          dashboard at:          10.6.105.22:35633
2025-09-03 10:52:54,613 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,613 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,613 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,613 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,613 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-_erizc3g
2025-09-03 10:52:54,613 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,615 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:40665
2025-09-03 10:52:54,615 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:40665
2025-09-03 10:52:54,615 - distributed.worker - INFO -          dashboard at:          10.6.105.22:34737
2025-09-03 10:52:54,615 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,615 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,615 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,615 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,615 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-_hr5_b8p
2025-09-03 10:52:54,615 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,632 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:45359
2025-09-03 10:52:54,632 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:45359
2025-09-03 10:52:54,632 - distributed.worker - INFO -          dashboard at:          10.6.105.22:34639
2025-09-03 10:52:54,632 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,632 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,632 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,632 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,632 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-33j1wbgw
2025-09-03 10:52:54,632 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,671 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:43837
2025-09-03 10:52:54,671 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:43837
2025-09-03 10:52:54,671 - distributed.worker - INFO -          dashboard at:          10.6.105.22:35803
2025-09-03 10:52:54,671 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,671 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,671 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,671 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,671 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ne2815jx
2025-09-03 10:52:54,671 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,714 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:44453
2025-09-03 10:52:54,714 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:44453
2025-09-03 10:52:54,714 - distributed.worker - INFO -          dashboard at:          10.6.105.22:44037
2025-09-03 10:52:54,714 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,714 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,714 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,714 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,714 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-87g3m2f6
2025-09-03 10:52:54,714 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,798 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:38585
2025-09-03 10:52:54,798 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:38585
2025-09-03 10:52:54,798 - distributed.worker - INFO -          dashboard at:          10.6.105.22:33739
2025-09-03 10:52:54,798 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,798 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,798 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,798 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,798 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-yia6xt3y
2025-09-03 10:52:54,798 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,843 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:38381
2025-09-03 10:52:54,843 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:38381
2025-09-03 10:52:54,843 - distributed.worker - INFO -          dashboard at:          10.6.105.22:37611
2025-09-03 10:52:54,843 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,843 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,843 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,843 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,843 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-k9ksla8f
2025-09-03 10:52:54,843 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,915 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:45731
2025-09-03 10:52:54,920 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:45731
2025-09-03 10:52:54,920 - distributed.worker - INFO -          dashboard at:          10.6.105.22:42663
2025-09-03 10:52:54,920 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,920 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,920 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,920 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,920 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ccy6hdjt
2025-09-03 10:52:54,920 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,055 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:37749
2025-09-03 10:52:55,055 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:37749
2025-09-03 10:52:55,055 - distributed.worker - INFO -          dashboard at:          10.6.105.22:42983
2025-09-03 10:52:55,055 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,055 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,055 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,055 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,055 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-471dbw3f
2025-09-03 10:52:55,055 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,062 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:34059
2025-09-03 10:52:55,062 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:34059
2025-09-03 10:52:55,062 - distributed.worker - INFO -          dashboard at:          10.6.105.22:38635
2025-09-03 10:52:55,062 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,062 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,062 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,062 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,062 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-x45f5ohv
2025-09-03 10:52:55,062 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,066 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:39473
2025-09-03 10:52:55,066 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:39473
2025-09-03 10:52:55,066 - distributed.worker - INFO -          dashboard at:          10.6.105.22:46459
2025-09-03 10:52:55,066 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,066 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,066 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,066 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,066 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-oht8c9mb
2025-09-03 10:52:55,066 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,082 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:32791
2025-09-03 10:52:55,082 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:32791
2025-09-03 10:52:55,082 - distributed.worker - INFO -          dashboard at:          10.6.105.22:42149
2025-09-03 10:52:55,083 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,083 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,083 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,083 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,083 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-5nol7ybg
2025-09-03 10:52:55,083 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,099 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:38485
2025-09-03 10:52:55,099 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:38485
2025-09-03 10:52:55,099 - distributed.worker - INFO -          dashboard at:          10.6.105.22:39475
2025-09-03 10:52:55,099 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,099 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,100 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,100 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,100 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-75tazu14
2025-09-03 10:52:55,100 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,125 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:35695
2025-09-03 10:52:55,125 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:35695
2025-09-03 10:52:55,125 - distributed.worker - INFO -          dashboard at:          10.6.105.22:38403
2025-09-03 10:52:55,125 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,125 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,125 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,125 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,125 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-d4o5uj18
2025-09-03 10:52:55,125 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,134 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:41279
2025-09-03 10:52:55,134 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:41279
2025-09-03 10:52:55,134 - distributed.worker - INFO -          dashboard at:          10.6.105.22:41433
2025-09-03 10:52:55,134 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,134 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,134 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,134 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,134 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-0ytl2d4u
2025-09-03 10:52:55,134 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,148 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:35917
2025-09-03 10:52:55,148 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:35917
2025-09-03 10:52:55,149 - distributed.worker - INFO -          dashboard at:          10.6.105.22:35807
2025-09-03 10:52:55,149 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,149 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,149 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,149 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,149 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-3igr997o
2025-09-03 10:52:55,149 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,149 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:33433
2025-09-03 10:52:55,149 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:33433
2025-09-03 10:52:55,149 - distributed.worker - INFO -          dashboard at:          10.6.105.22:41103
2025-09-03 10:52:55,149 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,149 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,149 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,149 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,149 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-n9oiu_oh
2025-09-03 10:52:55,149 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,299 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:40199
2025-09-03 10:52:55,299 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:40199
2025-09-03 10:52:55,299 - distributed.worker - INFO -          dashboard at:          10.6.105.22:46243
2025-09-03 10:52:55,299 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,299 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,299 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,299 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,299 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-go3ry_bn
2025-09-03 10:52:55,299 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,373 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:45869
2025-09-03 10:52:55,373 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:45869
2025-09-03 10:52:55,373 - distributed.worker - INFO -          dashboard at:          10.6.105.22:32861
2025-09-03 10:52:55,373 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,373 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,373 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,373 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,373 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-kdnls92g
2025-09-03 10:52:55,373 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,390 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:41793
2025-09-03 10:52:55,391 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:41793
2025-09-03 10:52:55,391 - distributed.worker - INFO -          dashboard at:          10.6.105.22:45919
2025-09-03 10:52:55,391 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,391 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,391 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,391 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,391 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-i43lqw9t
2025-09-03 10:52:55,391 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,411 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:44409
2025-09-03 10:52:55,411 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:44409
2025-09-03 10:52:55,411 - distributed.worker - INFO -          dashboard at:          10.6.105.22:43593
2025-09-03 10:52:55,411 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,411 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,411 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,411 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,411 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-e32v1nio
2025-09-03 10:52:55,411 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,428 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:40313
2025-09-03 10:52:55,429 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:40313
2025-09-03 10:52:55,429 - distributed.worker - INFO -          dashboard at:          10.6.105.22:34641
2025-09-03 10:52:55,429 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,429 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,429 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,429 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,429 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-3zu6rgj1
2025-09-03 10:52:55,429 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,442 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:44245
2025-09-03 10:52:55,442 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:44245
2025-09-03 10:52:55,442 - distributed.worker - INFO -          dashboard at:          10.6.105.22:38923
2025-09-03 10:52:55,442 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,442 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,442 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,442 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,442 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-n0np6mtb
2025-09-03 10:52:55,442 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,448 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:42333
2025-09-03 10:52:55,448 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:42333
2025-09-03 10:52:55,448 - distributed.worker - INFO -          dashboard at:          10.6.105.22:36487
2025-09-03 10:52:55,449 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,449 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,449 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,449 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,449 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-4b99mt14
2025-09-03 10:52:55,449 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,463 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:43993
2025-09-03 10:52:55,463 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:43993
2025-09-03 10:52:55,463 - distributed.worker - INFO -          dashboard at:          10.6.105.22:43273
2025-09-03 10:52:55,463 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,463 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,463 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,463 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,463 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-tllm35ac
2025-09-03 10:52:55,463 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,465 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:36085
2025-09-03 10:52:55,465 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:36085
2025-09-03 10:52:55,465 - distributed.worker - INFO -          dashboard at:          10.6.105.22:40143
2025-09-03 10:52:55,465 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,465 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,465 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,465 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,465 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-vwg5ojtg
2025-09-03 10:52:55,466 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,540 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:32843
2025-09-03 10:52:55,540 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:32843
2025-09-03 10:52:55,540 - distributed.worker - INFO -          dashboard at:          10.6.105.22:35953
2025-09-03 10:52:55,540 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,540 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,540 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,540 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,540 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-6mvu1yxe
2025-09-03 10:52:55,540 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,540 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:46749
2025-09-03 10:52:55,541 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:46749
2025-09-03 10:52:55,541 - distributed.worker - INFO -          dashboard at:          10.6.105.22:36755
2025-09-03 10:52:55,541 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,541 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,541 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,541 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,541 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-rgyiu9gq
2025-09-03 10:52:55,541 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,560 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:42909
2025-09-03 10:52:55,560 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:42909
2025-09-03 10:52:55,560 - distributed.worker - INFO -          dashboard at:          10.6.105.22:37959
2025-09-03 10:52:55,561 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,561 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,561 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,561 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,561 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-p200vnwy
2025-09-03 10:52:55,561 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,571 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:39865
2025-09-03 10:52:55,571 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:39865
2025-09-03 10:52:55,571 - distributed.worker - INFO -          dashboard at:          10.6.105.22:42327
2025-09-03 10:52:55,571 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,571 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,571 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,571 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,571 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-yya3vnkf
2025-09-03 10:52:55,571 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,588 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:43007
2025-09-03 10:52:55,588 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:43007
2025-09-03 10:52:55,588 - distributed.worker - INFO -          dashboard at:          10.6.105.22:41789
2025-09-03 10:52:55,588 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,588 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,588 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,588 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,588 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-v91lw1oe
2025-09-03 10:52:55,589 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,597 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:33163
2025-09-03 10:52:55,597 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:33163
2025-09-03 10:52:55,597 - distributed.worker - INFO -          dashboard at:          10.6.105.22:34091
2025-09-03 10:52:55,597 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,597 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,597 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,597 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,597 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-aimp97xn
2025-09-03 10:52:55,597 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,603 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:45853
2025-09-03 10:52:55,603 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:45853
2025-09-03 10:52:55,603 - distributed.worker - INFO -          dashboard at:          10.6.105.22:36223
2025-09-03 10:52:55,603 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,603 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,603 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,603 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,604 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-095chgkt
2025-09-03 10:52:55,604 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,605 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:34209
2025-09-03 10:52:55,605 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:34209
2025-09-03 10:52:55,605 - distributed.worker - INFO -          dashboard at:          10.6.105.22:44211
2025-09-03 10:52:55,605 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,605 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,605 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,605 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,605 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-nfxkjxen
2025-09-03 10:52:55,605 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,621 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:42029
2025-09-03 10:52:55,621 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:42029
2025-09-03 10:52:55,621 - distributed.worker - INFO -          dashboard at:          10.6.105.22:39267
2025-09-03 10:52:55,621 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,621 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,621 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,621 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,621 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-b083kli9
2025-09-03 10:52:55,621 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,622 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:36389
2025-09-03 10:52:55,622 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:36389
2025-09-03 10:52:55,622 - distributed.worker - INFO -          dashboard at:          10.6.105.22:45589
2025-09-03 10:52:55,622 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,622 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,622 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,622 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,622 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-z_t4_ijc
2025-09-03 10:52:55,622 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,629 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:43729
2025-09-03 10:52:55,629 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:43729
2025-09-03 10:52:55,629 - distributed.worker - INFO -          dashboard at:          10.6.105.22:46465
2025-09-03 10:52:55,629 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,629 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,629 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,629 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,629 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-uf5u3vys
2025-09-03 10:52:55,629 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,631 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:42841
2025-09-03 10:52:55,631 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:42841
2025-09-03 10:52:55,631 - distributed.worker - INFO -          dashboard at:          10.6.105.22:34973
2025-09-03 10:52:55,631 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,631 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,631 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,631 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,631 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-0is7avmr
2025-09-03 10:52:55,631 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,631 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:43725
2025-09-03 10:52:55,631 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:43725
2025-09-03 10:52:55,631 - distributed.worker - INFO -          dashboard at:          10.6.105.22:40715
2025-09-03 10:52:55,631 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,631 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,631 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,631 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,632 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-o74jw3sh
2025-09-03 10:52:55,632 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,633 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:39809
2025-09-03 10:52:55,633 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:39809
2025-09-03 10:52:55,633 - distributed.worker - INFO -          dashboard at:          10.6.105.22:35979
2025-09-03 10:52:55,633 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,633 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,633 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,633 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,633 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-2g2i3_p5
2025-09-03 10:52:55,633 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,640 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:40763
2025-09-03 10:52:55,640 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:40763
2025-09-03 10:52:55,640 - distributed.worker - INFO -          dashboard at:          10.6.105.22:39497
2025-09-03 10:52:55,640 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,640 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,640 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:38127
2025-09-03 10:52:55,640 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,640 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,640 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:38127
2025-09-03 10:52:55,640 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-t8j23zzc
2025-09-03 10:52:55,640 - distributed.worker - INFO -          dashboard at:          10.6.105.22:34671
2025-09-03 10:52:55,640 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,640 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,640 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,640 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,640 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,640 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-rmn6owdk
2025-09-03 10:52:55,640 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,698 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:40695
2025-09-03 10:52:55,699 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:40695
2025-09-03 10:52:55,699 - distributed.worker - INFO -          dashboard at:          10.6.105.22:42045
2025-09-03 10:52:55,699 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,699 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,699 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,699 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,699 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ld7aqvvy
2025-09-03 10:52:55,699 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,728 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:38083
2025-09-03 10:52:55,728 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:38083
2025-09-03 10:52:55,728 - distributed.worker - INFO -          dashboard at:          10.6.105.22:41835
2025-09-03 10:52:55,728 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,728 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,728 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,728 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,728 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-8hqpwlsp
2025-09-03 10:52:55,729 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,732 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:38275
2025-09-03 10:52:55,732 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:38275
2025-09-03 10:52:55,732 - distributed.worker - INFO -          dashboard at:          10.6.105.22:41647
2025-09-03 10:52:55,732 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,732 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,732 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,732 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,732 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-fpes_om_
2025-09-03 10:52:55,732 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,733 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:44701
2025-09-03 10:52:55,733 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:44701
2025-09-03 10:52:55,733 - distributed.worker - INFO -          dashboard at:          10.6.105.22:34847
2025-09-03 10:52:55,733 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,733 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,733 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,733 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,733 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ctmxstjf
2025-09-03 10:52:55,734 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,831 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:40219
2025-09-03 10:52:55,832 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:40219
2025-09-03 10:52:55,832 - distributed.worker - INFO -          dashboard at:          10.6.105.22:35909
2025-09-03 10:52:55,832 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,832 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,832 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,832 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,832 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-1c1d52tt
2025-09-03 10:52:55,832 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,856 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:34403
2025-09-03 10:52:55,856 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:34403
2025-09-03 10:52:55,856 - distributed.worker - INFO -          dashboard at:          10.6.105.22:45121
2025-09-03 10:52:55,856 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,856 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,856 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,856 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,856 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-xb5r7l86
2025-09-03 10:52:55,856 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,858 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:35835
2025-09-03 10:52:55,858 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:35835
2025-09-03 10:52:55,858 - distributed.worker - INFO -          dashboard at:          10.6.105.22:46387
2025-09-03 10:52:55,858 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,858 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,858 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,858 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,858 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-izhx3_ub
2025-09-03 10:52:55,858 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,865 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:33993
2025-09-03 10:52:55,865 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:33993
2025-09-03 10:52:55,865 - distributed.worker - INFO -          dashboard at:          10.6.105.22:44505
2025-09-03 10:52:55,865 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,865 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,865 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,865 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,865 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-dx3_ydeo
2025-09-03 10:52:55,865 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,924 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:36263
2025-09-03 10:52:55,924 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:36263
2025-09-03 10:52:55,924 - distributed.worker - INFO -          dashboard at:          10.6.105.22:42723
2025-09-03 10:52:55,924 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,924 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,924 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,924 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,924 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-u0955pek
2025-09-03 10:52:55,924 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,955 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:36367
2025-09-03 10:52:55,955 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:36367
2025-09-03 10:52:55,955 - distributed.worker - INFO -          dashboard at:          10.6.105.22:44741
2025-09-03 10:52:55,955 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,955 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,955 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,955 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,955 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-56qxq5o7
2025-09-03 10:52:55,955 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,956 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:46491
2025-09-03 10:52:55,956 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:46491
2025-09-03 10:52:55,956 - distributed.worker - INFO -          dashboard at:          10.6.105.22:35443
2025-09-03 10:52:55,956 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,956 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,957 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,957 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,957 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-giha2ltl
2025-09-03 10:52:55,957 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,958 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:40817
2025-09-03 10:52:55,958 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:40817
2025-09-03 10:52:55,958 - distributed.worker - INFO -          dashboard at:          10.6.105.22:46449
2025-09-03 10:52:55,958 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,958 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,958 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,958 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,958 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-1j7ei3jq
2025-09-03 10:52:55,958 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,971 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:43683
2025-09-03 10:52:55,971 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:43683
2025-09-03 10:52:55,971 - distributed.worker - INFO -          dashboard at:          10.6.105.22:45093
2025-09-03 10:52:55,971 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,971 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,971 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,971 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,971 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-wyp9js7l
2025-09-03 10:52:55,971 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,975 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:39647
2025-09-03 10:52:55,975 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:39647
2025-09-03 10:52:55,975 - distributed.worker - INFO -          dashboard at:          10.6.105.22:40095
2025-09-03 10:52:55,975 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,975 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,975 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,975 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,975 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-taj4jtw3
2025-09-03 10:52:55,975 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,979 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:55,980 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,981 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,982 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:55,996 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:55,997 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,997 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,999 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,012 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,013 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,013 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,013 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:33881
2025-09-03 10:52:56,014 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:33881
2025-09-03 10:52:56,014 - distributed.worker - INFO -          dashboard at:          10.6.105.22:39925
2025-09-03 10:52:56,014 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,014 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,014 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,014 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,014 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-fgajuxiy
2025-09-03 10:52:56,014 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,015 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,028 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,030 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,030 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,031 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,045 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,046 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,046 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,048 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,060 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:39817
2025-09-03 10:52:56,060 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:39817
2025-09-03 10:52:56,060 - distributed.worker - INFO -          dashboard at:          10.6.105.22:44449
2025-09-03 10:52:56,060 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,060 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,060 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,061 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,061 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-shyzgir9
2025-09-03 10:52:56,061 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,061 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,062 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,062 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,064 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,078 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,079 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,079 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,081 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,089 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:36683
2025-09-03 10:52:56,090 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:36683
2025-09-03 10:52:56,090 - distributed.worker - INFO -          dashboard at:          10.6.105.22:42613
2025-09-03 10:52:56,090 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,090 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,090 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,090 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,090 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-iqgxdgy2
2025-09-03 10:52:56,090 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,094 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,095 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,095 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,097 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,108 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:39153
2025-09-03 10:52:56,109 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:39153
2025-09-03 10:52:56,109 - distributed.worker - INFO -          dashboard at:          10.6.105.22:38539
2025-09-03 10:52:56,109 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,109 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,109 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,109 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,109 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-i0hv8uw4
2025-09-03 10:52:56,109 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,109 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:45217
2025-09-03 10:52:56,109 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:45217
2025-09-03 10:52:56,109 - distributed.worker - INFO -          dashboard at:          10.6.105.22:43985
2025-09-03 10:52:56,109 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,109 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,109 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:39815
2025-09-03 10:52:56,109 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,109 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:39815
2025-09-03 10:52:56,109 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,109 - distributed.worker - INFO -          dashboard at:          10.6.105.22:34881
2025-09-03 10:52:56,109 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-318tw2cg
2025-09-03 10:52:56,109 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,109 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,109 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,109 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,109 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,109 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-uh325uy_
2025-09-03 10:52:56,109 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,110 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:42543
2025-09-03 10:52:56,110 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,110 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:42543
2025-09-03 10:52:56,110 - distributed.worker - INFO -          dashboard at:          10.6.105.22:36349
2025-09-03 10:52:56,110 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,110 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,110 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,110 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,111 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-9yxyw8ke
2025-09-03 10:52:56,111 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,111 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,111 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,113 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,114 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:41291
2025-09-03 10:52:56,114 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:41291
2025-09-03 10:52:56,114 - distributed.worker - INFO -          dashboard at:          10.6.105.22:33335
2025-09-03 10:52:56,114 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,114 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,114 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,114 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,114 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-jbp6ewmk
2025-09-03 10:52:56,114 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,117 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:44443
2025-09-03 10:52:56,117 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:44443
2025-09-03 10:52:56,117 - distributed.worker - INFO -          dashboard at:          10.6.105.22:45045
2025-09-03 10:52:56,117 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,117 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,117 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,117 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,117 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-v9f90w95
2025-09-03 10:52:56,117 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,126 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,127 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,128 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,129 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,134 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:33201
2025-09-03 10:52:56,134 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:33201
2025-09-03 10:52:56,134 - distributed.worker - INFO -          dashboard at:          10.6.105.22:36599
2025-09-03 10:52:56,134 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,134 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,134 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,134 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,134 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-s7961vqf
2025-09-03 10:52:56,134 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,134 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:43675
2025-09-03 10:52:56,134 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:43675
2025-09-03 10:52:56,134 - distributed.worker - INFO -          dashboard at:          10.6.105.22:36643
2025-09-03 10:52:56,134 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,134 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,134 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,134 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,134 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-u51lxscw
2025-09-03 10:52:56,134 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,141 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:36205
2025-09-03 10:52:56,141 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:36205
2025-09-03 10:52:56,141 - distributed.worker - INFO -          dashboard at:          10.6.105.22:45739
2025-09-03 10:52:56,141 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,141 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,141 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,141 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,141 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-imzf700s
2025-09-03 10:52:56,141 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,143 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,144 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,144 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,146 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,159 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,161 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,161 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,162 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,176 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,177 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,177 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,178 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,192 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,193 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,193 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,195 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,208 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,209 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,209 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,211 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,225 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,226 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,226 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,228 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,241 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,242 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,242 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,244 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,257 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,258 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,259 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,260 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,290 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,292 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,292 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,293 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,373 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,374 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,374 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,376 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,406 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,407 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,407 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,408 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,439 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,440 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,440 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,442 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,505 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,506 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,506 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,508 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,964 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,965 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,965 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,967 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,996 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,998 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,998 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,000 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,095 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,097 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,097 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,099 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,883 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,884 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,884 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,886 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,967 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,968 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,968 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,970 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,983 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,984 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,984 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,986 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,999 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,000 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,001 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,002 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,115 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,117 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,117 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,119 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,132 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,133 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,134 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,136 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,149 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,150 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,150 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,152 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,166 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,167 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,167 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,169 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,232 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,234 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,234 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,236 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,266 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,267 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,267 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,269 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,282 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,283 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,283 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,285 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,298 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,300 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,300 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,302 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,317 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,319 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,319 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,321 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,334 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,335 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,335 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,337 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,536 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,537 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,537 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,539 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,552 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,553 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,553 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,555 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,569 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,571 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,571 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,573 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,586 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,587 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,587 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,589 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,603 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,604 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,604 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,606 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,619 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,620 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,620 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,622 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,636 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,637 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,637 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,639 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,653 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,654 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,654 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,656 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,669 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,671 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,671 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,673 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,686 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,687 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,687 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,689 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:59,164 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:59,168 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:59,168 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:59,174 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:59,283 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:59,284 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:59,285 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:59,287 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:59,300 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:59,301 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:59,301 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:59,303 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:59,368 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:59,370 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:59,370 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:59,372 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:00,376 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:00,377 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:00,377 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:00,379 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:00,393 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:00,394 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:00,394 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:00,396 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:00,410 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:00,412 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:00,412 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:00,414 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:03,179 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:03,181 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:03,181 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:03,183 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:03,212 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:03,213 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:03,213 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:03,215 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:03,245 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:03,246 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:03,246 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:03,248 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:03,327 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:03,329 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:03,329 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:03,331 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:03,344 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:03,346 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:03,346 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:03,348 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:03,377 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:03,378 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:03,379 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:03,380 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:03,380 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:41293'
2025-09-03 10:53:03,389 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:40527'
2025-09-03 10:53:03,392 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:42027'
2025-09-03 10:53:03,394 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:33391'
2025-09-03 10:53:03,395 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:03,396 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:03,397 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:03,398 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:03,401 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.22:42949'
2025-09-03 10:53:03,746 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:03,748 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:03,748 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:03,749 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:04,235 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:37139
2025-09-03 10:53:04,236 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:37139
2025-09-03 10:53:04,236 - distributed.worker - INFO -          dashboard at:          10.6.105.22:46135
2025-09-03 10:53:04,236 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:04,236 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:04,236 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:04,236 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:04,236 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-03mzy05a
2025-09-03 10:53:04,236 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:04,236 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:34849
2025-09-03 10:53:04,236 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:34849
2025-09-03 10:53:04,236 - distributed.worker - INFO -          dashboard at:          10.6.105.22:42669
2025-09-03 10:53:04,236 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:04,236 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:04,236 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:04,236 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:04,236 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-f6_w513x
2025-09-03 10:53:04,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:04,238 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:46319
2025-09-03 10:53:04,238 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:46319
2025-09-03 10:53:04,238 - distributed.worker - INFO -          dashboard at:          10.6.105.22:45479
2025-09-03 10:53:04,238 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:04,238 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:04,238 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:04,238 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:04,238 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-72su4oco
2025-09-03 10:53:04,238 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:04,241 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:42581
2025-09-03 10:53:04,241 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:42581
2025-09-03 10:53:04,242 - distributed.worker - INFO -          dashboard at:          10.6.105.22:42371
2025-09-03 10:53:04,242 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:04,242 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:04,242 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:04,242 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:04,242 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-y9dab015
2025-09-03 10:53:04,242 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:04,283 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.22:36585
2025-09-03 10:53:04,283 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.22:36585
2025-09-03 10:53:04,283 - distributed.worker - INFO -          dashboard at:          10.6.105.22:42053
2025-09-03 10:53:04,283 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:04,283 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:04,283 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:53:04,283 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:53:04,283 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-isgfut33
2025-09-03 10:53:04,283 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:04,855 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:04,857 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:04,857 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:04,859 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:04,873 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:04,876 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:04,876 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:04,879 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:07,240 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:07,242 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:07,242 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:07,243 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:07,256 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:07,258 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:07,258 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:07,260 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:07,273 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:07,275 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:07,275 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:07,277 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:07,291 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:07,293 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:07,294 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:07,297 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:07,306 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:07,307 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:07,307 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:07,309 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:09,149 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:09,151 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:09,151 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:09,153 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:09,166 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:09,168 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:09,168 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:09,169 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:09,333 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:09,334 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:09,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:09,336 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:09,350 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:09,351 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:09,351 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:09,353 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:09,366 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:09,367 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:09,368 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:09,369 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:10,900 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:10,901 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:10,902 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:10,903 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:10,917 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:10,919 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:10,919 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:10,921 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:10,934 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:10,936 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:10,936 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:10,938 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:10,952 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:10,953 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:10,953 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:10,955 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:24,799 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:24,844 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:24,921 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,150 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,150 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,430 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,443 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,450 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,464 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,467 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,700 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,729 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,733 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,735 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,976 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:26,015 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:26,984 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:27,001 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:27,018 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:27,114 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:27,510 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:28,502 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:28,504 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:28,504 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:28,506 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:28,520 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:28,521 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:28,522 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:28,523 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:28,973 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:28,989 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:29,004 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:29,543 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:29,556 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:32,768 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:32,770 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:32,770 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:32,772 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:32,786 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:32,787 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:32,787 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:32,789 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:34,108 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:34,109 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:34,109 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:34,111 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:34,125 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:34,127 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:34,127 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:34,129 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:34,143 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:34,144 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:34,144 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:34,146 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:34,160 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:34,161 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:34,161 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:34,163 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:34,177 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:34,179 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:34,179 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:34,180 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:34,237 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:34,238 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:34,240 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:34,243 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:34,251 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:34,284 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:35,861 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:35,880 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:39,659 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:39,661 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:39,661 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:39,663 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:39,678 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:39,679 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:39,680 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:39,681 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:39,696 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:39,698 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:39,698 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:39,700 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:39,715 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:39,716 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:39,717 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:39,718 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:39,733 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:39,735 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:39,735 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:39,736 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:40,154 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:40,172 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:40,337 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:40,355 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:40,372 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:40,580 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:40,581 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:40,581 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:40,583 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:40,604 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:40,606 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:40,606 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:40,608 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:46,619 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:46,621 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:46,621 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:46,623 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:46,637 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:46,638 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:46,638 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:46,640 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:47,203 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:47,205 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:47,205 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:47,207 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:47,225 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:47,226 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:47,227 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:47,228 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:47,244 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:47,245 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:47,245 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:47,247 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:48,056 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:48,057 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:48,057 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:48,059 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:48,074 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:48,075 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:48,075 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:48,077 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:56:37,813 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:37,821 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,448 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,450 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,495 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,506 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,623 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,625 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,297 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,299 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,372 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,377 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,448 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,451 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,475 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,476 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,609 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,611 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,639 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,642 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,134 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,136 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,235 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,236 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,241 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,241 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,254 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,255 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,306 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,310 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,387 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,390 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,527 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,534 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,633 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,635 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,325 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,328 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,339 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,345 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,407 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,412 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,653 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,658 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,213 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,214 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,349 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,350 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,386 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,387 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,466 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,473 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,674 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,677 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,106 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,107 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,163 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,164 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,529 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,530 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,627 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,633 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,706 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,712 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,752 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,754 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,861 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,862 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,958 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,960 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,169 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,174 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,501 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,502 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,572 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,577 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,636 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,646 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,805 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,811 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,865 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,866 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,959 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,960 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,252 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,255 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,747 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,752 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,852 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,853 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,866 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,871 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,023 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,024 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,563 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,568 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,891 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,893 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,083 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,085 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,187 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,192 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,244 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,245 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,411 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,412 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,639 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,640 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,687 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,693 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,695 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,696 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,837 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,842 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,869 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,870 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,998 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,001 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,401 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,403 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,429 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,434 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,479 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,482 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,650 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,652 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,699 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,701 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,016 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,022 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,270 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,272 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,325 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,330 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,799 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,800 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,007 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,009 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,150 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,155 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,188 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,193 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,198 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,204 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,243 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,244 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,283 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,285 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,619 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,621 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,151 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,156 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,248 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,254 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,683 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,685 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,802 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,807 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,170 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,176 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,687 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,688 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,764 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,765 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:53,605 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:53,607 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:54,267 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:54,269 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:54,638 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:54,647 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:54,874 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:54,875 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:55,242 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:55,243 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:58,883 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:58,886 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:59,710 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:59,714 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:00,184 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:00,189 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:00,817 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:00,819 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:00,827 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:00,828 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:01,083 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:01,089 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:01,204 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:01,207 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:01,603 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:01,608 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:01,981 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:01,982 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:02,013 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:02,014 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:02,974 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:02,976 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:03,623 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:03,628 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:03,849 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:03,852 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:04,121 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:04,126 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:04,164 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:04,171 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:04,682 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:04,687 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:04,862 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:04,868 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:09,992 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:09,994 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,002 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,004 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,018 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,020 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,075 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,077 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,332 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,334 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,331 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,337 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,339 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,339 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,341 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,342 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,343 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,344 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,343 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,346 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,355 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,357 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,359 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,361 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,361 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,366 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,387 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,396 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,500 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,502 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,502 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,504 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,504 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,506 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,506 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,508 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,516 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,517 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,518 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,519 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,522 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,525 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,531 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,536 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,544 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,550 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,644 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,646 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,658 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,661 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,666 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,668 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,745 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,747 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,749 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,748 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,751 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,753 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,807 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,812 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,879 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,881 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,887 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,893 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,897 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,899 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,920 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,922 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,932 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,934 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,107 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,110 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,112 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,115 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,118 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,120 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,163 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,165 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,181 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,183 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,265 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,268 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,297 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,305 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,369 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,368 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,371 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,373 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,384 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,390 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,391 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,393 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,395 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,397 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,400 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,405 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,409 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,411 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,421 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,423 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,490 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,491 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,492 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,493 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,493 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,496 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,514 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,516 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,516 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,519 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,545 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,548 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,556 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,558 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,566 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,569 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,591 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,593 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,633 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,635 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,645 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,647 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,679 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,681 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,686 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,688 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,765 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,767 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,766 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,771 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,853 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,855 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,877 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,885 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,895 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,897 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,987 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,992 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,034 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,036 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,063 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,065 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,072 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,077 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,079 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,080 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,114 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,116 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,150 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,152 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,260 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,262 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,310 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,314 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,335 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,337 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,365 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,366 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,368 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,368 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,383 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,385 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,492 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,494 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,610 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,613 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,614 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,617 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,620 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,625 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,765 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,768 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,780 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,782 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,846 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,848 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,849 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,851 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,865 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,867 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,880 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,884 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,952 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,954 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,969 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,971 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,980 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,984 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,022 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,025 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,036 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,040 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,043 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,046 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,156 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,161 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,243 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,247 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,376 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,378 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,396 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,398 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:14,356 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:14,358 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:15,123 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:15,125 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:15,417 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:15,425 - distributed.utils - INFO - Reload module qme_vars from .py file
