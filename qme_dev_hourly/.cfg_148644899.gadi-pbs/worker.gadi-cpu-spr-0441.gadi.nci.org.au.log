Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-03 16:27:01,775 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:37191'
2025-09-03 16:27:01,788 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:33599'
2025-09-03 16:27:01,792 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:36415'
2025-09-03 16:27:01,796 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:43967'
2025-09-03 16:27:01,801 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:43009'
2025-09-03 16:27:01,805 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:39927'
2025-09-03 16:27:01,809 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:34301'
2025-09-03 16:27:01,813 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:44533'
2025-09-03 16:27:01,818 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:46321'
2025-09-03 16:27:01,824 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:46507'
2025-09-03 16:27:01,828 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:42087'
2025-09-03 16:27:01,832 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:45577'
2025-09-03 16:27:01,836 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:39649'
2025-09-03 16:27:01,840 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:34445'
2025-09-03 16:27:01,844 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:34197'
2025-09-03 16:27:01,848 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:38769'
2025-09-03 16:27:01,851 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:45297'
2025-09-03 16:27:01,854 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:37637'
2025-09-03 16:27:01,858 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:37269'
2025-09-03 16:27:01,862 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:44889'
2025-09-03 16:27:01,949 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:40585'
2025-09-03 16:27:01,954 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:41395'
2025-09-03 16:27:01,958 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:41983'
2025-09-03 16:27:01,963 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:40193'
2025-09-03 16:27:01,968 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:42415'
2025-09-03 16:27:01,971 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:32815'
2025-09-03 16:27:01,976 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:34029'
2025-09-03 16:27:01,981 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:40857'
2025-09-03 16:27:01,986 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:36805'
2025-09-03 16:27:01,991 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:38479'
2025-09-03 16:27:01,995 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:39885'
2025-09-03 16:27:02,000 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:42937'
2025-09-03 16:27:02,004 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:45877'
2025-09-03 16:27:02,009 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:42005'
2025-09-03 16:27:02,014 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:40475'
2025-09-03 16:27:02,019 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:34903'
2025-09-03 16:27:02,023 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:37733'
2025-09-03 16:27:02,027 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:46543'
2025-09-03 16:27:02,031 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:41913'
2025-09-03 16:27:02,036 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:38219'
2025-09-03 16:27:02,040 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:45783'
2025-09-03 16:27:02,043 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:35123'
2025-09-03 16:27:02,048 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:43583'
2025-09-03 16:27:02,053 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:42855'
2025-09-03 16:27:02,057 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:38907'
2025-09-03 16:27:02,061 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:40963'
2025-09-03 16:27:02,066 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:42031'
2025-09-03 16:27:02,070 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:39917'
2025-09-03 16:27:02,073 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:38555'
2025-09-03 16:27:02,078 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:46153'
2025-09-03 16:27:02,082 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:35493'
2025-09-03 16:27:02,088 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.9:39277'
2025-09-03 16:27:02,939 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:38115
2025-09-03 16:27:02,939 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:36195
2025-09-03 16:27:02,939 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:38115
2025-09-03 16:27:02,939 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:36195
2025-09-03 16:27:02,939 - distributed.worker - INFO -          dashboard at:           10.6.102.9:46207
2025-09-03 16:27:02,939 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:02,939 - distributed.worker - INFO -          dashboard at:           10.6.102.9:35369
2025-09-03 16:27:02,939 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:44367
2025-09-03 16:27:02,939 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,939 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:02,939 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:02,939 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,939 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:44367
2025-09-03 16:27:02,939 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:33355
2025-09-03 16:27:02,939 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:02,939 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:02,939 - distributed.worker - INFO -          dashboard at:           10.6.102.9:38885
2025-09-03 16:27:02,939 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:02,939 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:33355
2025-09-03 16:27:02,939 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-_fuzwf8i
2025-09-03 16:27:02,939 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:02,939 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-z_yqgyg3
2025-09-03 16:27:02,939 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,939 - distributed.worker - INFO -          dashboard at:           10.6.102.9:40105
2025-09-03 16:27:02,939 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,939 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:02,939 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,939 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:02,939 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:02,939 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,939 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-jhc505me
2025-09-03 16:27:02,939 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:02,939 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:02,939 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,939 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-9r83j6yq
2025-09-03 16:27:02,939 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,952 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:46327
2025-09-03 16:27:02,952 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:46327
2025-09-03 16:27:02,952 - distributed.worker - INFO -          dashboard at:           10.6.102.9:40321
2025-09-03 16:27:02,952 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:02,952 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,952 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:02,952 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:02,952 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-y4el7dz9
2025-09-03 16:27:02,952 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,955 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:02,955 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:02,955 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,956 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:02,957 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:42203
2025-09-03 16:27:02,957 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:42203
2025-09-03 16:27:02,957 - distributed.worker - INFO -          dashboard at:           10.6.102.9:35517
2025-09-03 16:27:02,957 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:02,957 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,957 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:02,957 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:02,957 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-kv_vnsh3
2025-09-03 16:27:02,957 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,958 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:02,958 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:02,958 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,959 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:02,962 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:02,962 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:43061
2025-09-03 16:27:02,962 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:43061
2025-09-03 16:27:02,963 - distributed.worker - INFO -          dashboard at:           10.6.102.9:40081
2025-09-03 16:27:02,963 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:02,963 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,963 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:02,963 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:02,963 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-0dzs7qk7
2025-09-03 16:27:02,963 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,963 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:02,963 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,964 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:02,966 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:02,967 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:02,967 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,968 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:02,978 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:02,978 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:36301
2025-09-03 16:27:02,979 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:36301
2025-09-03 16:27:02,979 - distributed.worker - INFO -          dashboard at:           10.6.102.9:39411
2025-09-03 16:27:02,979 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:02,979 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,979 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:02,979 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:02,979 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-hx20k20g
2025-09-03 16:27:02,979 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:02,979 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,979 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,980 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:02,981 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:36923
2025-09-03 16:27:02,981 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:36923
2025-09-03 16:27:02,981 - distributed.worker - INFO -          dashboard at:           10.6.102.9:41813
2025-09-03 16:27:02,981 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:02,981 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,981 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:02,981 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:02,981 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-5pztk6rn
2025-09-03 16:27:02,981 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,981 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:02,982 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:02,982 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,984 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:02,984 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:02,984 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:02,985 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,986 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:02,992 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:40255
2025-09-03 16:27:02,992 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:40255
2025-09-03 16:27:02,992 - distributed.worker - INFO -          dashboard at:           10.6.102.9:39161
2025-09-03 16:27:02,992 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:02,992 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,992 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:02,992 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:02,992 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-xv2gkhrg
2025-09-03 16:27:02,992 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,993 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:40763
2025-09-03 16:27:02,994 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:40763
2025-09-03 16:27:02,994 - distributed.worker - INFO -          dashboard at:           10.6.102.9:34157
2025-09-03 16:27:02,994 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:02,994 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,994 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:02,994 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:02,994 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-a3dxn6ge
2025-09-03 16:27:02,994 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,996 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:34307
2025-09-03 16:27:02,996 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:34307
2025-09-03 16:27:02,996 - distributed.worker - INFO -          dashboard at:           10.6.102.9:33127
2025-09-03 16:27:02,996 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:02,996 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,996 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:02,996 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:02,996 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-rxhbqun0
2025-09-03 16:27:02,996 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,997 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:42959
2025-09-03 16:27:02,997 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:42959
2025-09-03 16:27:02,997 - distributed.worker - INFO -          dashboard at:           10.6.102.9:41245
2025-09-03 16:27:02,997 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:02,997 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:02,998 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:02,998 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:02,998 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-gu_f1hjk
2025-09-03 16:27:02,998 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,001 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,001 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,001 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,003 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,005 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:33691
2025-09-03 16:27:03,005 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:33691
2025-09-03 16:27:03,005 - distributed.worker - INFO -          dashboard at:           10.6.102.9:36839
2025-09-03 16:27:03,005 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,005 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,005 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,005 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,005 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-giq6dwhf
2025-09-03 16:27:03,005 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,006 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,007 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,007 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,007 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,008 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,008 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,008 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,009 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,009 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,010 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,010 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,010 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,011 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:36005
2025-09-03 16:27:03,011 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:36005
2025-09-03 16:27:03,011 - distributed.worker - INFO -          dashboard at:           10.6.102.9:43377
2025-09-03 16:27:03,011 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,011 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,011 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,011 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,011 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-uu3o2fj1
2025-09-03 16:27:03,011 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,013 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,013 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,013 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,013 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:35941
2025-09-03 16:27:03,013 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:35941
2025-09-03 16:27:03,013 - distributed.worker - INFO -          dashboard at:           10.6.102.9:40425
2025-09-03 16:27:03,013 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,013 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,013 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,014 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,014 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-s72z7apw
2025-09-03 16:27:03,014 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,014 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,015 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:44175
2025-09-03 16:27:03,015 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:44175
2025-09-03 16:27:03,015 - distributed.worker - INFO -          dashboard at:           10.6.102.9:33999
2025-09-03 16:27:03,015 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,015 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,015 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,015 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,015 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-zh4turj3
2025-09-03 16:27:03,015 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,016 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,017 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,018 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,019 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,022 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:33699
2025-09-03 16:27:03,022 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:33699
2025-09-03 16:27:03,022 - distributed.worker - INFO -          dashboard at:           10.6.102.9:46835
2025-09-03 16:27:03,022 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,022 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,022 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,022 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,022 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-0kawx7n0
2025-09-03 16:27:03,022 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,023 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:33465
2025-09-03 16:27:03,023 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:33465
2025-09-03 16:27:03,023 - distributed.worker - INFO -          dashboard at:           10.6.102.9:37979
2025-09-03 16:27:03,023 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,023 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,023 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,023 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,023 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-0rzpux9h
2025-09-03 16:27:03,023 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,028 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,029 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,029 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,031 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,032 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,033 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,033 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,033 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,033 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,033 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,034 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,035 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,035 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,036 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,036 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,037 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,037 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,038 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,038 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,039 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,041 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,041 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,041 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,043 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,056 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:38353
2025-09-03 16:27:03,057 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:38353
2025-09-03 16:27:03,057 - distributed.worker - INFO -          dashboard at:           10.6.102.9:39887
2025-09-03 16:27:03,057 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,057 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,057 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,057 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,057 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-xan2bkqw
2025-09-03 16:27:03,057 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,079 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,080 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,080 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,081 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,104 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:43619
2025-09-03 16:27:03,104 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:43619
2025-09-03 16:27:03,104 - distributed.worker - INFO -          dashboard at:           10.6.102.9:35677
2025-09-03 16:27:03,104 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,104 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,104 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,104 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,104 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-eyags1lr
2025-09-03 16:27:03,104 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,130 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:40015
2025-09-03 16:27:03,130 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:40015
2025-09-03 16:27:03,130 - distributed.worker - INFO -          dashboard at:           10.6.102.9:46489
2025-09-03 16:27:03,130 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,130 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,130 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,130 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,130 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-o32j84ax
2025-09-03 16:27:03,130 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,137 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,138 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,139 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,140 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,146 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,147 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,147 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,147 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,152 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:38603
2025-09-03 16:27:03,152 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:38603
2025-09-03 16:27:03,152 - distributed.worker - INFO -          dashboard at:           10.6.102.9:39381
2025-09-03 16:27:03,152 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,152 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,152 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,152 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,153 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-hlp5k_3q
2025-09-03 16:27:03,153 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,160 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:38451
2025-09-03 16:27:03,160 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:38451
2025-09-03 16:27:03,160 - distributed.worker - INFO -          dashboard at:           10.6.102.9:45479
2025-09-03 16:27:03,160 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,160 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,160 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,161 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,161 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-wnayrciw
2025-09-03 16:27:03,161 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,173 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,173 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,174 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,175 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,181 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,182 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,182 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,183 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,324 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:39501
2025-09-03 16:27:03,324 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:39501
2025-09-03 16:27:03,324 - distributed.worker - INFO -          dashboard at:           10.6.102.9:36303
2025-09-03 16:27:03,324 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,324 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,324 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,324 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,324 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-id4gm530
2025-09-03 16:27:03,324 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,346 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,347 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,348 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,349 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,351 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:45839
2025-09-03 16:27:03,352 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:45839
2025-09-03 16:27:03,352 - distributed.worker - INFO -          dashboard at:           10.6.102.9:46001
2025-09-03 16:27:03,352 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,352 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,352 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,352 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,352 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-s3l1exvn
2025-09-03 16:27:03,352 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,353 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:39727
2025-09-03 16:27:03,353 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:39727
2025-09-03 16:27:03,353 - distributed.worker - INFO -          dashboard at:           10.6.102.9:35115
2025-09-03 16:27:03,353 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,353 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,353 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,353 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:36179
2025-09-03 16:27:03,353 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,353 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:36179
2025-09-03 16:27:03,353 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-88patgiw
2025-09-03 16:27:03,354 - distributed.worker - INFO -          dashboard at:           10.6.102.9:33107
2025-09-03 16:27:03,354 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,354 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,354 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,354 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,354 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,354 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-eq08guow
2025-09-03 16:27:03,354 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,357 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:41413
2025-09-03 16:27:03,357 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:41413
2025-09-03 16:27:03,357 - distributed.worker - INFO -          dashboard at:           10.6.102.9:34389
2025-09-03 16:27:03,357 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,357 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,357 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,357 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,357 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-mw8iehme
2025-09-03 16:27:03,357 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,357 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:45023
2025-09-03 16:27:03,357 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:45023
2025-09-03 16:27:03,357 - distributed.worker - INFO -          dashboard at:           10.6.102.9:40019
2025-09-03 16:27:03,357 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,357 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,357 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,357 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,357 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-15njyxq_
2025-09-03 16:27:03,357 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,364 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:40101
2025-09-03 16:27:03,364 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:40101
2025-09-03 16:27:03,364 - distributed.worker - INFO -          dashboard at:           10.6.102.9:36417
2025-09-03 16:27:03,364 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,364 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,364 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,364 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,364 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-2aia0rj4
2025-09-03 16:27:03,364 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,365 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:33689
2025-09-03 16:27:03,365 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:33689
2025-09-03 16:27:03,365 - distributed.worker - INFO -          dashboard at:           10.6.102.9:38809
2025-09-03 16:27:03,365 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,365 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,366 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,366 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,366 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-tzzvtxr8
2025-09-03 16:27:03,366 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,368 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,368 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,368 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,369 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,373 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,374 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:36513
2025-09-03 16:27:03,374 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:35197
2025-09-03 16:27:03,374 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:36513
2025-09-03 16:27:03,374 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:35197
2025-09-03 16:27:03,374 - distributed.worker - INFO -          dashboard at:           10.6.102.9:44825
2025-09-03 16:27:03,374 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:43987
2025-09-03 16:27:03,374 - distributed.worker - INFO -          dashboard at:           10.6.102.9:33041
2025-09-03 16:27:03,374 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,374 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,374 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,374 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:43987
2025-09-03 16:27:03,374 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,374 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,374 - distributed.worker - INFO -          dashboard at:           10.6.102.9:36909
2025-09-03 16:27:03,374 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,374 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,374 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,374 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,374 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-_2922fb2
2025-09-03 16:27:03,374 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,374 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-dfrrhqju
2025-09-03 16:27:03,374 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,374 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,374 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,374 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,374 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-2zkipyog
2025-09-03 16:27:03,374 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,374 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,374 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,376 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,376 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,377 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,377 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,377 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:41781
2025-09-03 16:27:03,377 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:41781
2025-09-03 16:27:03,377 - distributed.worker - INFO -          dashboard at:           10.6.102.9:34143
2025-09-03 16:27:03,377 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,377 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,377 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,377 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,377 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-nbux_g_z
2025-09-03 16:27:03,377 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,378 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,378 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,379 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,379 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,381 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,381 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,381 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:34405
2025-09-03 16:27:03,381 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:34405
2025-09-03 16:27:03,381 - distributed.worker - INFO -          dashboard at:           10.6.102.9:41255
2025-09-03 16:27:03,381 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,381 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,381 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,381 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,381 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-k5vt7c83
2025-09-03 16:27:03,382 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,382 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:43291
2025-09-03 16:27:03,382 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:43291
2025-09-03 16:27:03,382 - distributed.worker - INFO -          dashboard at:           10.6.102.9:36803
2025-09-03 16:27:03,382 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,382 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,382 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,382 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,382 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,382 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-h48e6_od
2025-09-03 16:27:03,382 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,382 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,384 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,387 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,388 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,388 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:44215
2025-09-03 16:27:03,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,388 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:44215
2025-09-03 16:27:03,388 - distributed.worker - INFO -          dashboard at:           10.6.102.9:33467
2025-09-03 16:27:03,388 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,389 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,389 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,389 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-ip61ga8l
2025-09-03 16:27:03,389 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,389 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:41203
2025-09-03 16:27:03,389 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:41203
2025-09-03 16:27:03,389 - distributed.worker - INFO -          dashboard at:           10.6.102.9:38869
2025-09-03 16:27:03,389 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,389 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,389 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,389 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,389 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-vvnoylrk
2025-09-03 16:27:03,389 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,390 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,390 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,391 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,391 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,392 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,395 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:33061
2025-09-03 16:27:03,395 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:33061
2025-09-03 16:27:03,395 - distributed.worker - INFO -          dashboard at:           10.6.102.9:39677
2025-09-03 16:27:03,395 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,395 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,395 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,395 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,395 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-cyfgabj8
2025-09-03 16:27:03,395 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,401 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:41591
2025-09-03 16:27:03,401 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:41591
2025-09-03 16:27:03,401 - distributed.worker - INFO -          dashboard at:           10.6.102.9:38035
2025-09-03 16:27:03,401 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,401 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,402 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,402 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,402 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-vugmuby2
2025-09-03 16:27:03,402 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,402 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:42021
2025-09-03 16:27:03,402 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:42021
2025-09-03 16:27:03,402 - distributed.worker - INFO -          dashboard at:           10.6.102.9:44431
2025-09-03 16:27:03,402 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,402 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,402 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,402 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,402 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-1qy2mnz4
2025-09-03 16:27:03,402 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,405 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,406 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,407 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,407 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:37943
2025-09-03 16:27:03,407 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:37943
2025-09-03 16:27:03,408 - distributed.worker - INFO -          dashboard at:           10.6.102.9:45239
2025-09-03 16:27:03,408 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,408 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,407 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,408 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,408 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-snv3aros
2025-09-03 16:27:03,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,408 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,408 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,408 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,409 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,409 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:44919
2025-09-03 16:27:03,409 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:44919
2025-09-03 16:27:03,409 - distributed.worker - INFO -          dashboard at:           10.6.102.9:39655
2025-09-03 16:27:03,409 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,409 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,409 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,409 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,409 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:40549
2025-09-03 16:27:03,409 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-e0ic5jxx
2025-09-03 16:27:03,409 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:40549
2025-09-03 16:27:03,409 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,409 - distributed.worker - INFO -          dashboard at:           10.6.102.9:34693
2025-09-03 16:27:03,409 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,409 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,409 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,409 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,409 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,409 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,409 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-4q2m_uij
2025-09-03 16:27:03,409 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,409 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,410 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,410 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,410 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:43279
2025-09-03 16:27:03,410 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,411 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:43279
2025-09-03 16:27:03,411 - distributed.worker - INFO -          dashboard at:           10.6.102.9:34303
2025-09-03 16:27:03,411 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,411 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,411 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,411 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,411 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,411 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,411 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-_me0qneb
2025-09-03 16:27:03,411 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,411 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,412 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,412 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,412 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,412 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,413 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,413 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,414 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,418 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,419 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,419 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,419 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,420 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:35641
2025-09-03 16:27:03,420 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:35641
2025-09-03 16:27:03,420 - distributed.worker - INFO -          dashboard at:           10.6.102.9:33629
2025-09-03 16:27:03,420 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,420 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,420 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,420 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,420 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-gdlfydxn
2025-09-03 16:27:03,420 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,423 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,423 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,424 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,424 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,424 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,425 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,425 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,425 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,425 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,425 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,426 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,426 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,426 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,426 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,426 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,427 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,427 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,427 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,428 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,428 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,428 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:44067
2025-09-03 16:27:03,428 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:44067
2025-09-03 16:27:03,428 - distributed.worker - INFO -          dashboard at:           10.6.102.9:36271
2025-09-03 16:27:03,428 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,428 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,428 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,428 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,428 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-dl8qf522
2025-09-03 16:27:03,428 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,428 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,428 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,429 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,429 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,430 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,430 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,430 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,430 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,431 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,432 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,432 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,433 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:42457
2025-09-03 16:27:03,433 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:42457
2025-09-03 16:27:03,433 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,433 - distributed.worker - INFO -          dashboard at:           10.6.102.9:33909
2025-09-03 16:27:03,433 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,433 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,433 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,433 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,433 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-15np859m
2025-09-03 16:27:03,433 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,435 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,435 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:41807
2025-09-03 16:27:03,435 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:41807
2025-09-03 16:27:03,435 - distributed.worker - INFO -          dashboard at:           10.6.102.9:35887
2025-09-03 16:27:03,435 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,435 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,435 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,436 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,436 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-1qqc022l
2025-09-03 16:27:03,436 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,436 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,436 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,437 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,437 - distributed.worker - INFO -       Start worker at:     tcp://10.6.102.9:33817
2025-09-03 16:27:03,438 - distributed.worker - INFO -          Listening to:     tcp://10.6.102.9:33817
2025-09-03 16:27:03,438 - distributed.worker - INFO -          dashboard at:           10.6.102.9:39173
2025-09-03 16:27:03,438 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,438 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,438 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:03,438 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:03,438 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-o92_65z6
2025-09-03 16:27:03,438 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,447 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,448 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,448 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,449 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,453 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,454 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,454 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,454 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,455 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,455 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,455 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,456 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:03,456 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:03,457 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:03,458 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:03,459 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:35,720 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,720 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,720 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,720 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,720 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,720 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,721 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,721 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,721 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,721 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,720 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,721 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,721 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,722 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,722 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,722 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,722 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,722 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,721 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,722 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,722 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,722 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,722 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,722 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,722 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,722 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,722 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,722 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,722 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,723 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,723 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,723 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,723 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,723 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,723 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,723 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,723 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,723 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,724 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,724 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,724 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,724 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,724 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,724 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,724 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,724 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,724 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,724 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,724 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,725 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,725 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,724 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,725 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,725 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,725 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,725 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,725 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,725 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,725 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,723 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,725 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,725 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,725 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,725 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,726 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,724 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,726 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,727 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,725 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,727 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,727 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,727 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,725 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,727 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,728 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,728 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,728 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,728 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,728 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,728 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,727 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,728 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,727 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,728 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,728 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,731 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,729 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,729 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,732 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,734 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,735 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,733 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,730 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,736 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,737 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,738 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,738 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,739 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,740 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,740 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,741 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,741 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,743 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,745 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:38,396 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,396 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,397 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,397 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,397 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,397 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,396 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,397 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,397 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,397 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,397 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,398 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,398 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,397 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,398 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,398 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,398 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,398 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,398 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,398 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,399 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,399 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,399 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,399 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,399 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,400 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,399 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,398 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,399 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,400 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,399 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,400 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,399 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,400 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,400 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,400 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,400 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,400 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,400 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,400 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,401 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,401 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,401 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,401 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,401 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,401 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,401 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,401 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,401 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,401 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,400 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,401 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,401 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,400 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,401 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,401 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,401 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,401 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,401 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,400 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,401 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,401 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,399 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,401 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,402 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,401 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,401 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,402 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,402 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,402 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,401 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,403 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,402 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,401 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,401 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,404 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,401 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,402 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,402 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,404 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,402 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,402 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,404 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,403 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,405 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,403 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,405 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,401 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,401 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,403 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,405 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,405 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,405 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,405 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,405 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,405 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,405 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,406 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,406 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,406 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,406 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,406 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,407 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,408 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,795 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,796 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,796 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,796 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,796 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,796 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,796 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,796 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,796 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,796 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,797 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,797 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,797 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,797 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,797 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,797 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,797 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,797 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,797 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,798 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,798 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,798 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,798 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,798 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,798 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,798 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,798 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,798 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,799 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,799 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,799 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,799 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,799 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,799 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,799 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,799 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,800 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,800 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,800 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,800 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,800 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,800 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,800 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,800 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,800 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,800 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,800 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,800 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,800 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,800 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,801 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,801 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,801 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,801 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,801 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,801 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,801 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,801 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,801 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,801 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,801 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,802 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,802 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,802 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,802 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,802 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,802 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,802 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,803 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,803 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,803 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,803 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,804 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,804 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,804 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,804 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,804 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,804 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,804 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,804 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,804 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,804 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,804 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,805 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,804 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,805 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,805 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,805 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,805 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,805 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,805 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,805 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,805 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,805 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,805 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,805 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,805 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,805 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,805 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,805 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,805 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,805 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,806 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,806 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:39,238 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,238 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,238 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,238 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,238 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,238 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,238 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,238 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,239 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,239 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,239 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,239 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,239 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,239 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,239 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,239 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,239 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,239 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,240 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,240 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,240 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,240 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,240 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,240 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,240 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,240 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,240 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,240 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,240 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,240 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,241 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,241 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,241 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,241 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,241 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,241 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,241 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,241 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,241 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,241 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,241 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,241 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,241 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,241 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,241 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,241 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,242 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,242 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,241 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,242 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,241 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,242 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,242 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,242 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,242 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,242 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,242 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,242 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,242 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,242 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,242 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,242 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,242 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,242 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,242 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,242 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,242 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,243 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,243 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,243 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,243 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,243 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,243 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,243 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,243 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,243 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,243 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,243 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,243 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,243 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,243 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,244 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,244 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,244 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,244 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,244 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,244 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,244 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,244 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,245 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,245 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,245 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,245 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,245 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,245 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,246 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,246 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,246 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,246 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,246 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,246 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,247 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,247 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,247 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:30:58,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,221 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,221 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,222 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,222 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,223 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,223 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,238 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,246 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,247 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,268 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,268 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,275 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,287 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,309 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,319 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,328 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,336 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,337 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,354 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,356 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,365 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,371 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,374 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,378 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,378 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,389 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,401 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,401 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:58,454 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:48,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:48,378 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:49,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:49,611 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:49,998 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:50,687 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:51,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:51,156 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:51,250 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:51,459 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:51,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:51,853 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:52,166 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:52,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:52,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:52,641 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:52,643 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:52,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:52,817 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:52,824 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:53,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:53,253 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:53,322 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:53,323 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:53,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:53,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:53,328 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:53,757 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:55,446 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:55,762 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:55,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:55,776 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:56,888 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:56,889 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:56,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:57,298 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:58,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:58,881 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:00,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:00,790 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:00,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:00,854 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:00,855 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:00,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:03,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:04,305 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:06,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:06,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:10,098 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:10,139 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:10,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:10,472 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:10,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:11,347 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:11,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:11,892 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:11,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:12,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:12,796 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:12,797 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:12,805 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:13,053 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:13,072 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:13,533 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:14,727 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:14,727 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:14,738 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:14,782 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:14,821 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:15,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:17,363 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:19,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:19,540 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:20,451 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:21,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:21,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:21,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:21,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:21,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:21,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:21,795 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:22,310 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:22,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:22,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:22,317 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:25,475 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:32,779 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:32,800 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:33,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:34,155 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:34,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:34,391 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:34,560 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:34,591 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:34,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:34,596 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:35,599 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:35,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:35,890 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:35,974 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:36,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:36,654 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:37,661 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:38,494 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:38,921 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:39,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:39,814 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:39,815 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:39,965 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:40,095 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:41,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:41,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:41,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:41,824 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:41,826 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:41,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:42,054 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:42,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:42,569 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:42,837 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:43,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:43,723 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:43,887 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:43,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:43,898 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:43,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:43,943 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:45,883 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:46,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:47,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:47,807 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:48,172 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:51,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:51,969 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:52,974 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:02,250 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:03,833 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:04,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:04,693 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:04,697 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:05,166 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:05,166 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:05,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:05,282 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:05,430 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:05,431 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:05,588 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:05,591 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:05,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:05,869 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:05,882 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:06,058 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:06,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:06,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:06,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:06,313 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:06,837 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:06,840 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:06,843 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:06,963 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:06,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:07,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:07,427 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:07,430 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:08,202 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:08,611 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:09,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:10,475 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:10,786 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:15,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:15,464 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:17,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:17,045 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:18,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:18,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:18,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:19,281 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:20,190 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:20,279 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:21,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:21,438 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:21,460 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:21,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,004 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,004 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,014 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,021 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,195 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,288 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,370 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,608 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,921 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,943 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:23,056 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:23,087 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:23,095 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:23,283 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:23,303 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:23,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:24,288 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:24,892 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:26,508 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:26,844 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:26,928 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:28,686 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:05,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:05,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:09,370 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:09,371 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:09,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:09,871 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:09,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:09,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:10,850 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:10,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:10,950 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:10,952 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:12,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:12,649 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:12,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:13,023 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:13,247 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:13,288 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:13,325 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:13,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:13,338 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:13,531 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:14,520 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:14,523 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:14,525 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:14,885 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:14,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:15,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:16,137 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:16,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:16,318 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:16,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:18,276 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:18,283 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:20,023 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:20,025 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:22,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:22,477 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:22,481 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:22,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:23,421 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:25,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:26,208 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:26,214 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:26,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:26,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:30,940 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:32,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:32,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:32,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:33,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:33,108 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:33,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:33,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:34,449 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:34,454 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:34,458 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:34,508 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:34,611 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:34,695 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:34,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:34,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:35,501 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:36,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:36,521 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:36,522 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:36,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:36,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:36,718 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:37,031 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:38,148 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:38,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:38,151 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:38,256 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:38,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:38,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:38,562 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:38,641 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:38,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:39,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:40,072 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:40,073 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:40,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:40,815 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:43,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:44,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:47,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:52,427 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:52,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:52,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:53,467 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:53,470 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:53,473 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:53,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:53,571 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:53,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:53,881 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:53,882 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:53,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:54,492 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:54,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:54,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:55,592 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:55,816 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:55,817 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:56,596 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:56,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:56,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:56,830 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:56,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:56,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:56,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:57,000 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:58,205 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:58,212 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:03,202 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:05,105 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:05,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:06,228 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:07,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:08,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:08,845 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:10,984 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:11,031 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:11,325 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:12,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:12,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:12,755 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:12,756 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:12,794 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:12,835 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:12,837 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:13,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:15,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:15,531 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:15,534 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:15,558 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:16,101 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:16,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:16,943 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:16,948 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:16,950 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:16,959 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:17,053 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:17,057 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:17,100 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:17,312 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:17,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:17,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:21,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:22,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:23,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:23,666 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:23,671 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:23,674 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:23,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:27,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:27,807 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:27,815 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:27,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:31,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:32,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:32,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:33,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:33,194 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:33,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:33,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:33,973 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:34,226 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:35,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:35,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:35,770 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:35,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:36,127 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:36,143 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:36,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:36,288 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:36,481 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:39,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:39,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:39,833 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:39,838 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:39,981 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:40,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:40,611 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:42,119 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:42,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:42,854 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:42,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:42,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:42,860 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:43,017 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:43,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:43,614 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:44,465 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:44,468 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:44,677 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:45,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:45,628 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:45,764 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:48,606 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:49,479 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:50,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:51,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:51,323 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,211 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,218 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,420 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,655 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:53,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:53,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:53,695 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:54,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:54,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:55,085 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:55,279 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:55,927 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:55,927 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:55,928 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:56,512 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:56,824 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:57,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:57,052 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:57,077 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:57,190 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:57,365 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:57,472 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:57,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:57,657 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:58,159 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:58,162 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:58,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:58,972 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:58,982 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:59,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:59,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:59,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:59,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:59,813 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:00,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:01,231 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:02,528 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:04,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:11,843 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:13,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:13,434 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:14,187 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:14,194 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:14,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:14,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:15,268 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:15,861 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:16,101 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:16,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:16,995 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:17,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:17,181 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:17,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:17,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:17,709 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:17,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:17,843 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:18,864 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:19,157 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:19,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:19,736 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:19,738 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:19,742 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:19,865 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:19,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:19,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:20,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:22,558 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:23,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:24,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:24,179 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:24,618 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:24,695 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:24,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:25,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:25,267 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:30,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:30,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:32,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:33,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:34,398 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:35,950 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:35,950 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:36,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:36,965 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:37,087 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:37,357 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:37,834 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:37,841 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:37,850 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:37,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:38,749 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:38,753 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:39,467 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:39,470 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:39,478 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:39,487 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:39,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:39,492 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:39,498 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:39,606 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:40,365 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:40,472 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:40,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:41,742 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:41,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:41,858 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:43,347 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:43,348 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:43,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:43,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:45,921 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:50,203 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:54,203 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:58,845 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:59,192 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:59,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:00,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:00,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:00,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:01,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:01,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:01,070 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:02,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:03,070 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:03,687 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:04,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:04,505 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:04,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:04,592 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:04,596 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:04,882 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:04,883 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:04,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:05,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:05,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:05,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:05,645 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:05,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:05,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:06,087 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:06,527 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:06,679 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:06,686 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:08,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:08,834 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:08,835 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:09,387 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:10,074 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:12,956 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:14,677 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:15,356 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:15,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:16,543 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:21,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:28,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:28,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:29,784 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:30,337 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:30,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:31,352 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:31,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:31,354 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:31,381 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:31,940 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:31,959 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:32,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:32,135 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:32,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:33,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:34,074 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:35,752 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:35,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:35,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:35,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:35,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:37,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:37,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:37,560 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:37,561 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:37,569 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:37,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:39,196 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:39,203 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:39,694 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:39,694 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:39,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:39,697 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:39,699 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:39,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:43,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:43,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:44,740 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:46,617 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:47,028 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:47,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:47,284 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:47,289 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:47,367 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:48,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:48,770 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:49,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:50,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:51,046 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:51,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:51,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:51,714 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:51,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:51,729 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:52,250 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:52,498 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:52,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:52,753 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:53,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:53,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:53,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:53,132 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:53,180 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:53,182 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:53,811 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:53,958 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:53,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:54,180 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:54,292 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:55,362 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:56,225 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:57,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:01,451 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:01,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:02,496 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:02,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:02,737 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:02,737 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:02,747 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:03,209 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:03,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:03,449 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:03,450 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:03,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:05,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:05,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:05,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:05,548 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:06,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:06,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:06,574 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:06,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:06,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:06,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:06,903 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:07,034 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:07,035 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:07,537 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:07,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:07,650 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:07,655 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:08,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:09,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:09,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:10,523 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:10,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:10,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:10,714 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:12,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:12,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:13,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:14,483 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:15,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:17,251 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:17,251 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:35,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:35,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:35,394 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:43,751 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:53,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:53,119 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:53,987 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:53,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:54,131 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:54,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:54,765 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:55,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:55,541 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:55,942 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:56,361 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:56,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:56,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:56,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:56,397 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:57,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:57,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,370 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,928 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,929 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:59,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:59,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:59,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:59,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:00,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:00,371 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:00,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:01,022 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:01,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:01,028 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:01,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:01,033 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:01,040 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:01,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:01,396 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:01,787 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:01,987 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:05,787 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:06,642 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:07,892 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:08,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:08,371 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:09,798 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:11,070 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:11,092 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:11,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:11,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:14,028 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:14,410 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:15,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:16,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:16,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:28,784 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:30,899 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:31,023 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:31,228 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:31,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:31,765 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:32,354 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:32,355 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:32,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:32,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:33,551 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:33,554 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:33,559 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:33,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:33,639 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:33,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:34,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:34,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:34,017 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:34,303 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:34,303 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:34,833 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:34,835 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:36,838 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:36,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:36,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:37,098 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:37,099 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:37,101 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:37,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:38,459 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:38,461 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:38,462 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:38,464 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:42,057 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:42,511 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:42,511 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:42,512 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:44,370 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:44,374 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:47,840 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:47,840 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:47,841 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:52,522 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:52,904 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:52,910 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:53,444 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:53,913 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:53,918 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:54,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:54,595 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:54,599 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:54,730 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:55,255 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:55,255 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:55,256 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:55,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:55,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:55,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:56,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:56,414 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:56,543 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,011 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,084 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,131 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,237 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,520 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:58,096 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:58,096 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:58,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:58,207 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:58,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:58,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:59,201 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:00,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:00,362 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:02,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:02,022 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:06,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:13,172 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:14,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:14,953 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:14,970 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:14,982 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:15,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:15,160 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:15,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:15,496 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:15,569 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:15,728 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:15,730 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:15,785 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:16,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:16,398 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:16,398 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:16,399 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:16,514 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:16,701 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:16,888 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:16,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,092 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,311 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,313 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,568 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,854 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,859 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,961 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:18,655 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:19,821 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:19,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:19,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:19,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:19,845 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:19,997 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:20,246 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:20,251 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:20,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:20,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:20,849 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:21,084 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:21,088 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:21,310 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:21,317 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:25,231 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:26,052 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:26,542 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:26,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:26,981 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:26,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:27,011 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:27,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:27,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:27,837 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:27,927 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:28,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:28,481 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:28,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:28,748 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:28,958 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:29,143 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:29,374 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:29,387 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:30,139 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:30,589 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:30,590 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:30,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:30,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:31,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:31,603 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:31,606 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:31,606 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:31,609 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:31,689 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:31,695 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:31,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:31,887 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:46,329 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:46,986 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:47,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:47,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:47,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:47,467 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:47,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:47,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:48,047 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:48,428 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:48,630 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:48,630 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:48,716 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:48,749 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:48,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:48,774 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:49,464 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:49,994 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:50,298 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:50,401 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:50,663 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:50,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:50,884 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:50,887 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:51,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:54,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:54,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:56,658 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:00,942 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:02,501 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:02,811 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:03,218 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:03,218 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:03,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:03,222 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:03,223 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:04,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:05,140 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:05,325 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:06,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:06,166 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:06,671 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:06,677 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:06,679 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:07,080 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:07,081 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:07,826 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:07,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:08,651 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:23,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:23,023 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:23,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:23,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:23,039 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:25,255 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:26,774 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:26,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:26,782 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:26,915 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:28,158 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:28,178 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:28,319 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:28,595 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:28,651 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:29,458 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:29,463 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:34,662 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:36,694 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:36,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:36,805 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:36,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:36,902 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:36,956 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:37,298 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:37,307 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:37,385 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:37,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:37,451 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:37,840 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:38,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:38,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:38,106 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:38,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:38,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:38,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:38,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:39,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:39,097 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:39,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:39,357 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:39,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:39,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:39,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:40,040 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:40,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:40,046 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:40,046 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:40,048 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:40,176 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:40,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:41,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:41,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:43,378 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:44,621 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:47,692 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:47,694 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:48,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:50,192 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:55,774 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:56,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:00,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:01,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:01,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:01,288 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:01,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:01,485 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:01,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:03,548 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:03,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:03,860 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:04,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:04,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:04,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:04,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:04,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:05,562 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:05,567 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:05,807 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:06,915 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:07,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:07,758 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:07,768 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:07,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:08,521 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:08,522 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:09,052 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:09,284 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:11,391 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:11,426 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:11,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:13,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:13,542 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:13,815 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,209 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:15,508 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:15,793 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:15,795 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:16,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:16,675 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:16,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:16,961 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:17,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:17,223 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:17,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:17,833 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:17,834 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:17,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:18,005 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:18,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:18,014 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:18,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:18,321 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:18,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:18,329 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:18,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:18,736 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:18,860 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:18,887 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:19,007 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:19,040 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:19,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:19,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:20,526 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:20,747 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:21,312 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:27,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:28,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:28,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:28,748 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:30,454 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:30,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:31,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:32,899 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:32,975 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:33,000 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:33,825 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:34,160 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:34,161 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:34,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:34,273 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:34,383 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:34,385 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:34,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:34,468 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.102.1:33077
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 231, in read
    buffer = await read_bytes_rw(stream, buffer_nbytes)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 367, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.102.9:57418 remote=tcp://10.6.102.1:33077>: Stream is closed
2025-09-03 16:42:34,830 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,830 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,830 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,831 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:35197. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,830 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,831 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:42021. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,831 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:39727. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,831 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:40101. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,831 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,831 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,831 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,830 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,831 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,830 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,830 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,832 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:38353. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,832 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:41807. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,830 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,831 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,830 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,832 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:40015. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,832 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:33817. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,832 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:35641. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,830 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,833 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:33689. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,832 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,833 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:39501. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,833 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:33465. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,833 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:37943. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,831 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,833 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:43987. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,831 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,832 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,833 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:45023. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,833 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:36301. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,831 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,831 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,834 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:38451. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,832 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,834 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:33699. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,831 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,832 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,834 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:44367. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,831 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,834 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:36005. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,832 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,834 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:41203. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,834 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:35941. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,832 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,834 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:42959. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,835 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:42203. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,832 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,832 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,835 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:34405. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,835 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:38115. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,832 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,835 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:44175. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,835 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:33691. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,836 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:46327. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,836 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:36195. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,832 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,838 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:43619. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,844 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:43583'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,848 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1964, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,848 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1965, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,849 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,849 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,849 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,849 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,849 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,854 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:40857'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,854 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:40963'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,855 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:38479'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,855 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:39917'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,855 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:34197'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,856 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:41983'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,856 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3257, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,856 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3419, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,856 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.10:39723, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,856 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2710, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,856 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:36805'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,856 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3431, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,856 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1354, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,856 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1357, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,856 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:32815'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,857 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1132, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,857 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:38219'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,857 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1056, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,857 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,857 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1130, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,857 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 4022, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,857 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3374, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,857 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,857 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,857 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,857 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:40585'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,857 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3386, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,857 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3036, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,857 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,857 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,857 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,858 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:39885'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,858 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1661, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,858 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:46543'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,858 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3035, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,858 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.7:36125, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,859 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,858 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1183, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,859 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,859 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,859 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,859 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.25:32773, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,859 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,859 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,859 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,859 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1801, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,859 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,859 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,859 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,859 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 5235, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,859 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,859 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,859 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,859 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 8548, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,859 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,859 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2480, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,860 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,860 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,860 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,860 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.35:33503, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,860 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,860 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,860 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,860 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.9:43061, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,860 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,860 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,860 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1071, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,861 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,861 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,861 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,862 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1341, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,862 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.10:37795, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,862 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2755, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,863 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.16:38355, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,865 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:39277'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,866 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:34445'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,866 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:42005'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,866 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:40193'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,866 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,866 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:42087'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,867 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:43009'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,867 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:46321'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,866 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,867 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:37269'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,867 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2612, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,867 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:39927'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,867 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1701, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,868 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:45297'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,867 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3424, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,868 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:42415'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,867 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,868 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1767, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,868 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3292, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,868 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:37191'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,868 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1695, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,868 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1377, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,868 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2130, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,868 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,868 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:34029'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,868 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1697, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,868 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1123, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,868 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2020, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,869 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1126, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,868 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3046, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,869 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,869 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:36415'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,869 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,869 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1122, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,869 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2019, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,869 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,869 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:44533'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,869 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,869 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3429, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,869 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,869 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,869 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2836, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,869 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:37637'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,869 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,869 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,869 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2604, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,870 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:33599'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,870 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,870 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,870 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1159, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,870 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,870 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,870 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:41395'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,870 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,870 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,870 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1378, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,870 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,870 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,870 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.24:36583, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,870 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,870 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.16:35937, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,870 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,870 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,870 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,870 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.9:36923, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,870 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,870 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2084, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,870 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,870 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,870 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,871 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2715, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,871 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 8255, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,871 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.1:38235, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,871 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2386, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,872 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,872 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,872 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,872 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 5202, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,872 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,872 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,872 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3443, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,872 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1966, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,872 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,872 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1116, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,872 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,872 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,872 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3666, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,872 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3519, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,873 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,873 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,873 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,873 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,873 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,873 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,873 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,873 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,874 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,874 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,874 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,874 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,874 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,874 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,875 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,875 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,875 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,875 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,875 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,875 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,875 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,875 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,875 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,875 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,875 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2007, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,876 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.3:41205, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,876 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.3:44205, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,876 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,876 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 4969, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,876 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,876 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,876 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,877 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1064, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,877 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,877 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.23:36181, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,878 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,879 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,879 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,879 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,879 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1581, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,880 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2486, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,880 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.14:39263, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,890 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.14:33483, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,902 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,903 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,903 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,903 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,903 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,904 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,904 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,904 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,904 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,904 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:35,127 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:35,129 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:43061. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,132 - distributed.worker - ERROR - failed during get data with tcp://10.6.102.9:43061 -> tcp://10.6.102.9:33689
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.9:43061 remote=tcp://10.6.102.9:55586>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-03 16:42:35,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:35,157 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.9:38562 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.102.9:38562 remote=tcp://10.6.102.1:8786>: Stream is closed
2025-09-03 16:42:35,163 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:34301'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,164 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2853, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:35,165 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:35,165 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:35,165 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:35,165 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:35,165 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:35,175 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x149e70533bd0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:35,192 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:35,195 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:44919. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:35,221 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.9:38872 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:35,226 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:42855'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,227 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:35,227 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:35,227 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:35,227 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:35,228 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:35,236 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:35,237 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:41591. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,237 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14d879c7e850>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:35,243 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:35,245 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:35,249 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.9:38856 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.102.9:38856 remote=tcp://10.6.102.1:8786>: Stream is closed
2025-09-03 16:42:35,254 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:35493'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,255 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:35,255 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:35,255 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:35,255 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:35,255 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:35,263 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x153612e71510>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:35,271 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:35,485 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:35,531 - distributed.worker - ERROR - failed during get data with tcp://10.6.102.9:44367 -> tcp://10.6.102.24:36273
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.9:44367 remote=tcp://10.6.102.24:59874>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-03 16:42:35,531 - distributed.worker - ERROR - failed during get data with tcp://10.6.102.9:38115 -> tcp://10.6.102.24:36273
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.9:38115 remote=tcp://10.6.102.24:36870>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-03 16:42:35,542 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:35,620 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:35,620 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:42457. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,625 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:35,628 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.9:38898 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:35,631 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:45783'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,631 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3159, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:35,631 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:35,632 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:35,632 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:35,632 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:35,632 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:35,637 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14bb5778b4d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:35,643 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:35,872 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:35,873 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:33061. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:35,884 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.9:38850 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.102.9:38850 remote=tcp://10.6.102.1:8786>: Stream is closed
2025-09-03 16:42:35,888 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:35123'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,889 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:35,889 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:35,889 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:35,889 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:35,889 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:35,894 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x148df90a7b90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:35,901 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,126 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:36,127 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:43279. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,143 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:36,148 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.9:38880 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:36,151 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:42937'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,152 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.34:45639, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:36,153 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:36,153 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:36,153 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:36,153 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:36,153 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:36,159 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14a80dbe0610>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:36,166 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,314 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:36,315 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:33355. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,315 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:36,316 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:38603. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,317 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.102.1:41453
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2876, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.9:54062 remote=tcp://10.6.102.1:41453>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-03 16:42:36,322 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:36,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:36,327 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.9:38530 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.102.9:38530 remote=tcp://10.6.102.1:8786>: Stream is closed
2025-09-03 16:42:36,328 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.9:38702 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.102.9:38702 remote=tcp://10.6.102.1:8786>: Stream is closed
2025-09-03 16:42:36,331 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:43967'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,331 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:44889'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,331 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:36,332 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:36,332 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:36,332 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:36,332 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:36,332 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2884, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:36,332 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:36,332 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:36,332 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:36,332 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:36,333 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:36,337 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:36,338 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14853e990810>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:36,343 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,450 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,479 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,806 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:36,808 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:34307. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,824 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.102.6:42751
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.102.9:59394 remote=tcp://10.6.102.6:42751>: Stream is closed
2025-09-03 16:42:36,832 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:36,837 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.9:38590 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:36,840 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:39649'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,841 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:36,841 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:36,841 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:36,842 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:36,842 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:36,849 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14eefddf02d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:36,856 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,995 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,997 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,100 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:37,102 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:37,103 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:40549. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,118 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:37,119 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:43291. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,124 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.9:38874 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:37,126 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:37,128 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:42031'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,129 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3261, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:37,129 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:37,129 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:37,130 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:37,130 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:37,130 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:37,130 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.9:38842 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.102.9:38842 remote=tcp://10.6.102.1:8786>: Stream is closed
2025-09-03 16:42:37,136 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1541dd83e3d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:37,139 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:45877'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,140 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.34:43851, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:37,140 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:37,141 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:37,141 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:37,141 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:37,141 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:37,147 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1461cdc6b590>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:37,154 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,154 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,155 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,162 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:37,163 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:36513. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:37,175 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.9:38798 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.102.9:38798 remote=tcp://10.6.102.1:8786>: Stream is closed
2025-09-03 16:42:37,180 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:38907'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,181 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:37,181 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:37,181 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:37,181 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:37,181 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:37,187 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14ace6b73c90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:37,195 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:37,200 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:37,202 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:36923. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,203 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,219 - distributed.worker - ERROR - failed during get data with tcp://10.6.102.9:36923 -> tcp://10.6.102.9:38115
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.9:36923 remote=tcp://10.6.102.9:32886>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-03 16:42:37,229 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.9:38572 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:37,232 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:46507'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,233 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:37,233 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:37,233 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:37,233 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:37,233 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:37,237 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x147bb48225d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:37,243 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,249 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:37,274 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:37,293 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:37,293 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:40763. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,300 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:37,302 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.9:38588 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:37,305 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:45577'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,305 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:37,305 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:37,306 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:37,306 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:37,306 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:37,309 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14d681ef9a10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:37,314 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,331 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:37,333 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:44215. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,334 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,349 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:37,354 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.9:38844 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:37,357 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:41913'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,358 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:37,358 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:37,358 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:37,358 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:37,358 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:37,364 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1485d8b96110>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:37,370 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,372 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:37,373 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:41413. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:37,383 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.9:38760 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.102.9:38760 remote=tcp://10.6.102.1:8786>: Stream is closed
2025-09-03 16:42:37,388 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:38555'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,389 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:37,389 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:37,389 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:37,389 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:37,389 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:37,393 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x149558553a90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:37,399 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,419 - distributed.worker - ERROR - failed during get data with tcp://10.6.102.9:33465 -> tcp://10.6.102.7:42621
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.9:33465 remote=tcp://10.6.102.7:48328>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-03 16:42:37,463 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,484 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,496 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:37,546 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:37,645 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:37,683 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,719 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:35493'. Reason: nanny-close-gracefully
2025-09-03 16:42:37,722 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:42855'. Reason: nanny-close-gracefully
2025-09-03 16:42:37,724 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:35493' closed.
2025-09-03 16:42:37,725 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:42855' closed.
2025-09-03 16:42:37,832 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:37,834 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,835 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:45839. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,835 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,851 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.102.36:42387
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.102.9:44706 remote=tcp://10.6.102.36:42387>: Stream is closed
2025-09-03 16:42:37,856 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.102.7:40343
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.102.9:43566 remote=tcp://10.6.102.7:40343>: Stream is closed
2025-09-03 16:42:37,859 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:37,863 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.9:38730 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:37,865 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:40475'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,866 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:37,866 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:37,867 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:37,867 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:37,867 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:37,871 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x146c4415f290>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:37,876 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,904 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:37,970 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:36415'. Reason: nanny-close-gracefully
2025-09-03 16:42:37,971 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:36415' closed.
2025-09-03 16:42:38,022 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:34029'. Reason: nanny-close-gracefully
2025-09-03 16:42:38,035 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:34029' closed.
2025-09-03 16:42:38,071 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:38,072 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:40255. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:38,075 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:38,077 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:36179. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:38,085 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:45783'. Reason: nanny-close-gracefully
2025-09-03 16:42:38,088 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:45783' closed.
2025-09-03 16:42:38,087 - distributed.worker - ERROR - failed during get data with tcp://10.6.102.9:40255 -> tcp://10.6.102.15:44285
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.9:40255 remote=tcp://10.6.102.15:42162>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-03 16:42:38,092 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:38,093 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,097 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:38,097 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.9:38578 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:38,099 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:38769'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:38,100 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:38,100 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:38,100 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:38,100 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:38,100 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:38,102 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.9:38752 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:38,105 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:34903'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:38,104 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14a35537e6d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:38,106 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:38,106 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:38,106 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:38,106 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:38,106 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:38,109 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,110 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x153ac1c6d3d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:38,116 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,117 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,169 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,181 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,332 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:35123'. Reason: nanny-close-gracefully
2025-09-03 16:42:38,333 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:35123' closed.
2025-09-03 16:42:38,346 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,428 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,454 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,483 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,599 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:42937'. Reason: nanny-close-gracefully
2025-09-03 16:42:38,600 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:42937' closed.
2025-09-03 16:42:38,622 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,796 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:43967'. Reason: nanny-close-gracefully
2025-09-03 16:42:38,797 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:43967' closed.
2025-09-03 16:42:38,859 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,881 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:38,882 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:44067. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:38,893 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:39927'. Reason: nanny-close-gracefully
2025-09-03 16:42:38,894 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:39927' closed.
2025-09-03 16:42:38,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:38,906 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.9:38896 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:38,907 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:34301'. Reason: nanny-close-gracefully
2025-09-03 16:42:38,908 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:34301' closed.
2025-09-03 16:42:38,909 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:46153'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:38,912 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:38,913 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:38,913 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:38,913 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:38,913 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:38,915 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1485fc572f10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:38,920 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,923 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,993 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,994 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,998 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,998 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,001 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,006 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:39,007 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.9:41781. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:39,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:39,017 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.9:38826 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.102.9:38826 remote=tcp://10.6.102.1:8786>: Stream is closed
2025-09-03 16:42:39,026 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.9:37733'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:39,027 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:39,027 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:39,027 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:39,027 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:39,027 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:39,031 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,030 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14ea373bae90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:39,037 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,132 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,158 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,158 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,175 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,177 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,177 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,181 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,198 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,207 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,246 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,275 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,316 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,332 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,338 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,373 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,392 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:39649'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,393 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:39649' closed.
2025-09-03 16:42:39,401 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,427 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,467 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,473 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,481 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:41983'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,487 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:41983' closed.
2025-09-03 16:42:39,487 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,487 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:40193'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,494 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:40193' closed.
2025-09-03 16:42:39,664 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:42087'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,665 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:42087' closed.
2025-09-03 16:42:39,677 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:38907'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,678 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:38907' closed.
2025-09-03 16:42:39,686 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,707 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:39917'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,708 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:45297'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,708 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:39917' closed.
2025-09-03 16:42:39,709 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:45297' closed.
2025-09-03 16:42:39,727 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:45877'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,728 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:45877' closed.
2025-09-03 16:42:39,791 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:46507'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,792 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:46507' closed.
2025-09-03 16:42:39,838 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,838 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,840 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:45577'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,841 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:45577' closed.
2025-09-03 16:42:39,879 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,890 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:37637'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,891 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:37637' closed.
2025-09-03 16:42:39,911 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:34445'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,913 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:38555'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,914 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:34445' closed.
2025-09-03 16:42:39,914 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:38555' closed.
2025-09-03 16:42:39,924 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:41913'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,924 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:41913' closed.
2025-09-03 16:42:40,034 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:43009'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,035 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:43009' closed.
2025-09-03 16:42:40,097 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,111 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,118 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,121 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,123 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:40857'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,124 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:40857' closed.
2025-09-03 16:42:40,186 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,344 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:42031'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,345 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:42031' closed.
2025-09-03 16:42:40,373 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:40475'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,374 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:40475' closed.
2025-09-03 16:42:40,377 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:44889'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,381 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:44889' closed.
2025-09-03 16:42:40,432 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,536 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:38769'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,537 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:38769' closed.
2025-09-03 16:42:40,545 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:46321'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,545 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:46321' closed.
2025-09-03 16:42:40,569 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:34903'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,569 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:42415'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,572 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:34903' closed.
2025-09-03 16:42:40,573 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:42415' closed.
2025-09-03 16:42:40,626 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,663 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:39277'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,663 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:39277' closed.
2025-09-03 16:42:40,856 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:38219'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,857 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:38219' closed.
2025-09-03 16:42:40,922 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,927 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,997 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,998 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,002 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,035 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,040 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,061 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:42005'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,062 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:42005' closed.
2025-09-03 16:42:41,136 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,179 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,181 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,182 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,186 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,279 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,335 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,343 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:46153'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,344 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:46153' closed.
2025-09-03 16:42:41,430 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,476 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,504 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:37191'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,505 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:37191' closed.
2025-09-03 16:42:41,574 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:37733'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,575 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:37733' closed.
2025-09-03 16:42:41,616 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:33599'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,620 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:39885'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,620 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:33599' closed.
2025-09-03 16:42:41,620 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:40585'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,621 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:39885' closed.
2025-09-03 16:42:41,623 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:40585' closed.
2025-09-03 16:42:41,632 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:41395'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,633 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:41395' closed.
2025-09-03 16:42:41,648 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:40963'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,649 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:40963' closed.
2025-09-03 16:42:41,652 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:37269'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,653 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:37269' closed.
2025-09-03 16:42:41,661 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:34197'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,662 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:34197' closed.
2025-09-03 16:42:41,768 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:44533'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,769 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:44533' closed.
2025-09-03 16:42:41,798 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:32815'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,799 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:32815' closed.
2025-09-03 16:42:41,803 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:43583'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,803 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:43583' closed.
2025-09-03 16:42:41,888 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:46543'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,889 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:46543' closed.
2025-09-03 16:42:41,925 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:38479'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,926 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:38479' closed.
2025-09-03 16:42:41,950 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.9:36805'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,951 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.9:36805' closed.
2025-09-03 16:42:41,953 - distributed.dask_worker - INFO - End worker
