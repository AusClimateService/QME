Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-05 09:11:48,683 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:36479'
2025-09-05 09:11:48,690 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:42337'
2025-09-05 09:11:48,693 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:40545'
2025-09-05 09:11:48,697 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:46133'
2025-09-05 09:11:48,702 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:42691'
2025-09-05 09:11:48,706 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:35857'
2025-09-05 09:11:48,710 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:32977'
2025-09-05 09:11:48,715 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:34493'
2025-09-05 09:11:48,719 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:38473'
2025-09-05 09:11:48,725 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:40765'
2025-09-05 09:11:48,730 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:40915'
2025-09-05 09:11:48,734 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:38249'
2025-09-05 09:11:48,739 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:36581'
2025-09-05 09:11:48,743 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:36085'
2025-09-05 09:11:48,747 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:41537'
2025-09-05 09:11:48,755 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:43881'
2025-09-05 09:11:48,759 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:43511'
2025-09-05 09:11:48,763 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:34073'
2025-09-05 09:11:48,768 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:42251'
2025-09-05 09:11:48,772 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:34891'
2025-09-05 09:11:48,846 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:41661'
2025-09-05 09:11:48,851 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:34179'
2025-09-05 09:11:48,856 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:38635'
2025-09-05 09:11:48,860 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:35863'
2025-09-05 09:11:48,864 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:37273'
2025-09-05 09:11:48,868 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:38331'
2025-09-05 09:11:48,874 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:39287'
2025-09-05 09:11:48,878 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:39245'
2025-09-05 09:11:48,883 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:42189'
2025-09-05 09:11:48,887 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:45967'
2025-09-05 09:11:48,891 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:42329'
2025-09-05 09:11:48,895 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:34785'
2025-09-05 09:11:48,900 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:44101'
2025-09-05 09:11:48,904 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:34323'
2025-09-05 09:11:48,909 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:40495'
2025-09-05 09:11:48,912 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:40809'
2025-09-05 09:11:48,917 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:36515'
2025-09-05 09:11:48,923 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:39027'
2025-09-05 09:11:48,927 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:42861'
2025-09-05 09:11:48,931 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:40139'
2025-09-05 09:11:48,935 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:46455'
2025-09-05 09:11:48,940 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:39643'
2025-09-05 09:11:48,944 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:37485'
2025-09-05 09:11:48,948 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:32769'
2025-09-05 09:11:48,952 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:46663'
2025-09-05 09:11:48,954 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:39343'
2025-09-05 09:11:48,958 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:39407'
2025-09-05 09:11:48,961 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:42133'
2025-09-05 09:11:48,965 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:34101'
2025-09-05 09:11:48,968 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:43515'
2025-09-05 09:11:48,971 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:35777'
2025-09-05 09:11:48,977 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.63:35143'
2025-09-05 09:11:49,798 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:39903
2025-09-05 09:11:49,799 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:44215
2025-09-05 09:11:49,799 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:39903
2025-09-05 09:11:49,799 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:44215
2025-09-05 09:11:49,799 - distributed.worker - INFO -          dashboard at:          10.6.101.63:37069
2025-09-05 09:11:49,799 - distributed.worker - INFO -          dashboard at:          10.6.101.63:43101
2025-09-05 09:11:49,799 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,799 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,799 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,799 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,799 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,799 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:38851
2025-09-05 09:11:49,799 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,799 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,799 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:38851
2025-09-05 09:11:49,799 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,799 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-rgdjamyr
2025-09-05 09:11:49,799 - distributed.worker - INFO -          dashboard at:          10.6.101.63:35979
2025-09-05 09:11:49,799 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-9usnbtin
2025-09-05 09:11:49,799 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,799 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,799 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,799 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,799 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,799 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,799 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-65pnz2l9
2025-09-05 09:11:49,799 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,812 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:46859
2025-09-05 09:11:49,812 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:46859
2025-09-05 09:11:49,812 - distributed.worker - INFO -          dashboard at:          10.6.101.63:41129
2025-09-05 09:11:49,812 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,812 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,812 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,812 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,813 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-6sd1f92w
2025-09-05 09:11:49,813 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,826 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,827 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,827 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,828 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,834 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,835 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,835 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,836 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,841 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,842 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,842 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,844 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,846 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,847 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,847 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,849 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,875 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:40333
2025-09-05 09:11:49,875 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:40333
2025-09-05 09:11:49,875 - distributed.worker - INFO -          dashboard at:          10.6.101.63:46289
2025-09-05 09:11:49,875 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,875 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,875 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,875 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,875 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-f95nttbg
2025-09-05 09:11:49,875 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,892 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,892 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,892 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,893 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,907 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:37555
2025-09-05 09:11:49,907 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:37555
2025-09-05 09:11:49,907 - distributed.worker - INFO -          dashboard at:          10.6.101.63:46023
2025-09-05 09:11:49,908 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,908 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,908 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,908 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,908 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-90u_vn2e
2025-09-05 09:11:49,908 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,908 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:39677
2025-09-05 09:11:49,908 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:39677
2025-09-05 09:11:49,908 - distributed.worker - INFO -          dashboard at:          10.6.101.63:43103
2025-09-05 09:11:49,908 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,908 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,908 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,908 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,909 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-94oj_lgj
2025-09-05 09:11:49,909 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,915 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:40217
2025-09-05 09:11:49,915 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:40217
2025-09-05 09:11:49,915 - distributed.worker - INFO -          dashboard at:          10.6.101.63:45661
2025-09-05 09:11:49,915 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,915 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,915 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,915 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,915 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-rwl3n3qz
2025-09-05 09:11:49,915 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,920 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:35125
2025-09-05 09:11:49,920 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:35125
2025-09-05 09:11:49,920 - distributed.worker - INFO -          dashboard at:          10.6.101.63:40445
2025-09-05 09:11:49,920 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,920 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,920 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,920 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,920 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-a9w6bu32
2025-09-05 09:11:49,920 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,923 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:36057
2025-09-05 09:11:49,923 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:36057
2025-09-05 09:11:49,923 - distributed.worker - INFO -          dashboard at:          10.6.101.63:38627
2025-09-05 09:11:49,924 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,924 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,924 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,924 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,924 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-z5ayi1ww
2025-09-05 09:11:49,924 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,924 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:39317
2025-09-05 09:11:49,925 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:39317
2025-09-05 09:11:49,925 - distributed.worker - INFO -          dashboard at:          10.6.101.63:36641
2025-09-05 09:11:49,925 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,925 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,925 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,925 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,925 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-9hyhsy1c
2025-09-05 09:11:49,925 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,929 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:46411
2025-09-05 09:11:49,930 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:46411
2025-09-05 09:11:49,930 - distributed.worker - INFO -          dashboard at:          10.6.101.63:38681
2025-09-05 09:11:49,930 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,930 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,930 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,930 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,930 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-q76nx_fr
2025-09-05 09:11:49,930 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,934 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:39963
2025-09-05 09:11:49,934 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:39963
2025-09-05 09:11:49,934 - distributed.worker - INFO -          dashboard at:          10.6.101.63:39885
2025-09-05 09:11:49,934 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,934 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,934 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,934 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,934 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-bevro6v_
2025-09-05 09:11:49,934 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,935 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,936 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:43443
2025-09-05 09:11:49,936 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:43443
2025-09-05 09:11:49,936 - distributed.worker - INFO -          dashboard at:          10.6.101.63:34223
2025-09-05 09:11:49,936 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,936 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,936 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,936 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,936 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,936 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-_d_bwybh
2025-09-05 09:11:49,936 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,936 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,938 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,940 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:45885
2025-09-05 09:11:49,941 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:45885
2025-09-05 09:11:49,941 - distributed.worker - INFO -          dashboard at:          10.6.101.63:40453
2025-09-05 09:11:49,941 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,941 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,941 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,941 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,941 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-yyek6cod
2025-09-05 09:11:49,941 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,942 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,943 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,944 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,945 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,950 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:39297
2025-09-05 09:11:49,950 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:39297
2025-09-05 09:11:49,950 - distributed.worker - INFO -          dashboard at:          10.6.101.63:42053
2025-09-05 09:11:49,950 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,950 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,950 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,950 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,950 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-ql0c2apl
2025-09-05 09:11:49,950 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,951 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,952 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,952 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,953 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:36235
2025-09-05 09:11:49,953 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:36235
2025-09-05 09:11:49,953 - distributed.worker - INFO -          dashboard at:          10.6.101.63:44587
2025-09-05 09:11:49,953 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,953 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,953 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,953 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,953 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-l_82yggu
2025-09-05 09:11:49,953 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,954 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,954 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:44981
2025-09-05 09:11:49,954 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:44981
2025-09-05 09:11:49,954 - distributed.worker - INFO -          dashboard at:          10.6.101.63:45797
2025-09-05 09:11:49,954 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,954 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,954 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,954 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,954 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-2oufi2bt
2025-09-05 09:11:49,954 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,955 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,955 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,955 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,956 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,960 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,961 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,961 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,963 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,964 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,965 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,965 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,967 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,967 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,968 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,968 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,970 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,972 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,973 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,973 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,974 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,975 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,976 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,976 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,977 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,978 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,978 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,978 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,980 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,988 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,989 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,989 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,991 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,991 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,992 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,992 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,993 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:49,994 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,994 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,995 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,996 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:49,997 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:43587
2025-09-05 09:11:49,997 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:43587
2025-09-05 09:11:49,997 - distributed.worker - INFO -          dashboard at:          10.6.101.63:33387
2025-09-05 09:11:49,997 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:49,997 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:49,997 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:49,997 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:49,997 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-rjsa61au
2025-09-05 09:11:49,997 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,018 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,019 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,020 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,021 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,032 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:45689
2025-09-05 09:11:50,032 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:45689
2025-09-05 09:11:50,032 - distributed.worker - INFO -          dashboard at:          10.6.101.63:37531
2025-09-05 09:11:50,032 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,032 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,032 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,032 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,032 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-7uqbx6zs
2025-09-05 09:11:50,032 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,041 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:34787
2025-09-05 09:11:50,041 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:34787
2025-09-05 09:11:50,041 - distributed.worker - INFO -          dashboard at:          10.6.101.63:45471
2025-09-05 09:11:50,041 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,041 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,041 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,041 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,041 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-lz1epwog
2025-09-05 09:11:50,041 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,056 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,056 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,056 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,057 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,061 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:42665
2025-09-05 09:11:50,061 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:42665
2025-09-05 09:11:50,061 - distributed.worker - INFO -          dashboard at:          10.6.101.63:46335
2025-09-05 09:11:50,061 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,061 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,061 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,061 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,061 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-wtmrhgvl
2025-09-05 09:11:50,061 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,065 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,066 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,067 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,068 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,091 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,092 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,092 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,094 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,211 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:35183
2025-09-05 09:11:50,211 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:35183
2025-09-05 09:11:50,211 - distributed.worker - INFO -          dashboard at:          10.6.101.63:44943
2025-09-05 09:11:50,211 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,211 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,211 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,211 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,211 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-6jfh_8kc
2025-09-05 09:11:50,211 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,221 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:33025
2025-09-05 09:11:50,221 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:33025
2025-09-05 09:11:50,221 - distributed.worker - INFO -          dashboard at:          10.6.101.63:45081
2025-09-05 09:11:50,221 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,221 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,221 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,221 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,221 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-kmg73ihs
2025-09-05 09:11:50,221 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,227 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:39603
2025-09-05 09:11:50,227 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:39603
2025-09-05 09:11:50,227 - distributed.worker - INFO -          dashboard at:          10.6.101.63:41711
2025-09-05 09:11:50,227 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,227 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,227 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,227 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,227 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-10w43e46
2025-09-05 09:11:50,227 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,232 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:44995
2025-09-05 09:11:50,232 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:44995
2025-09-05 09:11:50,232 - distributed.worker - INFO -          dashboard at:          10.6.101.63:43049
2025-09-05 09:11:50,232 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,232 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,232 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,232 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,232 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-y9qpm55u
2025-09-05 09:11:50,232 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,238 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,238 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:44549
2025-09-05 09:11:50,239 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:44549
2025-09-05 09:11:50,239 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:40011
2025-09-05 09:11:50,239 - distributed.worker - INFO -          dashboard at:          10.6.101.63:45587
2025-09-05 09:11:50,239 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:40011
2025-09-05 09:11:50,239 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,239 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,239 - distributed.worker - INFO -          dashboard at:          10.6.101.63:36215
2025-09-05 09:11:50,239 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:34029
2025-09-05 09:11:50,239 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,239 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,239 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,239 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:34029
2025-09-05 09:11:50,239 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,239 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-h6hdo295
2025-09-05 09:11:50,239 - distributed.worker - INFO -          dashboard at:          10.6.101.63:38015
2025-09-05 09:11:50,239 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,239 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,239 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,239 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,239 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,239 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,239 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-f7p8uvl3
2025-09-05 09:11:50,239 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,239 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,239 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,239 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-d6o3htpg
2025-09-05 09:11:50,239 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,239 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,240 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,246 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,247 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:38307
2025-09-05 09:11:50,247 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:38307
2025-09-05 09:11:50,247 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,247 - distributed.worker - INFO -          dashboard at:          10.6.101.63:40263
2025-09-05 09:11:50,247 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:35077
2025-09-05 09:11:50,247 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,247 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,247 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:35077
2025-09-05 09:11:50,247 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,247 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,247 - distributed.worker - INFO -          dashboard at:          10.6.101.63:35249
2025-09-05 09:11:50,247 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,247 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,247 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-lgdfw3a8
2025-09-05 09:11:50,247 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,247 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,247 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,247 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,248 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-but85r_5
2025-09-05 09:11:50,248 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,249 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,254 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,255 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,255 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,257 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:39305
2025-09-05 09:11:50,257 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:39305
2025-09-05 09:11:50,257 - distributed.worker - INFO -          dashboard at:          10.6.101.63:40963
2025-09-05 09:11:50,257 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,257 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,257 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,257 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,257 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,257 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-nlzxbofl
2025-09-05 09:11:50,257 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,260 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:45509
2025-09-05 09:11:50,260 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:45509
2025-09-05 09:11:50,261 - distributed.worker - INFO -          dashboard at:          10.6.101.63:40167
2025-09-05 09:11:50,261 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,261 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,261 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,261 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:40773
2025-09-05 09:11:50,261 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,261 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:40773
2025-09-05 09:11:50,261 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-jc_6dloc
2025-09-05 09:11:50,261 - distributed.worker - INFO -          dashboard at:          10.6.101.63:46311
2025-09-05 09:11:50,261 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,261 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,261 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,261 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,261 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,261 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-wcmplk16
2025-09-05 09:11:50,261 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,261 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,262 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,262 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,264 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,266 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:44857
2025-09-05 09:11:50,266 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:44857
2025-09-05 09:11:50,266 - distributed.worker - INFO -          dashboard at:          10.6.101.63:33903
2025-09-05 09:11:50,266 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,266 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,266 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,266 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,266 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-i2vcv8g_
2025-09-05 09:11:50,266 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,266 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,267 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,267 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,267 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,271 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,272 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,272 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,273 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:35721
2025-09-05 09:11:50,273 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:35721
2025-09-05 09:11:50,273 - distributed.worker - INFO -          dashboard at:          10.6.101.63:41689
2025-09-05 09:11:50,273 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,273 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,273 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,273 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,273 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-8yqgtoh6
2025-09-05 09:11:50,273 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,273 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,274 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,275 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,275 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,277 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,277 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,278 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,278 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,279 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:46169
2025-09-05 09:11:50,280 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:46169
2025-09-05 09:11:50,280 - distributed.worker - INFO -          dashboard at:          10.6.101.63:43451
2025-09-05 09:11:50,280 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,280 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,280 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,280 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,280 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,280 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-t82_nfi1
2025-09-05 09:11:50,280 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,280 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,281 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,281 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,281 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,283 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,284 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:34555
2025-09-05 09:11:50,284 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:34555
2025-09-05 09:11:50,284 - distributed.worker - INFO -          dashboard at:          10.6.101.63:34915
2025-09-05 09:11:50,284 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,284 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,284 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,284 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,284 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-34a20xoc
2025-09-05 09:11:50,284 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,284 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,284 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,286 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:43453
2025-09-05 09:11:50,286 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,286 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:43453
2025-09-05 09:11:50,286 - distributed.worker - INFO -          dashboard at:          10.6.101.63:38279
2025-09-05 09:11:50,286 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,286 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,286 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,286 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,286 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,286 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-993egzl7
2025-09-05 09:11:50,286 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,287 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,287 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,288 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,289 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,290 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,290 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:38259
2025-09-05 09:11:50,290 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:38259
2025-09-05 09:11:50,290 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,290 - distributed.worker - INFO -          dashboard at:          10.6.101.63:37883
2025-09-05 09:11:50,290 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,290 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,290 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,290 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,290 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-neslzuxv
2025-09-05 09:11:50,290 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,291 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:42593
2025-09-05 09:11:50,291 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:42593
2025-09-05 09:11:50,291 - distributed.worker - INFO -          dashboard at:          10.6.101.63:32891
2025-09-05 09:11:50,291 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,291 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,291 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,291 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,291 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-sz446an3
2025-09-05 09:11:50,291 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,292 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,293 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:36835
2025-09-05 09:11:50,293 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:36835
2025-09-05 09:11:50,293 - distributed.worker - INFO -          dashboard at:          10.6.101.63:37153
2025-09-05 09:11:50,293 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,293 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,293 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,293 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,293 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-_u1dq7nm
2025-09-05 09:11:50,293 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:45097
2025-09-05 09:11:50,294 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,294 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:45097
2025-09-05 09:11:50,294 - distributed.worker - INFO -          dashboard at:          10.6.101.63:34971
2025-09-05 09:11:50,294 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,294 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,294 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,294 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,294 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-mchnjla5
2025-09-05 09:11:50,294 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,297 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,297 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:44061
2025-09-05 09:11:50,297 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:44061
2025-09-05 09:11:50,297 - distributed.worker - INFO -          dashboard at:          10.6.101.63:34845
2025-09-05 09:11:50,297 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,297 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,297 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,297 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,297 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-lqrvmrv1
2025-09-05 09:11:50,298 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,298 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,298 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,299 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,299 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:40269
2025-09-05 09:11:50,299 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:40269
2025-09-05 09:11:50,300 - distributed.worker - INFO -          dashboard at:          10.6.101.63:41335
2025-09-05 09:11:50,300 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,300 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,300 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,300 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,300 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-t92u3y67
2025-09-05 09:11:50,300 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,300 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,300 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:46557
2025-09-05 09:11:50,300 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:46841
2025-09-05 09:11:50,300 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:46557
2025-09-05 09:11:50,300 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:46841
2025-09-05 09:11:50,300 - distributed.worker - INFO -          dashboard at:          10.6.101.63:35313
2025-09-05 09:11:50,300 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,300 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,300 - distributed.worker - INFO -          dashboard at:          10.6.101.63:43329
2025-09-05 09:11:50,300 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,300 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,300 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,300 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,300 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,300 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,300 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,300 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,300 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-csek5sso
2025-09-05 09:11:50,300 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-4_j7l4ti
2025-09-05 09:11:50,300 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,300 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,301 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:41013
2025-09-05 09:11:50,301 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:41013
2025-09-05 09:11:50,301 - distributed.worker - INFO -          dashboard at:          10.6.101.63:35579
2025-09-05 09:11:50,301 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,301 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,301 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,301 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,301 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:45579
2025-09-05 09:11:50,301 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-5eylnovr
2025-09-05 09:11:50,301 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:45579
2025-09-05 09:11:50,301 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,301 - distributed.worker - INFO -          dashboard at:          10.6.101.63:35171
2025-09-05 09:11:50,301 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,301 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,301 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,301 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,301 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-8vyhvzz0
2025-09-05 09:11:50,301 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,301 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:34011
2025-09-05 09:11:50,302 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:34011
2025-09-05 09:11:50,302 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,302 - distributed.worker - INFO -          dashboard at:          10.6.101.63:41185
2025-09-05 09:11:50,302 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,302 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,302 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,302 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,302 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-m64rityx
2025-09-05 09:11:50,302 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,305 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:40929
2025-09-05 09:11:50,305 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:40929
2025-09-05 09:11:50,305 - distributed.worker - INFO -          dashboard at:          10.6.101.63:43917
2025-09-05 09:11:50,305 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,305 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,305 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,305 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,305 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-lrb082m7
2025-09-05 09:11:50,305 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,307 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,308 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,309 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,309 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,310 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,310 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,312 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,312 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.63:43197
2025-09-05 09:11:50,312 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.63:43197
2025-09-05 09:11:50,312 - distributed.worker - INFO -          dashboard at:          10.6.101.63:34253
2025-09-05 09:11:50,312 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,312 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,312 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:50,312 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:50,312 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-wn6bfind
2025-09-05 09:11:50,312 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,312 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,313 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,313 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,315 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,315 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,316 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,316 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,317 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,317 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,318 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,318 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,319 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,320 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,320 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,320 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,320 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,323 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,325 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,325 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,326 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,326 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,327 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,327 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,328 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,328 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,329 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,329 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,331 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,331 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,332 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,332 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,333 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,333 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,334 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,335 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,335 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,336 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,336 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,337 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,338 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,338 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,338 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,339 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,340 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,340 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,340 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,340 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,341 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,341 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,342 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,342 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:50,344 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:50,345 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:50,345 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:50,347 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:12:06,704 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,704 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,704 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,704 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,705 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,705 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,705 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,705 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,705 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,705 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,705 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,706 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,705 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,706 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,706 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,706 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,706 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,706 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,706 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,706 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,706 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,706 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,706 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,706 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,707 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,707 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,707 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,707 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,707 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,707 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,707 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,707 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,708 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,708 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,708 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,708 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,708 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,708 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,708 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,708 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,708 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,708 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,708 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,708 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,708 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,708 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,708 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,709 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,709 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,708 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,709 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,709 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,709 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,709 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,709 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,709 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,709 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,709 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,709 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,709 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,709 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,709 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,709 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,708 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,709 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,709 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,709 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,709 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,710 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,708 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,710 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,710 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,710 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,708 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,710 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,710 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,711 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,711 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,711 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,711 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,711 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,711 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,711 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,711 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,710 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,710 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,712 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,712 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,712 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,711 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,711 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,713 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,713 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,712 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,714 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,716 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,717 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,717 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,718 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,712 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,719 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,719 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,719 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,720 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:09,506 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,506 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,506 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,507 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,507 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,507 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,507 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,508 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,507 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,507 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,508 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,507 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,507 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,508 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,508 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,508 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,509 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,508 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,509 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,508 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,508 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,509 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,509 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,509 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,509 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,509 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,509 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,509 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,509 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,509 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,509 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,509 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,510 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,510 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,510 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,510 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,510 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,510 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,510 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,509 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,510 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,510 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,510 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,510 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,511 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,510 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,510 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,510 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,510 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,511 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,511 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,511 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,510 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,509 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,511 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,511 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,511 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,511 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,512 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,511 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,512 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,511 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,512 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,512 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,510 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,512 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,512 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,511 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,511 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,512 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,510 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,511 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,511 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,512 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,512 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,512 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,512 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,512 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,513 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,513 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,512 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,513 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,511 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,513 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,513 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,513 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,513 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,514 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,514 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,514 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,514 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,514 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,514 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,514 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,514 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,514 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,514 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,515 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,515 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,515 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,515 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,515 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,515 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,515 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,892 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,892 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,892 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,892 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,892 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,892 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,893 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,893 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,893 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,893 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,893 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,893 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,893 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,893 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,894 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,894 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,894 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,894 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,894 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,895 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,895 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,895 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,895 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,895 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,895 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,895 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,895 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,896 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,896 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,896 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,896 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,896 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,896 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,896 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,897 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,897 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,896 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,897 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,897 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,897 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,897 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,897 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,897 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,897 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,897 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,897 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,897 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,898 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,898 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,898 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,898 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,898 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,898 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,898 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,898 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,898 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,898 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,898 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,898 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,898 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,898 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,898 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,899 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,899 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,899 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,899 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,898 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,899 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,899 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,899 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,900 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,900 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,900 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,900 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,901 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,901 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,901 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,901 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,901 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,901 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,901 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,901 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,901 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,901 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,902 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,902 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,902 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,902 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,902 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,902 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,902 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,902 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,902 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,902 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,902 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,902 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,902 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,902 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,902 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,903 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,902 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,905 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,909 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,914 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:10,349 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,349 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,349 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,349 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,350 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,350 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,350 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,350 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,350 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,350 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,350 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,350 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,350 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,351 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,351 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,351 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,351 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,351 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,351 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,351 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,351 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,351 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,351 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,351 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,352 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,352 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,352 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,352 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,352 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,352 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,352 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,352 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,352 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,352 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,352 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,352 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,352 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,352 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,352 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,352 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,352 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,353 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,353 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,353 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,353 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,353 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,353 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,353 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,353 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,353 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,353 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,353 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,353 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,354 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,354 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,354 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,354 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,354 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,354 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,354 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,354 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,354 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,354 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,354 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,354 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,354 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,354 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,354 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,354 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,355 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,355 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,355 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,355 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,355 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,355 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,355 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,355 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,355 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,355 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,355 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,355 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,355 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,355 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,355 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,356 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,356 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,356 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,356 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,356 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,356 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,356 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,356 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,357 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,357 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,357 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,357 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,357 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,357 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,357 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,358 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,358 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,358 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,358 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,360 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:16:18,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,243 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,243 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,243 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,243 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,243 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,243 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,243 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,243 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,243 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,243 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,244 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,244 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,246 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,292 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,322 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,328 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,328 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,329 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,344 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,344 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:18,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:48,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:48,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:48,790 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:49,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:50,205 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:50,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:50,596 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:50,998 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:51,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,498 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,518 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,520 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:53,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:55,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:56,385 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:57,226 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:57,228 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,534 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:01,227 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:01,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:01,817 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:03,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:03,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:03,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:04,552 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:04,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:04,555 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:04,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:05,480 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:05,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:10,252 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:12,363 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:12,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:12,888 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:12,892 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:13,045 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:13,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:13,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:14,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:14,950 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:14,952 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,292 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,595 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,601 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,678 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,832 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,834 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,835 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,844 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,957 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,611 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,613 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,915 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,267 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,091 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,815 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:21,233 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:22,178 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:23,973 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:23,975 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:29,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:29,921 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:30,523 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:30,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:31,160 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:31,421 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:31,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:31,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:31,971 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:32,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:32,119 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:32,457 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:33,201 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:33,464 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,355 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,040 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,540 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,540 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:37,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:37,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:37,574 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:37,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,137 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,747 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:39,126 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:39,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:39,881 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:39,882 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:39,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:40,573 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:40,685 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:41,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:41,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:42,222 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:44,592 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:45,656 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:47,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:49,694 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:49,699 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:50,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:50,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:50,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:51,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:51,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:51,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,206 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,655 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,806 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,866 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,898 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,693 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:58,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:59,485 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:59,558 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:59,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:00,939 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:02,816 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:03,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:03,295 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:03,561 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:08,004 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:08,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:10,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:13,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:14,188 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:14,485 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:14,674 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,420 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,610 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,702 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,772 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,774 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,779 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,959 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,965 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,210 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,369 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,374 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,525 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,667 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,902 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,942 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,981 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,994 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:18,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:18,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:32,953 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:32,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,067 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,270 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,303 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,304 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,528 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,790 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,792 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,924 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,223 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,225 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,512 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,514 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,520 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,521 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,871 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:38,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:39,206 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:39,208 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:39,221 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:39,223 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:39,226 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:39,227 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:42,495 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:42,887 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:43,164 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:43,496 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:43,834 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:43,915 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,192 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,610 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,794 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,798 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,487 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,660 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,661 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,085 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,542 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,776 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,869 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,226 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,532 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,535 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,204 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,206 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:49,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:50,511 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:50,514 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:50,950 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:52,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:53,978 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:54,380 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:54,386 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:55,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:07,267 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:08,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,643 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,391 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,663 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,090 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,328 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,693 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:15,756 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:15,758 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:16,120 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:17,017 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:17,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:17,102 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:17,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:17,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:17,574 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:19,610 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:19,614 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:19,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:20,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:20,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:23,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:23,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:23,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:24,473 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:24,910 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:26,921 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:27,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:27,853 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:27,853 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:27,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:33,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:33,798 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:33,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:33,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:34,397 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:34,402 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:35,855 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:35,855 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:36,354 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:36,415 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:36,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:36,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:36,677 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:36,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:37,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:37,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,542 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,552 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,701 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,106 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,128 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,561 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,720 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,204 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:46,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:47,102 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,548 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,900 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,222 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,627 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,645 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,209 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:51,130 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:51,132 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:51,157 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:52,297 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:53,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:55,148 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:55,869 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:58,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:58,137 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:58,138 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:00,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:00,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:01,012 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:01,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:10,894 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.101.37:36691
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2876, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.101.63:59392 remote=tcp://10.6.101.37:36691>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:20:11,268 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,269 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:46557. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,268 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,268 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,268 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,268 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,268 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,269 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,268 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,268 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,268 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,271 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:34011. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,268 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,268 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,271 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:45097. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,269 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,269 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,268 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,271 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:34555. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,269 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,271 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:44549. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,271 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,268 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,269 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,271 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:44995. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,268 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,269 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,271 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:40011. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,271 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,269 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,269 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,269 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,269 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,271 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:44061. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,269 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,271 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:39305. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,272 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:45579. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,269 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,272 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:38259. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,272 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:34029. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,270 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,272 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:35721. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,270 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,272 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:46169. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,269 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,272 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:41013. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,272 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:43453. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,270 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,270 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,273 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:40929. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,271 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,273 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:39603. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,271 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,271 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,273 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:39677. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,271 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,271 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,273 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:42665. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,273 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:36057. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,273 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:40269. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,271 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,273 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:43587. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,270 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,273 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:45885. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,269 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,274 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:34787. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,274 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:36835. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,274 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:46411. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,272 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,274 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:45689. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,274 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:43443. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,274 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:39297. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,274 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:36235. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,275 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:35125. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,275 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:43197. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,275 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:44215. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,275 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:40217. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,275 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:38851. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,275 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:37555. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,275 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:39317. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,276 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:46859. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,276 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:38307. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,276 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:40333. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,276 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:35183. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,279 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:39407'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,281 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2983, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,282 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,282 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,282 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,282 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,282 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,283 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:37485'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,285 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2448, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,286 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.38:32801, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,286 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.63:44857, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,286 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.63:45509, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,286 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2477, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,287 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,287 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,287 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,287 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,287 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,294 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:44101'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,294 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:42189'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,295 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:36515'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,295 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:34785'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,295 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:39287'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,295 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2433, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,295 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:32769'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,296 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 5470, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,296 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:40139'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,296 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:38635'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,296 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,296 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:46455'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,296 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 948, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,296 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.39:42869, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,296 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:43515'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,296 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 958, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,296 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,296 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.38:37287, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,297 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.39:44451, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,296 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3099, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,297 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:42861'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,297 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.54:43085, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,297 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.67:36563, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,297 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3916, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,297 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1782, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,297 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:40495'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,297 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 5144, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,297 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2830, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,297 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2479, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,297 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.45:41443, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,297 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.53:43571, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,297 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2971, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,297 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:39643'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,297 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.45:43603, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,297 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3446, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,298 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2958, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,298 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.70:34319, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,298 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:38331'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,298 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 826, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,298 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.58:42983, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,298 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2897, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,298 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:39343'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,298 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2446, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,298 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 5456, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,298 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:46663'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,298 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1181, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,298 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.40:44565, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,298 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.49:45161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,298 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:38249'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,299 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2837, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,299 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2471, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,299 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.44:46617, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,299 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3956, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,299 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:34179'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,299 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3936, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,299 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.64:38415, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,299 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3937, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,299 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:34891'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,300 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 607, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,300 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1457, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,299 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1227, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,300 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1141, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,300 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:36085'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,300 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 5162, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,300 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 949, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,300 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 5408, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,300 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:45967'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,300 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.64:34457, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,300 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1060, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,301 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:42691'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,301 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.67:35033, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,301 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3463, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,301 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:43881'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,301 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.39:38607, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,301 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3444, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,301 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:42251'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,301 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2271, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,301 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:43511'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,301 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1709, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,302 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3845, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,302 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.63:33025, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,302 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:36581'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,302 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1717, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,302 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.55:35157, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,302 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1274, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,302 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.56:36129, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,302 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:40545'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,302 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2472, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1038, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,303 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:42337'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 6522, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.66:44873, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.65:40403, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3663, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,303 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:39245'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1322, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,304 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:40915'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 5414, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,304 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:41661'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 5396, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3000, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,304 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:34493'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 5413, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 5452, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,304 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:40765'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.64:37909, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:38473'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.47:37615, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.67:35771, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:34073'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 5420, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 5626, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,306 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.37:39523, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,306 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1146, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,306 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:36479'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,306 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.60:34589, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,306 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2436, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,306 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 5453, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,306 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2475, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,306 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.38:35691, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,306 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:35857'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,306 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1084, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,306 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 5455, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,306 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.41:46137, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,307 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:40809'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,307 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 5415, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,307 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.63:42593, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,307 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.51:40773, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,307 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2202, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,307 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2867, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,307 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 7283, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,308 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2473, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,308 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 5218, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,308 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3983, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,308 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.63:39903, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,309 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1031, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,309 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.63:42593, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,309 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1271, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,319 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1198, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,322 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:35777'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,322 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:35143'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,323 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2300, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,324 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3303, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,324 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 993, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,324 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.54:42707, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,325 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.47:35233, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,325 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1064, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,326 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,326 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,329 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:11,334 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:11,442 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:11,644 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:11,691 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,693 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:40773. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,694 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:11,708 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.63:43068 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:11,716 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:34101'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,717 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,717 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,717 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,718 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,718 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,730 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,002 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:12,005 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:44857. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:12,020 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.63:43074 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:12,026 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:42133'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:12,026 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:12,027 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:12,027 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:12,027 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:12,027 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:12,034 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,106 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:12,108 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:39903. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:12,116 - distributed.worker - ERROR - failed during get data with tcp://10.6.101.63:39903 -> tcp://10.6.101.63:39317
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.101.63:39903 remote=tcp://10.6.101.63:59072>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-05 09:20:12,127 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.63:42790 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:12,130 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:46133'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:12,131 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:12,132 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:12,132 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:12,132 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:12,132 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:12,140 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,856 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:13,268 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:13,269 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:13,270 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:13,333 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:13,338 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:13,446 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:13,647 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:13,698 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:13,734 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:13,759 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:13,894 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:42189'. Reason: nanny-close-gracefully
2025-09-05 09:20:13,896 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:42189' closed.
2025-09-05 09:20:13,898 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:40915'. Reason: nanny-close-gracefully
2025-09-05 09:20:13,898 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:45967'. Reason: nanny-close-gracefully
2025-09-05 09:20:13,899 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:40915' closed.
2025-09-05 09:20:13,900 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:45967' closed.
2025-09-05 09:20:14,024 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:39407'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,025 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:39407' closed.
2025-09-05 09:20:14,038 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,098 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:34785'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,099 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:34785' closed.
2025-09-05 09:20:14,114 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:34101'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,115 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:34101' closed.
2025-09-05 09:20:14,144 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,207 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:14,307 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:14,425 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:42133'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,426 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:42133' closed.
2025-09-05 09:20:14,539 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:46133'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,540 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:46133' closed.
2025-09-05 09:20:14,711 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:14,712 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:14,821 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:14,861 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:15,267 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:32769'. Reason: nanny-close-gracefully
2025-09-05 09:20:15,269 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:32769' closed.
2025-09-05 09:20:15,273 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:15,274 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:15,275 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:15,398 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:15,400 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:42593. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,408 - distributed.worker - ERROR - failed during get data with tcp://10.6.101.63:42593 -> tcp://10.6.101.63:36235
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.101.63:42593 remote=tcp://10.6.101.63:56744>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-05 09:20:15,414 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:15,419 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.63:43144 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:15,422 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:42329'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,423 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:15,424 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:15,424 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:15,424 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:15,424 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:15,426 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1545d6c56710>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:15,432 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,463 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:15,465 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:39963. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,467 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:15,470 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:45509. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,480 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:15,480 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:15,485 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.63:42890 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:15,485 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.63:43058 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:15,489 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:32977'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,490 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:15,490 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:15,490 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:15,490 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:15,490 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:37273'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,490 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:15,491 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:15,491 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:15,491 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:15,491 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:15,491 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:15,492 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14674ab046d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:15,494 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x153584ac6610>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:15,499 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,501 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,606 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,647 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:35857'. Reason: nanny-close-gracefully
2025-09-05 09:20:15,648 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:35857' closed.
2025-09-05 09:20:15,650 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:34179'. Reason: nanny-close-gracefully
2025-09-05 09:20:15,652 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:34179' closed.
2025-09-05 09:20:15,671 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:42251'. Reason: nanny-close-gracefully
2025-09-05 09:20:15,672 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:42251' closed.
2025-09-05 09:20:15,763 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:16,089 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:16,091 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:44981. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:16,099 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:16,104 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.63:42952 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:16,108 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:41537'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:16,109 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2273, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:16,109 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:16,110 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:16,110 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:16,110 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:16,110 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:16,112 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14dab6a60fd0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:16,158 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:43511'. Reason: nanny-close-gracefully
2025-09-05 09:20:16,160 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:43511' closed.
2025-09-05 09:20:16,211 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:16,236 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,294 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,312 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:16,609 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:39643'. Reason: nanny-close-gracefully
2025-09-05 09:20:16,611 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:39643' closed.
2025-09-05 09:20:16,706 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:35777'. Reason: nanny-close-gracefully
2025-09-05 09:20:16,708 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:35777' closed.
2025-09-05 09:20:16,715 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:16,716 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:16,825 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,068 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:17,071 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:33025. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:17,071 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,084 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:17,089 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.63:42984 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:17,094 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:39027'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:17,095 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:17,095 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:17,095 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:17,095 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:17,095 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:17,098 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:17,104 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,137 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:40545'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,138 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:40545' closed.
2025-09-05 09:20:17,158 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:43881'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,159 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:43881' closed.
2025-09-05 09:20:17,176 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:36479'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,177 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:36479' closed.
2025-09-05 09:20:17,401 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,425 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,427 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,428 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,426 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:17,428 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,429 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:35077. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:17,435 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,440 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:17,445 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.63:43044 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:17,449 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:35863'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:17,450 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:17,450 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:17,450 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:17,450 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:17,450 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:17,452 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x15472be07890>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:17,458 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,501 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,502 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,505 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,610 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,781 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,781 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,836 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:42329'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,837 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:42329' closed.
2025-09-05 09:20:17,892 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:37273'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,893 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:37273' closed.
2025-09-05 09:20:17,911 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:32977'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,912 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:32977' closed.
2025-09-05 09:20:17,974 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,974 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,982 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:39287'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,983 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:39287' closed.
2025-09-05 09:20:18,050 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,051 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,240 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,297 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,374 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,407 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,408 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,512 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,610 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,613 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,648 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:36515'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,649 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:36515' closed.
2025-09-05 09:20:18,672 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:36085'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,673 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:36085' closed.
2025-09-05 09:20:18,999 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,997 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:19,000 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.63:46841. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,011 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:19,016 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.63:43194 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:19,020 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.63:34323'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,021 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:19,022 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:19,022 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:19,022 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:19,022 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:19,025 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:19,030 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,075 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,107 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,137 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,405 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,430 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,431 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,432 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,434 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,489 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:38473'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,490 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:38473' closed.
2025-09-05 09:20:19,505 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,512 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:39027'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,513 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:39027' closed.
2025-09-05 09:20:19,752 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,785 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,785 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,806 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:44101'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,807 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:44101' closed.
2025-09-05 09:20:19,842 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:39245'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,843 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:39245' closed.
2025-09-05 09:20:19,849 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:46455'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,850 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:46455' closed.
2025-09-05 09:20:19,876 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,878 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,915 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:34073'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,916 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:34073' closed.
2025-09-05 09:20:19,954 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,968 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:35863'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,969 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:35863' closed.
2025-09-05 09:20:19,976 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:46663'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,977 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:46663' closed.
2025-09-05 09:20:19,978 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,979 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,012 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:40809'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,014 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:40809' closed.
2025-09-05 09:20:20,054 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,056 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,184 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:37485'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,185 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:37485' closed.
2025-09-05 09:20:20,192 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:40139'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,193 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:40139' closed.
2025-09-05 09:20:20,372 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:36581'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,373 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:36581' closed.
2025-09-05 09:20:20,378 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,411 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,412 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:41661'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,412 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,413 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:41661' closed.
2025-09-05 09:20:20,432 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:41537'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,433 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:41537' closed.
2025-09-05 09:20:20,490 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:38249'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,491 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:38249' closed.
2025-09-05 09:20:20,516 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,614 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,616 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,776 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:40765'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,777 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:40765' closed.
2025-09-05 09:20:20,794 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:42691'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,795 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:42691' closed.
2025-09-05 09:20:20,811 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:38635'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,813 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:38635' closed.
2025-09-05 09:20:20,908 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:34891'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,909 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:34891' closed.
2025-09-05 09:20:21,003 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,012 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:39343'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,013 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:39343' closed.
2025-09-05 09:20:21,034 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,035 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:42337'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,036 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:42337' closed.
2025-09-05 09:20:21,142 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,395 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:43515'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,396 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:43515' closed.
2025-09-05 09:20:21,451 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:34323'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,452 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:34323' closed.
2025-09-05 09:20:21,621 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:35143'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,622 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:35143' closed.
2025-09-05 09:20:21,756 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,880 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,882 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,958 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,190 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:38331'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,191 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:38331' closed.
2025-09-05 09:20:22,274 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:42861'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,274 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:42861' closed.
2025-09-05 09:20:22,336 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:40495'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,337 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:40495' closed.
2025-09-05 09:20:22,382 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.63:34493'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,383 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.63:34493' closed.
2025-09-05 09:20:22,385 - distributed.dask_worker - INFO - End worker
