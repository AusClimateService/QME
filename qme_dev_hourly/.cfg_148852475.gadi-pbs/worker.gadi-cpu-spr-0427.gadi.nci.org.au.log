Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-05 09:11:53,654 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:39369'
2025-09-05 09:11:53,660 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:46449'
2025-09-05 09:11:53,665 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:37883'
2025-09-05 09:11:53,670 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:46693'
2025-09-05 09:11:53,676 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:41585'
2025-09-05 09:11:53,680 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:34167'
2025-09-05 09:11:53,684 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:39313'
2025-09-05 09:11:53,688 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:41529'
2025-09-05 09:11:53,692 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:37293'
2025-09-05 09:11:53,697 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:46437'
2025-09-05 09:11:53,700 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:39087'
2025-09-05 09:11:53,704 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:34225'
2025-09-05 09:11:53,708 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:34313'
2025-09-05 09:11:53,712 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:33363'
2025-09-05 09:11:53,717 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:37069'
2025-09-05 09:11:53,722 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:33841'
2025-09-05 09:11:53,727 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:46335'
2025-09-05 09:11:53,731 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:35363'
2025-09-05 09:11:53,736 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:36105'
2025-09-05 09:11:53,740 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:45301'
2025-09-05 09:11:53,818 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:43693'
2025-09-05 09:11:53,822 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:46879'
2025-09-05 09:11:53,827 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:35511'
2025-09-05 09:11:53,831 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:41987'
2025-09-05 09:11:53,835 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:36013'
2025-09-05 09:11:53,839 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:43191'
2025-09-05 09:11:53,843 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:44707'
2025-09-05 09:11:53,849 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:45569'
2025-09-05 09:11:53,853 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:35365'
2025-09-05 09:11:53,858 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:44261'
2025-09-05 09:11:53,862 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:35017'
2025-09-05 09:11:53,868 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:43417'
2025-09-05 09:11:53,872 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:33347'
2025-09-05 09:11:53,876 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:34293'
2025-09-05 09:11:53,879 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:33113'
2025-09-05 09:11:53,885 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:43497'
2025-09-05 09:11:53,889 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:35693'
2025-09-05 09:11:53,894 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:37273'
2025-09-05 09:11:53,899 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:32807'
2025-09-05 09:11:53,902 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:39921'
2025-09-05 09:11:53,906 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:36801'
2025-09-05 09:11:53,910 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:36467'
2025-09-05 09:11:53,914 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:34325'
2025-09-05 09:11:53,918 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:43241'
2025-09-05 09:11:53,922 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:39641'
2025-09-05 09:11:53,927 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:45529'
2025-09-05 09:11:53,932 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:34529'
2025-09-05 09:11:53,935 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:46567'
2025-09-05 09:11:53,938 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:37879'
2025-09-05 09:11:53,943 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:45541'
2025-09-05 09:11:53,948 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:45325'
2025-09-05 09:11:53,953 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.67:44413'
2025-09-05 09:11:54,882 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:40313
2025-09-05 09:11:54,882 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:38973
2025-09-05 09:11:54,882 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:40313
2025-09-05 09:11:54,882 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:38973
2025-09-05 09:11:54,882 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:37635
2025-09-05 09:11:54,882 - distributed.worker - INFO -          dashboard at:          10.6.101.67:37837
2025-09-05 09:11:54,882 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:35771
2025-09-05 09:11:54,882 - distributed.worker - INFO -          dashboard at:          10.6.101.67:45071
2025-09-05 09:11:54,882 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:34499
2025-09-05 09:11:54,882 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:37635
2025-09-05 09:11:54,883 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,883 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:35771
2025-09-05 09:11:54,883 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,883 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:34499
2025-09-05 09:11:54,883 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,883 - distributed.worker - INFO -          dashboard at:          10.6.101.67:45699
2025-09-05 09:11:54,883 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,883 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:45963
2025-09-05 09:11:54,883 - distributed.worker - INFO -          dashboard at:          10.6.101.67:37681
2025-09-05 09:11:54,883 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,883 - distributed.worker - INFO -          dashboard at:          10.6.101.67:33111
2025-09-05 09:11:54,883 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,883 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,883 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:45963
2025-09-05 09:11:54,883 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,883 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,883 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,883 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,883 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,883 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,883 - distributed.worker - INFO -          dashboard at:          10.6.101.67:38137
2025-09-05 09:11:54,883 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:33643
2025-09-05 09:11:54,883 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-w_e05w_m
2025-09-05 09:11:54,883 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,883 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,883 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-k421ph_w
2025-09-05 09:11:54,883 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,883 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,883 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:33643
2025-09-05 09:11:54,883 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,883 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,883 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,883 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,883 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,883 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,883 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,883 - distributed.worker - INFO -          dashboard at:          10.6.101.67:41837
2025-09-05 09:11:54,883 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,883 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-p9536dm4
2025-09-05 09:11:54,883 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-1rimuq5u
2025-09-05 09:11:54,883 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,883 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-qfqcs5km
2025-09-05 09:11:54,883 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,883 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,883 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,883 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,883 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,883 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-neqv76lz
2025-09-05 09:11:54,883 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,883 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,883 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,883 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-j5q7inww
2025-09-05 09:11:54,883 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,891 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:42117
2025-09-05 09:11:54,891 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:42117
2025-09-05 09:11:54,891 - distributed.worker - INFO -          dashboard at:          10.6.101.67:41535
2025-09-05 09:11:54,891 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,891 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,891 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,891 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,892 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-4z60zf61
2025-09-05 09:11:54,892 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,894 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:40927
2025-09-05 09:11:54,895 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:40927
2025-09-05 09:11:54,895 - distributed.worker - INFO -          dashboard at:          10.6.101.67:32773
2025-09-05 09:11:54,895 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,895 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,895 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,895 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,895 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-s2iaiprs
2025-09-05 09:11:54,895 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,905 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:43507
2025-09-05 09:11:54,905 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:43507
2025-09-05 09:11:54,905 - distributed.worker - INFO -          dashboard at:          10.6.101.67:39311
2025-09-05 09:11:54,905 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,905 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,905 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,905 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,905 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-qzjjfrf0
2025-09-05 09:11:54,905 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,906 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:38423
2025-09-05 09:11:54,906 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:38423
2025-09-05 09:11:54,906 - distributed.worker - INFO -          dashboard at:          10.6.101.67:34705
2025-09-05 09:11:54,906 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,906 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,906 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,906 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,906 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-rvvwbqxi
2025-09-05 09:11:54,906 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,911 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,911 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:40691
2025-09-05 09:11:54,911 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:40691
2025-09-05 09:11:54,911 - distributed.worker - INFO -          dashboard at:          10.6.101.67:42287
2025-09-05 09:11:54,911 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,911 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,911 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,911 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,911 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-va8hv6_b
2025-09-05 09:11:54,911 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,911 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,911 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,912 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:44175
2025-09-05 09:11:54,912 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:44175
2025-09-05 09:11:54,912 - distributed.worker - INFO -          dashboard at:          10.6.101.67:39493
2025-09-05 09:11:54,912 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,912 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,912 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,912 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,912 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-d9i6ktmq
2025-09-05 09:11:54,912 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,912 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,913 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:41291
2025-09-05 09:11:54,913 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:41291
2025-09-05 09:11:54,913 - distributed.worker - INFO -          dashboard at:          10.6.101.67:43059
2025-09-05 09:11:54,913 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,913 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,913 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,913 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,913 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-hjdbj4xf
2025-09-05 09:11:54,913 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,915 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:36511
2025-09-05 09:11:54,915 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:36511
2025-09-05 09:11:54,915 - distributed.worker - INFO -          dashboard at:          10.6.101.67:33951
2025-09-05 09:11:54,915 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,915 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,915 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,915 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,915 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-5ofxoy2w
2025-09-05 09:11:54,915 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,919 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,920 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,920 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,921 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,925 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:35033
2025-09-05 09:11:54,925 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:35033
2025-09-05 09:11:54,925 - distributed.worker - INFO -          dashboard at:          10.6.101.67:43535
2025-09-05 09:11:54,925 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,925 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,925 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,925 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,925 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-3oifow7n
2025-09-05 09:11:54,925 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,926 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:36755
2025-09-05 09:11:54,926 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:36755
2025-09-05 09:11:54,926 - distributed.worker - INFO -          dashboard at:          10.6.101.67:39411
2025-09-05 09:11:54,926 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,926 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,926 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,926 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,926 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-pfrxk8_0
2025-09-05 09:11:54,926 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,930 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:42397
2025-09-05 09:11:54,930 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:42397
2025-09-05 09:11:54,930 - distributed.worker - INFO -          dashboard at:          10.6.101.67:35817
2025-09-05 09:11:54,930 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,930 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,930 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,930 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,930 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-l3c_1ul9
2025-09-05 09:11:54,930 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,936 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,937 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,938 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,938 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:46247
2025-09-05 09:11:54,938 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:46247
2025-09-05 09:11:54,938 - distributed.worker - INFO -          dashboard at:          10.6.101.67:37791
2025-09-05 09:11:54,938 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,938 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,938 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,938 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,938 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-a3ap9mn_
2025-09-05 09:11:54,938 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,939 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,941 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,942 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,942 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,944 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,945 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,946 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,946 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,948 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,948 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,949 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,950 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,951 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,952 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,952 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,953 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,954 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,956 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,957 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,957 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,958 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,959 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,959 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,959 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,961 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,975 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,976 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,976 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,977 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:39171
2025-09-05 09:11:54,977 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:39171
2025-09-05 09:11:54,977 - distributed.worker - INFO -          dashboard at:          10.6.101.67:34283
2025-09-05 09:11:54,977 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,977 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,977 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:54,977 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:54,977 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-8mvepbyc
2025-09-05 09:11:54,977 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,978 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,978 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,979 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,979 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,980 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,981 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,982 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,982 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,983 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,984 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,984 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,984 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,986 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,986 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,987 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,987 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,989 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,989 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,990 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,990 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,992 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,992 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,994 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,994 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,995 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,995 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,997 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,997 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:54,998 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:54,998 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:54,999 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:54,999 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,000 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,001 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,002 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,002 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,004 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,020 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,021 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,021 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,023 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,064 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:42419
2025-09-05 09:11:55,064 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:42419
2025-09-05 09:11:55,064 - distributed.worker - INFO -          dashboard at:          10.6.101.67:43465
2025-09-05 09:11:55,065 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,065 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,065 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,065 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:34285
2025-09-05 09:11:55,065 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,065 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:34285
2025-09-05 09:11:55,065 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-32w4v1il
2025-09-05 09:11:55,065 - distributed.worker - INFO -          dashboard at:          10.6.101.67:34819
2025-09-05 09:11:55,065 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,065 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,065 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,065 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,065 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,065 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-scpir9r7
2025-09-05 09:11:55,065 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,071 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:43713
2025-09-05 09:11:55,071 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:43713
2025-09-05 09:11:55,071 - distributed.worker - INFO -          dashboard at:          10.6.101.67:41813
2025-09-05 09:11:55,071 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,071 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,071 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,071 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,071 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-tmsgeec2
2025-09-05 09:11:55,071 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,092 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,093 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,093 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,095 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,099 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,100 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,100 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,102 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,106 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,107 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,107 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,109 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,153 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:45347
2025-09-05 09:11:55,153 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:45347
2025-09-05 09:11:55,153 - distributed.worker - INFO -          dashboard at:          10.6.101.67:34553
2025-09-05 09:11:55,153 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,153 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,153 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,153 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,153 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-8t5ji_fm
2025-09-05 09:11:55,154 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,155 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:37739
2025-09-05 09:11:55,155 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:37739
2025-09-05 09:11:55,155 - distributed.worker - INFO -          dashboard at:          10.6.101.67:40909
2025-09-05 09:11:55,155 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,155 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,155 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,155 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,155 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-kno8s_r1
2025-09-05 09:11:55,155 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,166 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:44183
2025-09-05 09:11:55,166 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:44183
2025-09-05 09:11:55,167 - distributed.worker - INFO -          dashboard at:          10.6.101.67:33235
2025-09-05 09:11:55,167 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,167 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,167 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,167 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,167 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-oaqomnnj
2025-09-05 09:11:55,167 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,168 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:40715
2025-09-05 09:11:55,168 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:40715
2025-09-05 09:11:55,168 - distributed.worker - INFO -          dashboard at:          10.6.101.67:38069
2025-09-05 09:11:55,168 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,168 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,168 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,168 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,168 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-xqbixjsc
2025-09-05 09:11:55,168 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,170 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:35747
2025-09-05 09:11:55,170 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:35747
2025-09-05 09:11:55,170 - distributed.worker - INFO -          dashboard at:          10.6.101.67:38009
2025-09-05 09:11:55,170 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,170 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,170 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,171 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,171 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-6lizkny7
2025-09-05 09:11:55,171 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,180 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:43653
2025-09-05 09:11:55,180 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:43653
2025-09-05 09:11:55,180 - distributed.worker - INFO -          dashboard at:          10.6.101.67:38203
2025-09-05 09:11:55,180 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,180 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,180 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,180 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,180 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-s0ztnl74
2025-09-05 09:11:55,180 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,182 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,183 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,183 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,185 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,190 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,191 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,192 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,193 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,197 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,198 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,200 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,204 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,205 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,205 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,206 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,208 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,209 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,209 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,211 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,218 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,219 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,219 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,221 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,330 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:36227
2025-09-05 09:11:55,330 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:36227
2025-09-05 09:11:55,330 - distributed.worker - INFO -          dashboard at:          10.6.101.67:34485
2025-09-05 09:11:55,330 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,330 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,330 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,330 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,330 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-wtkjywj0
2025-09-05 09:11:55,330 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,343 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:36451
2025-09-05 09:11:55,343 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:36451
2025-09-05 09:11:55,343 - distributed.worker - INFO -          dashboard at:          10.6.101.67:42395
2025-09-05 09:11:55,343 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,343 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,343 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,343 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,344 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-ukrh0hhl
2025-09-05 09:11:55,344 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,346 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:45721
2025-09-05 09:11:55,346 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:45721
2025-09-05 09:11:55,346 - distributed.worker - INFO -          dashboard at:          10.6.101.67:38891
2025-09-05 09:11:55,346 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,346 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,346 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,347 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,347 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-dnhactuu
2025-09-05 09:11:55,347 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,347 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:37249
2025-09-05 09:11:55,348 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:37249
2025-09-05 09:11:55,348 - distributed.worker - INFO -          dashboard at:          10.6.101.67:37637
2025-09-05 09:11:55,348 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,348 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,348 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,348 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,348 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-rpfpvr7q
2025-09-05 09:11:55,348 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,348 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:36563
2025-09-05 09:11:55,348 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:41421
2025-09-05 09:11:55,349 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:36563
2025-09-05 09:11:55,349 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:41421
2025-09-05 09:11:55,349 - distributed.worker - INFO -          dashboard at:          10.6.101.67:41857
2025-09-05 09:11:55,349 - distributed.worker - INFO -          dashboard at:          10.6.101.67:43801
2025-09-05 09:11:55,349 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,349 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,349 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,349 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,349 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,349 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,349 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,349 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,349 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-5ckwccd2
2025-09-05 09:11:55,349 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-nui03h0n
2025-09-05 09:11:55,349 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,349 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,356 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:37313
2025-09-05 09:11:55,356 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:37313
2025-09-05 09:11:55,356 - distributed.worker - INFO -          dashboard at:          10.6.101.67:37667
2025-09-05 09:11:55,356 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,356 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,356 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,356 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,356 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-o94ibgjy
2025-09-05 09:11:55,356 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,360 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,361 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:43585
2025-09-05 09:11:55,361 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:43585
2025-09-05 09:11:55,361 - distributed.worker - INFO -          dashboard at:          10.6.101.67:44289
2025-09-05 09:11:55,361 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,361 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,361 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,361 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,361 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,362 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-bjvjg7vp
2025-09-05 09:11:55,361 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,362 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,363 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,367 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:35353
2025-09-05 09:11:55,367 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:35353
2025-09-05 09:11:55,367 - distributed.worker - INFO -          dashboard at:          10.6.101.67:39883
2025-09-05 09:11:55,367 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,367 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,367 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,367 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,367 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-k6uzsyoh
2025-09-05 09:11:55,367 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,367 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:44853
2025-09-05 09:11:55,367 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:44853
2025-09-05 09:11:55,367 - distributed.worker - INFO -          dashboard at:          10.6.101.67:42329
2025-09-05 09:11:55,367 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,367 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,367 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,367 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,367 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-eyvuyxen
2025-09-05 09:11:55,367 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,369 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:33013
2025-09-05 09:11:55,369 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,369 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:33013
2025-09-05 09:11:55,369 - distributed.worker - INFO -          dashboard at:          10.6.101.67:33115
2025-09-05 09:11:55,369 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,369 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,369 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,369 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,369 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-souxx7jo
2025-09-05 09:11:55,369 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,369 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,369 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,369 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:35623
2025-09-05 09:11:55,369 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:35623
2025-09-05 09:11:55,369 - distributed.worker - INFO -          dashboard at:          10.6.101.67:37651
2025-09-05 09:11:55,369 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,369 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,369 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,369 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,369 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-bes5rwj5
2025-09-05 09:11:55,370 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,370 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,376 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:43955
2025-09-05 09:11:55,376 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:43955
2025-09-05 09:11:55,376 - distributed.worker - INFO -          dashboard at:          10.6.101.67:43269
2025-09-05 09:11:55,376 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,376 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,376 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,376 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,376 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-3gj772ix
2025-09-05 09:11:55,377 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,377 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:41423
2025-09-05 09:11:55,377 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:41423
2025-09-05 09:11:55,377 - distributed.worker - INFO -          dashboard at:          10.6.101.67:35959
2025-09-05 09:11:55,377 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,377 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,377 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,377 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,377 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-w5s942qn
2025-09-05 09:11:55,377 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,378 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:41843
2025-09-05 09:11:55,378 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:41843
2025-09-05 09:11:55,378 - distributed.worker - INFO -          dashboard at:          10.6.101.67:43103
2025-09-05 09:11:55,378 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,378 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,378 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,378 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,378 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-jqj31o_a
2025-09-05 09:11:55,378 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,380 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,381 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,382 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,382 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:44111
2025-09-05 09:11:55,382 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:44111
2025-09-05 09:11:55,382 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:41731
2025-09-05 09:11:55,382 - distributed.worker - INFO -          dashboard at:          10.6.101.67:33645
2025-09-05 09:11:55,382 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:41731
2025-09-05 09:11:55,382 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,382 - distributed.worker - INFO -          dashboard at:          10.6.101.67:43883
2025-09-05 09:11:55,382 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,382 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,382 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,382 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,382 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,382 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,382 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-rf7fwqnv
2025-09-05 09:11:55,382 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,382 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,382 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-6xj8iswj
2025-09-05 09:11:55,382 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,383 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,386 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:36829
2025-09-05 09:11:55,386 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,386 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:36829
2025-09-05 09:11:55,386 - distributed.worker - INFO -          dashboard at:          10.6.101.67:37387
2025-09-05 09:11:55,386 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,386 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,386 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,386 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,386 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-01fdi4d2
2025-09-05 09:11:55,386 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,387 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,387 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,388 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,389 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,390 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,390 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,390 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:36987
2025-09-05 09:11:55,390 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:36987
2025-09-05 09:11:55,390 - distributed.worker - INFO -          dashboard at:          10.6.101.67:38171
2025-09-05 09:11:55,390 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,390 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,390 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,390 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,390 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-er8i1uvr
2025-09-05 09:11:55,390 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,391 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:46727
2025-09-05 09:11:55,391 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:46727
2025-09-05 09:11:55,391 - distributed.worker - INFO -          dashboard at:          10.6.101.67:37381
2025-09-05 09:11:55,391 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,391 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,391 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,391 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,391 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,391 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-6l7pljca
2025-09-05 09:11:55,391 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,393 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,394 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:41537
2025-09-05 09:11:55,394 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:41537
2025-09-05 09:11:55,394 - distributed.worker - INFO -          dashboard at:          10.6.101.67:39193
2025-09-05 09:11:55,394 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,394 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,394 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,394 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,394 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-nq7fl7e7
2025-09-05 09:11:55,394 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,394 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,395 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,396 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,397 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,398 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,398 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,400 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,400 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:40951
2025-09-05 09:11:55,400 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,400 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:40951
2025-09-05 09:11:55,400 - distributed.worker - INFO -          dashboard at:          10.6.101.67:34027
2025-09-05 09:11:55,400 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,400 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,400 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,400 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,400 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-uhhbks9p
2025-09-05 09:11:55,400 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,401 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,401 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,402 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,403 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,404 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,404 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,406 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,406 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,407 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,407 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,409 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,411 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,411 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.67:39261
2025-09-05 09:11:55,411 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.67:39261
2025-09-05 09:11:55,411 - distributed.worker - INFO -          dashboard at:          10.6.101.67:40637
2025-09-05 09:11:55,411 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,411 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,411 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:55,411 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:55,411 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-ssoncfje
2025-09-05 09:11:55,411 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,412 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,412 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,414 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,414 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,415 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,415 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,417 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,418 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,418 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,418 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,418 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,421 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,421 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,421 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,422 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,423 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,424 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,424 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,424 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,427 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,428 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,428 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,430 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,430 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,431 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,431 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,433 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,433 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,434 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,434 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,435 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,436 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,436 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,436 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,438 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,438 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,439 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,439 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,440 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,441 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,441 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,441 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,443 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,443 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,444 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,444 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,445 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:55,446 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:55,446 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:55,447 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:55,448 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:12:06,727 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,727 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,727 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,727 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,727 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,728 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,728 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,728 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,728 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,728 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,728 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,728 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,728 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,728 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,728 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,728 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,729 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,729 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,729 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,729 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,729 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,729 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,729 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,729 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,729 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,730 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,730 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,730 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,730 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,730 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,730 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,730 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,730 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,730 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,730 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,730 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,730 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,730 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,730 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,730 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,730 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,731 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,731 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,731 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,731 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,731 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,731 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,731 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,731 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,731 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,731 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,731 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,731 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,731 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,731 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,731 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,731 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,732 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,732 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,732 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,732 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,732 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,732 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,732 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,732 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,732 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,732 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,732 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,732 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,732 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,732 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,732 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,732 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,732 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,732 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,732 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,732 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,732 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,733 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,733 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,734 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,734 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,734 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,734 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,734 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,735 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,735 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,735 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,735 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,734 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,735 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,735 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,735 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,734 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,736 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,736 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,734 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,736 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,736 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,735 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,740 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,742 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,742 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,743 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:09,530 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,530 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,530 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,530 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,530 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,530 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,531 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,531 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,531 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,531 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,531 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,532 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,532 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,532 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,532 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,533 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,533 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,532 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,532 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,533 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,533 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,532 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,532 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,533 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,533 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,533 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,533 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,533 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,532 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,533 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,533 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,534 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,531 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,533 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,534 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,534 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,534 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,534 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,534 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,533 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,534 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,534 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,534 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,535 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,535 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,535 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,534 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,534 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,534 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,535 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,534 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,534 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,535 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,535 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,535 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,535 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,532 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,534 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,532 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,534 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,536 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,535 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,536 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,536 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,535 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,532 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,535 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,535 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,535 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,536 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,536 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,536 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,536 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,536 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,536 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,537 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,534 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,537 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,537 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,537 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,537 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,537 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,534 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,537 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,538 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,536 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,538 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,538 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,538 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,538 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,538 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,538 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,539 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,539 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,539 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,539 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,539 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,539 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,540 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,540 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,540 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,541 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,546 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,547 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,917 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,917 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,917 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,917 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,917 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,918 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,918 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,918 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,918 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,918 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,918 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,918 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,918 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,919 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,919 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,919 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,919 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,919 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,919 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,919 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,919 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,919 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,920 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,920 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,920 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,920 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,920 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,920 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,921 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,921 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,921 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,921 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,921 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,921 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,921 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,921 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,922 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,921 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,922 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,922 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,922 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,922 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,922 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,922 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,922 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,922 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,923 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,923 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,923 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,923 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,923 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,923 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,923 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,923 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,923 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,923 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,923 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,923 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,923 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,923 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,923 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,923 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,923 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,924 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,924 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,924 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,924 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,924 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,924 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,924 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,924 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,925 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,925 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,925 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,925 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,925 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,925 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,925 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,926 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,925 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,926 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,926 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,926 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,926 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,926 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,926 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,926 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,926 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,926 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,926 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,927 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,927 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,927 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,927 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,927 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,927 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,927 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,927 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,927 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,927 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,928 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,929 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,929 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,929 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:10,373 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,373 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,374 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,374 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,374 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,374 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,374 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,374 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,374 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,374 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,375 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,375 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,375 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,375 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,375 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,375 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,375 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,375 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,375 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,375 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,376 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,376 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,376 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,376 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,376 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,376 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,376 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,376 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,376 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,376 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,376 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,376 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,376 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,376 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,377 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,377 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,377 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,377 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,377 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,377 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,377 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,377 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,377 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,377 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,377 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,377 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,377 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,377 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,377 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,377 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,377 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,378 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,377 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,378 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,378 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,378 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,378 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,378 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,378 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,378 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,378 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,378 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,378 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,378 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,378 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,378 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,378 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,379 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,379 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,379 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,379 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,379 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,379 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,379 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,379 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,379 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,379 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,379 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,379 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,379 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,379 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,380 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,380 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,380 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,380 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,380 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,380 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,380 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,380 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,381 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,381 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,381 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,381 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,381 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,381 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,381 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,381 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,381 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,381 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,382 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,382 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,382 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,382 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,385 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:16:16,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,846 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,899 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,975 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,978 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,000 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,028 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,031 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,070 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,073 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,082 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,128 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,129 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,139 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,156 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,172 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,189 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,198 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,225 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,238 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,244 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,249 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,249 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,254 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,268 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,279 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,288 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:50,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:50,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:50,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:50,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:53,101 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:54,089 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:54,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:54,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:54,805 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:54,807 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:55,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:55,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:56,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:56,066 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:56,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:56,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:56,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:56,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:56,580 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:56,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:57,494 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:57,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:57,865 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:57,866 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,127 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,385 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,858 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,861 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,979 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,986 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:59,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:59,498 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:59,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:59,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:59,973 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:59,974 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:59,975 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:01,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:01,550 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:01,731 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:02,209 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:02,273 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:03,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:03,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:03,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:03,753 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:04,421 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:08,531 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:08,833 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:10,868 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:10,871 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:10,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:11,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:11,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:12,089 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:12,498 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:12,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:12,501 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:12,627 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:12,633 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:13,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:13,441 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:13,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:13,689 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:13,694 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:13,811 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:13,814 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:13,815 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:14,438 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,561 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,130 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,487 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,494 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,692 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,080 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,492 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,492 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,495 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,965 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,971 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:19,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:20,881 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:23,398 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:23,399 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:28,589 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:29,741 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:30,610 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:31,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:31,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:31,561 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:31,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:32,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:32,313 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:32,864 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,888 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,958 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,959 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,234 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,234 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,073 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,281 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,679 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,679 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,967 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:37,052 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:37,056 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:37,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:37,521 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:37,525 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,615 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,963 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,972 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:39,000 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:40,288 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:40,329 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:40,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:40,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:40,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:40,352 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:40,402 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:40,467 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:40,973 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:40,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:41,140 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:50,348 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:50,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:50,899 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:51,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:51,537 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:51,538 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:51,589 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:51,714 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:51,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,439 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,441 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,689 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,772 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,755 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,270 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,271 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,273 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,755 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:56,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:56,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:56,748 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:57,599 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:57,942 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:58,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:00,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:00,786 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:00,800 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:04,270 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:09,811 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:12,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:13,936 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:14,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:14,469 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:14,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:14,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:14,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:14,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,011 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,049 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,440 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,815 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,823 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,076 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,295 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,587 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,589 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,592 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,039 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,283 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,953 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:32,866 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,161 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,165 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,171 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,171 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,414 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,526 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,528 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,531 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,374 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,479 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,720 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,211 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:38,798 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:38,854 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:39,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:39,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:39,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:40,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:40,950 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:40,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:40,952 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:41,034 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:41,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:42,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:42,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,359 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,723 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,473 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,510 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,678 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,679 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,731 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,732 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,748 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,882 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,372 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,741 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,751 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:49,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:49,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:50,889 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:52,085 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:53,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:53,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:53,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:53,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:53,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:55,255 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:55,814 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:57,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:01,518 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:01,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:06,355 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:07,171 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:07,175 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:07,473 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:08,275 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:08,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:08,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:08,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:08,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,077 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,403 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,192 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,194 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,195 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,196 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,289 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,936 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,940 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:12,481 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,363 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,851 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,974 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,974 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,979 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:15,188 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:15,189 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:15,748 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:15,959 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:15,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:16,421 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:16,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:17,883 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:17,890 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:21,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:32,421 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:34,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:35,813 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:35,965 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:35,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:36,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:36,793 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:37,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:37,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:37,712 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:37,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,777 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,860 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,861 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,866 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,396 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,606 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,611 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,649 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,841 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,843 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,848 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,864 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,046 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,066 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,080 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,087 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,285 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,588 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,592 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,457 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,463 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,464 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,467 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,516 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,595 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:47,293 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,732 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,033 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,196 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,295 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,303 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,304 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,825 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,512 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,787 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,870 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,882 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,882 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,904 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:51,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:53,367 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:53,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:53,370 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:53,371 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:53,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:54,365 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:54,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:55,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:55,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:55,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:56,040 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:57,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:05,017 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:09,384 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:11,264 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,264 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:36987. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,264 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,265 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:42117. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,264 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,264 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,264 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,264 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,264 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,264 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,264 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,264 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,264 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,264 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,267 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:41731. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,264 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,264 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,264 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,264 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,264 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,265 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,264 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,266 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:41537. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,264 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,266 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:45721. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,264 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,266 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:44111. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,264 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,264 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,266 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:41291. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,264 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,266 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:40691. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,264 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,264 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,264 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,264 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,267 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:43507. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,265 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,267 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:40927. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,265 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,267 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:37635. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,265 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,267 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:36755. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,265 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,265 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,265 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,265 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,267 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:38423. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,267 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:33643. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,267 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:41423. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,265 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,268 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:35747. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,268 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:35623. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,268 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:36451. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,264 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,268 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:43585. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,268 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:41843. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,268 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:38973. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,268 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:36227. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,268 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:37313. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,268 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:42397. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,269 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:35353. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,269 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:37249. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,269 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:44853. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,269 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:40715. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,269 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:41421. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,269 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:44183. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,269 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:45347. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,269 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:43713. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,269 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:34285. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,270 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:39261. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,270 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:42419. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,270 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:39171. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,270 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:34499. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,275 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:43241'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,278 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2917, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,278 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1365, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,279 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,279 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,279 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,279 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,279 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,281 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:41529'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,282 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2469, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,283 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2453, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,283 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,283 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,283 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,283 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,283 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,294 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:33113'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,295 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:45541'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,295 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:44261'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,295 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:36801'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,296 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:33363'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,296 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2298, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,296 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:39087'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,296 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 4084, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,296 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3481, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,296 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:39313'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,296 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3078, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,296 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3969, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,297 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:34313'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,297 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2085, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,297 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 939, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,297 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3484, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,297 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:37883'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,297 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2386, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,297 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.66:43957, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,297 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:35363'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,297 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2190, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,298 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:41987'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,297 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2037, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,298 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2950, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,298 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3609, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,298 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:35017'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,298 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1124, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,298 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1800, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,298 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1914, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,298 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:37293'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,298 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2416, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,298 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,298 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 5521, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,299 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:34167'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,299 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2384, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,299 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:44707'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,299 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3163, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,299 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 901, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,299 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:39641'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,299 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,300 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:39369'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,300 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:46567'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,300 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1082, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,300 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.54:38091, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,300 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:34293'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,300 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,300 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1076, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,300 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.64:37837, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,301 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.49:36741, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,301 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1086, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,301 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.41:33917, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,301 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:36013'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,301 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.66:33053, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,301 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1806, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,301 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2356, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,301 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.60:34531, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,301 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3948, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,301 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 4064, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,301 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:46879'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,301 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2731, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,301 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.64:45759, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,301 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2816, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,301 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.67:44175, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,301 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,302 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.64:41181, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,302 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:34529'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,302 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1126, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,302 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1119, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,302 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2189, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,302 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:43497'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,302 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3177, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,302 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2818, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:45301'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1249, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 772, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,303 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3720, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,303 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,303 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:35511'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,304 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:43191'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2715, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2734, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,304 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3505, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,304 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:45325'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3718, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 934, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:36105'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2383, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 638, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:33347'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,305 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2413, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:35365'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,306 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:45529'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,307 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 933, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,307 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3188, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,307 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3296, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,307 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 869, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,307 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2920, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,307 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3300, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,307 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:45569'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,308 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:43693'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,308 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2872, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,308 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2879, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,308 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:32807'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,309 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:41585'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,309 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2817, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,309 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3989, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,309 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 587, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,309 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 4008, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,311 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1759, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,311 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1753, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,312 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 906, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,314 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3150, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,315 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2198, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,315 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,316 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,316 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,316 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,316 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,319 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,320 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,320 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,321 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,321 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,326 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,277 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:12,281 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:44175. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:12,281 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:12,283 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:36563. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:12,292 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:12,289 - distributed.worker - ERROR - failed during get data with tcp://10.6.101.67:44175 -> tcp://10.6.101.67:36451
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.101.67:44175 remote=tcp://10.6.101.67:60720>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-05 09:20:12,295 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:12,298 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.67:52226 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:12,298 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.67:52428 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:12,302 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:34225'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:12,303 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:12,303 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:12,304 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:12,304 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:12,304 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:12,304 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:34325'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:12,305 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:12,306 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:12,306 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:12,306 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:12,306 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:12,311 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,314 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,548 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,767 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:12,770 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:36829. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:12,779 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:12,784 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.67:52544 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:12,789 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:39921'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:12,790 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:12,790 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:12,790 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:12,790 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:12,791 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:12,798 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,799 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:12,803 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:46727. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:12,817 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:12,822 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.67:52560 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:12,827 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:37879'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:12,828 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:12,828 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:12,828 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:12,829 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:12,829 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:12,837 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:13,230 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:13,232 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:43653. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:13,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:13,249 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.67:52380 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:13,253 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:43417'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:13,254 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:13,254 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:13,255 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:13,255 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:13,255 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:13,259 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:13,266 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:13,330 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:13,873 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:37293'. Reason: nanny-close-gracefully
2025-09-05 09:20:13,876 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:37293' closed.
2025-09-05 09:20:14,314 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,318 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,498 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:14,499 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:14,500 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:37739. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:14,508 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:14,515 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.67:52336 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:14,519 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:37273'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:14,520 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:14,520 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:14,520 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:14,520 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:14,520 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:14,523 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14e0b8571890>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:14,541 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:14,552 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,802 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,806 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:34225'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,807 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:34325'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,808 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:34225' closed.
2025-09-05 09:20:14,808 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:34325' closed.
2025-09-05 09:20:14,840 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,857 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:14,932 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:33113'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,936 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:33113' closed.
2025-09-05 09:20:15,193 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:39921'. Reason: nanny-close-gracefully
2025-09-05 09:20:15,195 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:39921' closed.
2025-09-05 09:20:15,269 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:15,295 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:37879'. Reason: nanny-close-gracefully
2025-09-05 09:20:15,297 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:37879' closed.
2025-09-05 09:20:15,353 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,572 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,664 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:43417'. Reason: nanny-close-gracefully
2025-09-05 09:20:15,665 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:43417' closed.
2025-09-05 09:20:15,911 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,917 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,119 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:16,121 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:43955. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:16,131 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:16,137 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.67:52500 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:16,143 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:36467'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:16,144 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:16,144 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:16,144 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:16,144 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:16,144 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:16,147 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14f3e3155950>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:16,155 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,279 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:16,281 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:46247. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:16,293 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:16,299 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.67:52282 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:16,303 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:46335'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:16,304 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:16,305 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:16,305 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:16,305 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:16,305 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:16,309 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1460b588fad0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:16,315 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,504 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:16,545 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:16,839 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,861 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:16,898 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:44261'. Reason: nanny-close-gracefully
2025-09-05 09:20:16,900 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:44261' closed.
2025-09-05 09:20:16,924 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:37273'. Reason: nanny-close-gracefully
2025-09-05 09:20:16,925 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:37273' closed.
2025-09-05 09:20:17,255 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:33347'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,256 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:33347' closed.
2025-09-05 09:20:17,305 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:17,307 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:45963. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:17,318 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:17,323 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.67:52154 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:17,328 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:46693'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:17,328 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:17,329 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:17,329 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:17,329 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:17,329 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:17,331 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14fef578a410>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:17,337 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,358 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,576 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,752 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:39313'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,754 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:39313' closed.
2025-09-05 09:20:17,776 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,777 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,916 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,921 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,961 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:46567'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,962 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:46567' closed.
2025-09-05 09:20:18,001 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,068 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,069 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,158 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,302 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:36801'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,304 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:36801' closed.
2025-09-05 09:20:18,306 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:43693'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,306 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:43693' closed.
2025-09-05 09:20:18,318 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,566 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:36467'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,567 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:36467' closed.
2025-09-05 09:20:18,643 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,648 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:18,650 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:35033. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,661 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:18,667 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.67:52264 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:18,671 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:33841'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,671 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:18,671 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:18,672 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:18,672 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:18,672 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:18,674 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x15298d00d5d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:18,677 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,681 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,688 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,730 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,730 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:18,732 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:40313. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,745 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:18,751 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:46335'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,751 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:46335' closed.
2025-09-05 09:20:18,750 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.67:52108 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:18,754 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:46437'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,754 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:18,755 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:18,755 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:18,755 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:18,755 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:18,757 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x15370d566d90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:18,762 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,843 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,979 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:18,981 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,982 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:40951. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:18,998 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.67:52572 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:19,002 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:35693'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,003 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:19,003 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:19,003 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:19,004 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:19,004 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:19,006 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14ba88be9410>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:19,011 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,238 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:36105'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,239 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:36105' closed.
2025-09-05 09:20:19,340 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,404 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:19,405 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:36511. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,410 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:19,414 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.67:52250 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.101.67:52250 remote=tcp://10.6.101.37:8753>: Stream is closed
2025-09-05 09:20:19,419 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:37069'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,420 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:19,420 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:19,420 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:19,420 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:19,420 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:19,422 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x146c1a22c110>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:19,427 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,465 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,717 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:46693'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,718 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:46693' closed.
2025-09-05 09:20:19,780 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,781 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,874 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,883 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,927 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,929 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,954 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,956 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,954 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:19,956 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:33013. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,969 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:19,973 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.67:52472 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:19,978 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:44413'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,979 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3120, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:19,979 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:19,979 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:19,979 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:19,979 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:19,979 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:19,981 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:19,986 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,005 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,050 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,069 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,073 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,073 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,080 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,082 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,134 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,143 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,165 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:45301'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,166 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:45301' closed.
2025-09-05 09:20:20,182 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:35511'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,183 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:35511' closed.
2025-09-05 09:20:20,187 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,194 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:20,196 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.67:35771. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:20,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:20,204 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.67:52136 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.101.67:52136 remote=tcp://10.6.101.37:8753>: Stream is closed
2025-09-05 09:20:20,209 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.67:46449'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:20,209 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:20,210 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:20,210 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:20,210 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:20,210 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:20,211 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,211 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:20,216 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,236 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,236 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,244 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,378 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:43241'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,380 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:43241' closed.
2025-09-05 09:20:20,543 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:34293'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,545 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:34293' closed.
2025-09-05 09:20:20,549 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:35365'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,550 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:35365' closed.
2025-09-05 09:20:20,647 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,682 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,685 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,734 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,765 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,986 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,014 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,052 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:41529'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,053 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:41529' closed.
2025-09-05 09:20:21,074 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:45529'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,075 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:45529' closed.
2025-09-05 09:20:21,084 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:33841'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,085 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:33841' closed.
2025-09-05 09:20:21,086 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:33363'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,087 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:33363' closed.
2025-09-05 09:20:21,123 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:37883'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,124 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:37883' closed.
2025-09-05 09:20:21,153 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:46437'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,154 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:46437' closed.
2025-09-05 09:20:21,407 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:35693'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,408 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:35693' closed.
2025-09-05 09:20:21,427 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:45541'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,428 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:45541' closed.
2025-09-05 09:20:21,430 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,471 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,812 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:37069'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,813 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:37069' closed.
2025-09-05 09:20:21,866 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:43497'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,867 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:43497' closed.
2025-09-05 09:20:21,878 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,887 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,931 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,933 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,958 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,961 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,990 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,055 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,073 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,084 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,086 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,139 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,148 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,191 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,216 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,219 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,240 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,240 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,249 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,283 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:34313'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,284 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:34313' closed.
2025-09-05 09:20:22,318 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:46879'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,319 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:46879' closed.
2025-09-05 09:20:22,342 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:45569'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,343 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:45569' closed.
2025-09-05 09:20:22,379 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:35363'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,380 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:35363' closed.
2025-09-05 09:20:22,382 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:43191'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,383 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:43191' closed.
2025-09-05 09:20:22,479 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:34529'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,483 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:34529' closed.
2025-09-05 09:20:22,485 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:36013'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,486 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:36013' closed.
2025-09-05 09:20:22,555 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:39641'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,556 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:39641' closed.
2025-09-05 09:20:22,569 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:34167'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,570 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:34167' closed.
2025-09-05 09:20:22,574 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:44413'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,575 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:44413' closed.
2025-09-05 09:20:22,633 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:35017'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,634 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:35017' closed.
2025-09-05 09:20:22,637 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:44707'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,638 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:44707' closed.
2025-09-05 09:20:22,654 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:45325'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,655 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:45325' closed.
2025-09-05 09:20:22,696 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:39087'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,696 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:39087' closed.
2025-09-05 09:20:22,725 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:46449'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,726 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:46449' closed.
2025-09-05 09:20:22,747 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:41585'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,748 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:41585' closed.
2025-09-05 09:20:22,774 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:39369'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,776 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:41987'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,777 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:39369' closed.
2025-09-05 09:20:22,777 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:41987' closed.
2025-09-05 09:20:22,875 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.67:32807'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,875 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.67:32807' closed.
2025-09-05 09:20:22,878 - distributed.dask_worker - INFO - End worker
