Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-10-01 12:01:58,247 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:41565'
2025-10-01 12:01:58,257 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:33495'
2025-10-01 12:01:58,262 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:41517'
2025-10-01 12:01:58,265 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:37591'
2025-10-01 12:01:58,268 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:40111'
2025-10-01 12:01:58,272 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:35557'
2025-10-01 12:01:58,275 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:39443'
2025-10-01 12:01:58,278 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:42853'
2025-10-01 12:01:58,283 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:36099'
2025-10-01 12:01:58,288 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:43089'
2025-10-01 12:01:58,293 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:46743'
2025-10-01 12:01:58,297 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:43601'
2025-10-01 12:01:58,301 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:37051'
2025-10-01 12:01:58,305 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:46681'
2025-10-01 12:01:58,309 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:40405'
2025-10-01 12:01:58,312 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:35171'
2025-10-01 12:01:58,317 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:37485'
2025-10-01 12:01:58,321 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:45669'
2025-10-01 12:01:58,325 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:38385'
2025-10-01 12:01:58,329 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:34641'
2025-10-01 12:01:58,333 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:41959'
2025-10-01 12:01:58,456 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:39073'
2025-10-01 12:01:58,460 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:37169'
2025-10-01 12:01:58,465 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:39483'
2025-10-01 12:01:58,470 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:46849'
2025-10-01 12:01:58,475 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:40943'
2025-10-01 12:01:58,479 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:45733'
2025-10-01 12:01:58,483 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:34493'
2025-10-01 12:01:58,487 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:39029'
2025-10-01 12:01:58,491 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:41635'
2025-10-01 12:01:58,495 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:37395'
2025-10-01 12:01:58,498 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:36463'
2025-10-01 12:01:58,502 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:33963'
2025-10-01 12:01:58,506 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:45365'
2025-10-01 12:01:58,511 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:38693'
2025-10-01 12:01:58,515 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:39221'
2025-10-01 12:01:58,520 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:42707'
2025-10-01 12:01:58,523 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:45611'
2025-10-01 12:01:58,528 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:43869'
2025-10-01 12:01:58,532 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:35817'
2025-10-01 12:01:58,536 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:43673'
2025-10-01 12:01:58,541 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:39245'
2025-10-01 12:01:58,544 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:38673'
2025-10-01 12:01:58,546 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:36707'
2025-10-01 12:01:58,549 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:43251'
2025-10-01 12:01:58,552 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:34339'
2025-10-01 12:01:58,555 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:45409'
2025-10-01 12:01:58,559 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.5.29:43519'
2025-10-01 12:01:59,632 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:39233
2025-10-01 12:01:59,632 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:45819
2025-10-01 12:01:59,632 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:33481
2025-10-01 12:01:59,632 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:41025
2025-10-01 12:01:59,632 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:41351
2025-10-01 12:01:59,633 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:39233
2025-10-01 12:01:59,633 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:45819
2025-10-01 12:01:59,633 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:33481
2025-10-01 12:01:59,633 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:40141
2025-10-01 12:01:59,633 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:41025
2025-10-01 12:01:59,633 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:41351
2025-10-01 12:01:59,633 - distributed.worker - INFO -          dashboard at:            10.6.5.29:33927
2025-10-01 12:01:59,633 - distributed.worker - INFO -          dashboard at:            10.6.5.29:40643
2025-10-01 12:01:59,633 - distributed.worker - INFO -          dashboard at:            10.6.5.29:40831
2025-10-01 12:01:59,633 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:40141
2025-10-01 12:01:59,633 - distributed.worker - INFO -          dashboard at:            10.6.5.29:37143
2025-10-01 12:01:59,633 - distributed.worker - INFO -          dashboard at:            10.6.5.29:38585
2025-10-01 12:01:59,633 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,633 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,633 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,633 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,633 - distributed.worker - INFO -          dashboard at:            10.6.5.29:39983
2025-10-01 12:01:59,633 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,633 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,633 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,633 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,633 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,633 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:01:59,633 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,633 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,633 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:01:59,633 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:01:59,633 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:01:59,633 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:01:59,633 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,633 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:01:59,633 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:01:59,633 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:01:59,633 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:01:59,633 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:01:59,633 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-p1r_wc9t
2025-10-01 12:01:59,633 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:01:59,633 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-8h4w605h
2025-10-01 12:01:59,633 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-xe99ss4l
2025-10-01 12:01:59,633 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-v_orjt0_
2025-10-01 12:01:59,633 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:01:59,633 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,633 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-gac2sfg_
2025-10-01 12:01:59,633 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,633 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,633 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,633 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-sjek4cpr
2025-10-01 12:01:59,633 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,633 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,643 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:38861
2025-10-01 12:01:59,643 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:38861
2025-10-01 12:01:59,643 - distributed.worker - INFO -          dashboard at:            10.6.5.29:44695
2025-10-01 12:01:59,643 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,643 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,643 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:01:59,643 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:01:59,644 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-m6v5ys3x
2025-10-01 12:01:59,644 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,645 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:01:59,646 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,646 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,646 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:01:59,647 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:01:59,647 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,647 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,648 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:01:59,649 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:01:59,649 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,649 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,650 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:01:59,651 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:01:59,652 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,652 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,652 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:01:59,654 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:01:59,654 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,654 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,655 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:01:59,657 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:40101
2025-10-01 12:01:59,657 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:40101
2025-10-01 12:01:59,657 - distributed.worker - INFO -          dashboard at:            10.6.5.29:38657
2025-10-01 12:01:59,657 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,658 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,658 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:01:59,658 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:01:59,658 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-4ajdd1t0
2025-10-01 12:01:59,658 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,659 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:01:59,659 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,659 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,660 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:01:59,661 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:01:59,661 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,661 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,662 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:01:59,674 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:01:59,674 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,675 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,676 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:01:59,781 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:36457
2025-10-01 12:01:59,782 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:36457
2025-10-01 12:01:59,782 - distributed.worker - INFO -          dashboard at:            10.6.5.29:44441
2025-10-01 12:01:59,782 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,782 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,782 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:01:59,782 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:01:59,782 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-txwrti0t
2025-10-01 12:01:59,782 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,803 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:01:59,804 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,804 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,805 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:01:59,815 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:46845
2025-10-01 12:01:59,815 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:46845
2025-10-01 12:01:59,815 - distributed.worker - INFO -          dashboard at:            10.6.5.29:41921
2025-10-01 12:01:59,815 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,816 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,816 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:01:59,816 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:01:59,816 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-utg5jbpz
2025-10-01 12:01:59,816 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,822 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:46407
2025-10-01 12:01:59,823 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:46407
2025-10-01 12:01:59,823 - distributed.worker - INFO -          dashboard at:            10.6.5.29:36539
2025-10-01 12:01:59,823 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,823 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,823 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:01:59,823 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:01:59,823 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-tb7210cq
2025-10-01 12:01:59,823 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,824 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:42803
2025-10-01 12:01:59,824 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:42803
2025-10-01 12:01:59,824 - distributed.worker - INFO -          dashboard at:            10.6.5.29:42147
2025-10-01 12:01:59,824 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,824 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,824 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:01:59,824 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:01:59,824 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-pl94md07
2025-10-01 12:01:59,825 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,826 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:42453
2025-10-01 12:01:59,826 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:42453
2025-10-01 12:01:59,827 - distributed.worker - INFO -          dashboard at:            10.6.5.29:42943
2025-10-01 12:01:59,827 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,827 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,827 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:01:59,827 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:01:59,827 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-116855xg
2025-10-01 12:01:59,827 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,829 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:44379
2025-10-01 12:01:59,829 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:44379
2025-10-01 12:01:59,829 - distributed.worker - INFO -          dashboard at:            10.6.5.29:38183
2025-10-01 12:01:59,829 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,829 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,829 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:01:59,829 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:01:59,829 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-pj2uf41p
2025-10-01 12:01:59,829 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,835 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:01:59,836 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,836 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,837 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:01:59,837 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:01:59,837 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,837 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,838 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:01:59,839 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:33081
2025-10-01 12:01:59,839 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:33081
2025-10-01 12:01:59,839 - distributed.worker - INFO -          dashboard at:            10.6.5.29:35677
2025-10-01 12:01:59,839 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,839 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,839 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:01:59,839 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:01:59,839 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-pr0p_xz8
2025-10-01 12:01:59,839 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,845 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:01:59,846 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,846 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,846 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:01:59,847 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,847 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,847 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:01:59,848 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:01:59,850 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:01:59,851 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,851 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,853 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:01:59,856 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:01:59,857 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,857 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,859 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:01:59,869 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:46075
2025-10-01 12:01:59,870 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:46075
2025-10-01 12:01:59,870 - distributed.worker - INFO -          dashboard at:            10.6.5.29:43107
2025-10-01 12:01:59,870 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,870 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,870 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:01:59,870 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:01:59,870 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-g5rejcds
2025-10-01 12:01:59,870 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,880 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:38319
2025-10-01 12:01:59,880 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:38319
2025-10-01 12:01:59,880 - distributed.worker - INFO -          dashboard at:            10.6.5.29:36979
2025-10-01 12:01:59,880 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,880 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,881 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:01:59,881 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:01:59,881 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-9rzxr0qq
2025-10-01 12:01:59,881 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,890 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:39319
2025-10-01 12:01:59,890 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:45045
2025-10-01 12:01:59,890 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:39319
2025-10-01 12:01:59,891 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:45045
2025-10-01 12:01:59,891 - distributed.worker - INFO -          dashboard at:            10.6.5.29:45609
2025-10-01 12:01:59,891 - distributed.worker - INFO -          dashboard at:            10.6.5.29:46027
2025-10-01 12:01:59,890 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:01:59,891 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,891 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,891 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,891 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,891 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:01:59,891 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:01:59,891 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:01:59,891 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:01:59,891 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-6k2_2m44
2025-10-01 12:01:59,891 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-4le8kscv
2025-10-01 12:01:59,891 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,891 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,891 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,891 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,893 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:01:59,902 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:01:59,902 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,902 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,904 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:01:59,904 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:36607
2025-10-01 12:01:59,904 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:36607
2025-10-01 12:01:59,904 - distributed.worker - INFO -          dashboard at:            10.6.5.29:41779
2025-10-01 12:01:59,904 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,904 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,904 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:01:59,904 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:01:59,904 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-skcfq197
2025-10-01 12:01:59,904 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,907 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:01:59,908 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,908 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,909 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:01:59,911 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:01:59,912 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,912 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,914 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:01:59,919 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:01:59,920 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,920 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,921 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:01:59,936 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:41101
2025-10-01 12:01:59,936 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:41101
2025-10-01 12:01:59,936 - distributed.worker - INFO -          dashboard at:            10.6.5.29:34885
2025-10-01 12:01:59,936 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,936 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,936 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:01:59,937 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:01:59,937 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-poayo028
2025-10-01 12:01:59,937 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,939 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:37255
2025-10-01 12:01:59,939 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:37255
2025-10-01 12:01:59,939 - distributed.worker - INFO -          dashboard at:            10.6.5.29:38977
2025-10-01 12:01:59,939 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,939 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,939 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:01:59,939 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:01:59,939 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-lfe2s9kz
2025-10-01 12:01:59,939 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,950 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:01:59,951 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,951 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,951 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:01:59,956 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:01:59,957 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,957 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,958 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:01:59,968 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:39755
2025-10-01 12:01:59,969 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:39755
2025-10-01 12:01:59,969 - distributed.worker - INFO -          dashboard at:            10.6.5.29:34615
2025-10-01 12:01:59,969 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,969 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,969 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:01:59,969 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:01:59,969 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-z5ajse99
2025-10-01 12:01:59,969 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,988 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:01:59,989 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,989 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,991 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:01:59,994 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:43935
2025-10-01 12:01:59,994 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:43935
2025-10-01 12:01:59,994 - distributed.worker - INFO -          dashboard at:            10.6.5.29:37097
2025-10-01 12:01:59,994 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:01:59,994 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:01:59,994 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:01:59,994 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:01:59,994 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-lkgdxm4a
2025-10-01 12:01:59,994 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,003 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:42545
2025-10-01 12:02:00,003 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:42545
2025-10-01 12:02:00,003 - distributed.worker - INFO -          dashboard at:            10.6.5.29:35285
2025-10-01 12:02:00,003 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,003 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,003 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:02:00,003 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:02:00,003 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-yx7f3zyc
2025-10-01 12:02:00,003 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,004 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:34413
2025-10-01 12:02:00,005 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:34413
2025-10-01 12:02:00,005 - distributed.worker - INFO -          dashboard at:            10.6.5.29:37879
2025-10-01 12:02:00,005 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,005 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,005 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:02:00,005 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:02:00,005 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-ms8je0ae
2025-10-01 12:02:00,005 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,006 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:38749
2025-10-01 12:02:00,006 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:38749
2025-10-01 12:02:00,006 - distributed.worker - INFO -          dashboard at:            10.6.5.29:33739
2025-10-01 12:02:00,007 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,007 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,007 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:02:00,007 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:02:00,007 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-s5y0tl5d
2025-10-01 12:02:00,007 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,008 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:40273
2025-10-01 12:02:00,008 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:40273
2025-10-01 12:02:00,008 - distributed.worker - INFO -          dashboard at:            10.6.5.29:39263
2025-10-01 12:02:00,008 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,008 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,008 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:02:00,008 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:02:00,008 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-m1k5t6hm
2025-10-01 12:02:00,008 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,010 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:38489
2025-10-01 12:02:00,010 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:38489
2025-10-01 12:02:00,010 - distributed.worker - INFO -          dashboard at:            10.6.5.29:46451
2025-10-01 12:02:00,010 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,010 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,010 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:02:00,010 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:02:00,010 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-b5acegt9
2025-10-01 12:02:00,010 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,015 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:02:00,017 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,017 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,018 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:02:00,018 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:02:00,019 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,019 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,019 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:02:00,023 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:33823
2025-10-01 12:02:00,023 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:33823
2025-10-01 12:02:00,023 - distributed.worker - INFO -          dashboard at:            10.6.5.29:39289
2025-10-01 12:02:00,023 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,023 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,023 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:02:00,023 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:02:00,023 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-o_s0s4hx
2025-10-01 12:02:00,023 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,024 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:02:00,024 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,025 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,026 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:02:00,027 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:02:00,028 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,028 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,029 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:02:00,029 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:02:00,030 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,030 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,030 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:02:00,031 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:02:00,031 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,031 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,033 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:02:00,043 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:02:00,044 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,044 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,045 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:02:00,048 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:34263
2025-10-01 12:02:00,048 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:34263
2025-10-01 12:02:00,048 - distributed.worker - INFO -          dashboard at:            10.6.5.29:42145
2025-10-01 12:02:00,048 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,048 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,049 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:02:00,049 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:02:00,049 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-fbwohhwi
2025-10-01 12:02:00,049 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,056 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:38003
2025-10-01 12:02:00,057 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:38003
2025-10-01 12:02:00,057 - distributed.worker - INFO -          dashboard at:            10.6.5.29:36553
2025-10-01 12:02:00,057 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,057 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,057 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:02:00,057 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:02:00,057 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-e870duvx
2025-10-01 12:02:00,057 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,058 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:37247
2025-10-01 12:02:00,058 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:37247
2025-10-01 12:02:00,058 - distributed.worker - INFO -          dashboard at:            10.6.5.29:41179
2025-10-01 12:02:00,058 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,058 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,058 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:02:00,058 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:02:00,058 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-gqrs62ed
2025-10-01 12:02:00,058 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,067 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:02:00,068 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,068 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,069 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:02:00,069 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:02:00,069 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,069 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,070 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:02:00,071 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:02:00,071 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,071 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,072 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:02:00,092 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:34101
2025-10-01 12:02:00,092 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:34101
2025-10-01 12:02:00,092 - distributed.worker - INFO -          dashboard at:            10.6.5.29:36791
2025-10-01 12:02:00,092 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,092 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,093 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:02:00,093 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:02:00,093 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-bmywx8pk
2025-10-01 12:02:00,093 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:35923
2025-10-01 12:02:00,093 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,093 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:35923
2025-10-01 12:02:00,093 - distributed.worker - INFO -          dashboard at:            10.6.5.29:43379
2025-10-01 12:02:00,093 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,093 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,093 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:02:00,093 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:02:00,093 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-fzportw6
2025-10-01 12:02:00,093 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,094 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:39609
2025-10-01 12:02:00,094 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:39609
2025-10-01 12:02:00,094 - distributed.worker - INFO -          dashboard at:            10.6.5.29:36851
2025-10-01 12:02:00,094 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:41579
2025-10-01 12:02:00,094 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,094 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:41579
2025-10-01 12:02:00,094 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,094 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:02:00,094 - distributed.worker - INFO -          dashboard at:            10.6.5.29:45107
2025-10-01 12:02:00,094 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:02:00,094 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,094 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-nlro9_w9
2025-10-01 12:02:00,094 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,094 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:02:00,094 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,094 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:02:00,094 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-0owhscgm
2025-10-01 12:02:00,094 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,094 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:41915
2025-10-01 12:02:00,094 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:41915
2025-10-01 12:02:00,094 - distributed.worker - INFO -          dashboard at:            10.6.5.29:41971
2025-10-01 12:02:00,094 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,095 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,095 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:02:00,095 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:02:00,095 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-pees_5au
2025-10-01 12:02:00,095 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,095 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:44309
2025-10-01 12:02:00,096 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:44309
2025-10-01 12:02:00,096 - distributed.worker - INFO -          dashboard at:            10.6.5.29:44581
2025-10-01 12:02:00,096 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,096 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,096 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:02:00,096 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:02:00,096 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-a_k8oa94
2025-10-01 12:02:00,096 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:35141
2025-10-01 12:02:00,096 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,096 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:35141
2025-10-01 12:02:00,096 - distributed.worker - INFO -          dashboard at:            10.6.5.29:34119
2025-10-01 12:02:00,096 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,096 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,096 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:02:00,096 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:02:00,096 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-yt5zx8kl
2025-10-01 12:02:00,096 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,099 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:38175
2025-10-01 12:02:00,099 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:38175
2025-10-01 12:02:00,100 - distributed.worker - INFO -          dashboard at:            10.6.5.29:43351
2025-10-01 12:02:00,100 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,100 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,100 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:02:00,100 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:02:00,100 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-f_nlt6m6
2025-10-01 12:02:00,100 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,101 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:44275
2025-10-01 12:02:00,101 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:44275
2025-10-01 12:02:00,101 - distributed.worker - INFO -          dashboard at:            10.6.5.29:45381
2025-10-01 12:02:00,101 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,101 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,101 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:02:00,101 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:02:00,101 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-z1akr0yn
2025-10-01 12:02:00,101 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,101 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:43311
2025-10-01 12:02:00,101 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:43311
2025-10-01 12:02:00,101 - distributed.worker - INFO -          dashboard at:            10.6.5.29:45495
2025-10-01 12:02:00,101 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,101 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,101 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:02:00,101 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:02:00,101 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-9ekjqrln
2025-10-01 12:02:00,102 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,103 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:35993
2025-10-01 12:02:00,103 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:35993
2025-10-01 12:02:00,103 - distributed.worker - INFO -          dashboard at:            10.6.5.29:39189
2025-10-01 12:02:00,103 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,103 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,103 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:02:00,103 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:02:00,103 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-vvpi8yh1
2025-10-01 12:02:00,103 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,105 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:39941
2025-10-01 12:02:00,105 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:39941
2025-10-01 12:02:00,106 - distributed.worker - INFO -          dashboard at:            10.6.5.29:38535
2025-10-01 12:02:00,106 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,106 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,106 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:02:00,106 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:02:00,106 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-zy88yi_n
2025-10-01 12:02:00,106 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,107 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:02:00,107 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,107 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,108 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:41703
2025-10-01 12:02:00,108 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:41703
2025-10-01 12:02:00,108 - distributed.worker - INFO -          dashboard at:            10.6.5.29:42547
2025-10-01 12:02:00,108 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:02:00,108 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,108 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,108 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:02:00,108 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:02:00,108 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-170iuslh
2025-10-01 12:02:00,108 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,109 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:37357
2025-10-01 12:02:00,109 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:37357
2025-10-01 12:02:00,109 - distributed.worker - INFO -          dashboard at:            10.6.5.29:42097
2025-10-01 12:02:00,109 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,109 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,109 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:02:00,109 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:02:00,109 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-y8550ger
2025-10-01 12:02:00,109 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,109 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:02:00,110 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,110 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,111 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:02:00,112 - distributed.worker - INFO -       Start worker at:      tcp://10.6.5.29:45341
2025-10-01 12:02:00,112 - distributed.worker - INFO -          Listening to:      tcp://10.6.5.29:45341
2025-10-01 12:02:00,112 - distributed.worker - INFO -          dashboard at:            10.6.5.29:41847
2025-10-01 12:02:00,112 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,112 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,112 - distributed.worker - INFO -               Threads:                          1
2025-10-01 12:02:00,113 - distributed.worker - INFO -                Memory:                  62.29 GiB
2025-10-01 12:02:00,113 - distributed.worker - INFO -       Local Directory: /jobfs/151329342.gadi-pbs/dask-scratch-space/worker-t0ltd5nx
2025-10-01 12:02:00,113 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,115 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:02:00,116 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,116 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,116 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:02:00,117 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,117 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,117 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:02:00,117 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:02:00,118 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:02:00,119 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,119 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:02:00,119 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,120 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,120 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,120 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:02:00,121 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,121 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,121 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:02:00,121 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:02:00,122 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:02:00,122 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:02:00,123 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,123 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,123 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:02:00,124 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,124 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:02:00,124 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,124 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:02:00,125 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,125 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,126 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:02:00,126 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:02:00,127 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:02:00,128 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,128 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,128 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:02:00,129 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,129 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,130 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:02:00,130 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:02:00,130 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:02:00,131 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,131 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,131 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:02:00,132 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,132 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,132 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:02:00,133 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-01 12:02:00,142 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 12:02:00,143 - distributed.worker - INFO -         Registered to:       tcp://10.6.5.28:8761
2025-10-01 12:02:00,143 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 12:02:00,145 - distributed.core - INFO - Starting established connection to tcp://10.6.5.28:8761
2025-10-02 08:54:35,018 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,018 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,018 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,020 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,019 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,019 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,020 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,151 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:43311. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,020 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,151 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:44309. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,020 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,151 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:35141. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,020 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,151 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:40101. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,020 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,020 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,151 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:45341. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,020 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,152 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:41915. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,020 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,152 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:44275. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,020 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,021 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,152 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:37357. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,020 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,020 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,152 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:38489. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,020 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,021 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,152 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:37247. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,021 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,152 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:38175. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,021 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,152 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:34263. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,021 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,153 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:35923. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,021 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,020 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,024 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,153 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:33823. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,021 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,021 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,153 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:39609. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,021 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,154 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:40273. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,022 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,154 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:39233. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,022 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,154 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:35993. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,022 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,154 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:33481. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,022 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,018 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:42676 remote=tcp://10.6.5.28:8761>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 08:54:35,019 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:42798 remote=tcp://10.6.5.28:8761>: Stream is closed
2025-10-02 08:54:35,018 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:42764 remote=tcp://10.6.5.28:8761>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 08:54:35,155 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:40141. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,018 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:42716 remote=tcp://10.6.5.28:8761>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 08:54:35,019 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:42700 remote=tcp://10.6.5.28:8761>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 08:54:35,021 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:42756 remote=tcp://10.6.5.28:8761>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 08:54:35,021 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:42792 remote=tcp://10.6.5.28:8761>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 08:54:35,021 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:42686 remote=tcp://10.6.5.28:8761>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 08:54:35,155 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,021 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:42666 remote=tcp://10.6.5.28:8761>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 08:54:35,156 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,021 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:42742 remote=tcp://10.6.5.28:8761>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 08:54:35,155 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:36457. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,021 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:42688 remote=tcp://10.6.5.28:8761>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 08:54:35,156 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,156 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,155 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:39941. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,156 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,156 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:46407. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,156 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:41703. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,156 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:41579. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,022 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:42668 remote=tcp://10.6.5.28:8761>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 08:54:35,156 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,021 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:42726 remote=tcp://10.6.5.28:8761>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 08:54:35,157 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:43935. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,022 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:42678 remote=tcp://10.6.5.28:8761>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 08:54:35,156 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,023 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.5.29:42778 remote=tcp://10.6.5.28:8761>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 08:54:35,156 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,153 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:38003. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,157 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:46075. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,153 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:34413. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,156 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,153 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:38861. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,157 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:38319. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,154 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:41351. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,156 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:34101. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,154 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:38749. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,157 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,156 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:37255. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,157 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:41025. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,156 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,157 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:45819. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,157 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,158 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:44379. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,158 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,158 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,158 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:39755. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,158 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:33081. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,158 - distributed.core - INFO - Connection to tcp://10.6.5.28:8761 has been closed.
2025-10-02 08:54:35,159 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:46845. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,156 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:42545. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,159 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:45045. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,159 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:39319. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,159 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:36607. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,159 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:42803. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,160 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:42453. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,161 - distributed.worker - INFO - Stopping worker at tcp://10.6.5.29:41101. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,169 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:33963'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,171 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,178 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:37395'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,178 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:43869'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,179 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,179 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:37591'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,179 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,179 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:43519'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,179 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,179 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:33495'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,180 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,173 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x150e05dbdbd0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 08:54:35,180 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,182 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,183 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,180 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14849c3776d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 08:54:35,189 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,181 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14fc78ed8850>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 08:54:35,182 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14c491bd4090>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 08:54:35,192 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,192 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,195 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,199 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:36707'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,199 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:43251'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,200 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:39221'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,200 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:39029'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,200 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,200 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:34493'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,200 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,200 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:35171'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,200 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,201 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:36463'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,201 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,201 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,201 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:37169'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,201 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,201 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:35817'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,201 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:45611'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,202 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:40405'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,202 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,202 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,202 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,202 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:38385'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,202 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:41959'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,202 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,202 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,202 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:43673'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,203 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,203 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:45733'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,203 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,203 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:34339'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,203 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:42707'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,203 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,203 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,203 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:40111'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,204 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:39443'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,204 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,204 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:46681'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,204 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:45365'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,204 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,204 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,204 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:39245'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,205 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,205 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:38673'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,205 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,205 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:35557'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,205 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,205 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:42853'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,205 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,205 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:40943'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,205 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,205 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,206 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:41517'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,206 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,206 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:37485'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,206 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,206 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:46849'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,206 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,206 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,206 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:41635'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,206 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,207 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:38693'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,207 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,207 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:34641'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,207 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,207 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:46743'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,207 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,207 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,207 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,207 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:45669'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,207 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,208 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,208 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:43601'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,208 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,208 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,208 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:45409'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,208 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,208 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:41565'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,208 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:36099'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,208 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,209 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:37051'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,209 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,209 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:43089'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,209 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,209 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:39073'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,209 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.5.29:39483'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 08:54:35,209 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,210 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,210 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,210 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,210 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 08:54:35,211 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,202 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14837941ca90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 08:54:35,203 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 08:54:35,202 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14ee1cf13350>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 08:54:35,214 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,203 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x154ee3f3df50>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 08:54:35,215 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,203 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14f8aa0e2810>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 08:54:35,204 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x15085d969490>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 08:54:35,215 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,204 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1541ca1a8150>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 08:54:35,216 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,205 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1546a1df0a10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 08:54:35,217 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,217 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,206 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x152e9cce1890>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 08:54:35,217 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,206 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1499de3bfed0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 08:54:35,218 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,208 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 08:54:35,207 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 08:54:35,218 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,219 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,208 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x146a4c828510>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 08:54:35,219 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,208 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14cbad4c8210>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 08:54:35,208 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14cff40d4a10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 08:54:35,220 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,220 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,220 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,221 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,221 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,209 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x150eb0a17210>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 08:54:35,222 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,222 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,211 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14bbe8e6f490>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 08:54:35,222 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,224 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,224 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,224 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,224 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,224 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,224 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,224 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,224 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,224 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,213 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14c924df1590>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 08:54:35,225 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,225 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,226 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,214 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14c3ad170ed0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 08:54:35,227 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,227 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,215 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14756bf64090>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 08:54:35,229 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,230 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:35,231 - distributed.nanny - INFO - Worker closed
2025-10-02 08:54:37,186 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 08:54:37,195 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 08:54:37,209 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 08:54:37,215 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 08:54:37,220 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 08:54:37,220 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 08:54:37,222 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 08:54:37,223 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 08:54:37,223 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 08:54:37,224 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 08:54:37,225 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 08:54:37,225 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 08:54:37,227 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 08:54:37,227 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 08:54:37,227 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 08:54:37,228 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 08:54:37,228 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 08:54:37,231 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 08:54:37,231 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 08:54:37,253 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 08:54:37,506 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:33963'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,513 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:33963' closed.
2025-10-02 08:54:37,532 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:37395'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,533 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:33495'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,537 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:37395' closed.
2025-10-02 08:54:37,539 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:43869'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,539 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:35817'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,541 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:33495' closed.
2025-10-02 08:54:37,541 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:41959'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,542 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:35171'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,543 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:43869' closed.
2025-10-02 08:54:37,543 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:35817' closed.
2025-10-02 08:54:37,543 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:41959' closed.
2025-10-02 08:54:37,544 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:35171' closed.
2025-10-02 08:54:37,602 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:46681'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,606 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:46681' closed.
2025-10-02 08:54:37,615 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:38385'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,616 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:38385' closed.
2025-10-02 08:54:37,699 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:40405'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,701 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:40405' closed.
2025-10-02 08:54:37,703 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:37591'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,704 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:37591' closed.
2025-10-02 08:54:37,709 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:45409'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,709 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:45409' closed.
2025-10-02 08:54:37,752 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:43519'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,753 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:43519' closed.
2025-10-02 08:54:37,755 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:45669'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,756 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:45669' closed.
2025-10-02 08:54:37,767 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:36463'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,768 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:36463' closed.
2025-10-02 08:54:37,779 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:40943'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,781 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:40943' closed.
2025-10-02 08:54:37,812 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:35557'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,813 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:35557' closed.
2025-10-02 08:54:37,822 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:41517'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,823 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:41517' closed.
2025-10-02 08:54:37,831 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:42853'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,832 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:42853' closed.
2025-10-02 08:54:37,834 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:39245'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,836 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:46849'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,837 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:39245' closed.
2025-10-02 08:54:37,838 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:46849' closed.
2025-10-02 08:54:37,840 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:39073'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,840 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:39073' closed.
2025-10-02 08:54:37,849 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:34493'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,849 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:38673'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,855 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:46743'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,856 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:39029'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,856 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:45611'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,857 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:41635'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,858 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:45733'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,860 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:34493' closed.
2025-10-02 08:54:37,860 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:38673' closed.
2025-10-02 08:54:37,860 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:36707'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,863 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:39483'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,864 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:34641'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,865 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:46743' closed.
2025-10-02 08:54:37,865 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:43251'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,866 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:43673'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,867 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:39029' closed.
2025-10-02 08:54:37,867 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:45611' closed.
2025-10-02 08:54:37,868 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:43089'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,868 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:41635' closed.
2025-10-02 08:54:37,870 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:45733' closed.
2025-10-02 08:54:37,871 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:41565'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,871 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:36707' closed.
2025-10-02 08:54:37,871 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:34339'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,872 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:37051'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,873 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:39443'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,873 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:36099'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,873 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:39483' closed.
2025-10-02 08:54:37,874 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:43601'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,874 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:34641' closed.
2025-10-02 08:54:37,874 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:43251' closed.
2025-10-02 08:54:37,874 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:43673' closed.
2025-10-02 08:54:37,874 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:37485'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,875 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:43089' closed.
2025-10-02 08:54:37,875 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:41565' closed.
2025-10-02 08:54:37,875 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:34339' closed.
2025-10-02 08:54:37,875 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:37051' closed.
2025-10-02 08:54:37,876 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:39443' closed.
2025-10-02 08:54:37,876 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:36099' closed.
2025-10-02 08:54:37,876 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:43601' closed.
2025-10-02 08:54:37,876 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:37485' closed.
2025-10-02 08:54:37,877 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:37169'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,878 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:40111'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,878 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:37169' closed.
2025-10-02 08:54:37,879 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:40111' closed.
2025-10-02 08:54:37,880 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:38693'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,880 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:38693' closed.
2025-10-02 08:54:37,885 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:42707'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,885 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:42707' closed.
2025-10-02 08:54:37,891 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:39221'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,892 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:39221' closed.
2025-10-02 08:54:37,926 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.5.29:45365'. Reason: nanny-close-gracefully
2025-10-02 08:54:37,926 - distributed.nanny - INFO - Nanny at 'tcp://10.6.5.29:45365' closed.
2025-10-02 08:54:37,928 - distributed.dask_worker - INFO - End worker
