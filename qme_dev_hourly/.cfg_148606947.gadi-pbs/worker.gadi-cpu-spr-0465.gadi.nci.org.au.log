Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-03 10:31:54,408 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:42165'
2025-09-03 10:31:54,417 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:36117'
2025-09-03 10:31:54,422 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:44697'
2025-09-03 10:31:54,429 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:39479'
2025-09-03 10:31:54,433 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:33655'
2025-09-03 10:31:54,484 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:34235'
2025-09-03 10:31:54,489 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:35269'
2025-09-03 10:31:54,495 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:33119'
2025-09-03 10:31:54,500 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:38067'
2025-09-03 10:31:54,506 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:44677'
2025-09-03 10:31:54,510 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:39851'
2025-09-03 10:31:54,519 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:33507'
2025-09-03 10:31:54,523 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:45429'
2025-09-03 10:31:54,527 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:36567'
2025-09-03 10:31:54,532 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:41941'
2025-09-03 10:31:54,537 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:37169'
2025-09-03 10:31:54,541 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:42871'
2025-09-03 10:31:54,546 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:44143'
2025-09-03 10:31:54,551 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:34809'
2025-09-03 10:31:54,556 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:42447'
2025-09-03 10:31:54,561 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:34191'
2025-09-03 10:31:54,566 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:34855'
2025-09-03 10:31:54,570 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:41615'
2025-09-03 10:31:54,575 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:34015'
2025-09-03 10:31:54,578 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:41815'
2025-09-03 10:31:54,584 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:39129'
2025-09-03 10:31:54,589 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:37433'
2025-09-03 10:31:54,593 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:40491'
2025-09-03 10:31:54,598 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:39653'
2025-09-03 10:31:54,603 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:45537'
2025-09-03 10:31:54,608 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:37243'
2025-09-03 10:31:54,612 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:35977'
2025-09-03 10:31:54,616 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:39637'
2025-09-03 10:31:54,621 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:42225'
2025-09-03 10:31:54,625 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:34323'
2025-09-03 10:31:54,629 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:45841'
2025-09-03 10:31:54,634 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:41583'
2025-09-03 10:31:54,638 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:42279'
2025-09-03 10:31:54,642 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:44971'
2025-09-03 10:31:54,645 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:42253'
2025-09-03 10:31:54,650 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:33109'
2025-09-03 10:31:54,806 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:33121'
2025-09-03 10:31:54,809 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:42677'
2025-09-03 10:31:54,813 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:36961'
2025-09-03 10:31:54,818 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:43425'
2025-09-03 10:31:54,821 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:33033'
2025-09-03 10:31:54,826 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:43721'
2025-09-03 10:31:54,831 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:38475'
2025-09-03 10:31:54,834 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:45151'
2025-09-03 10:31:54,839 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:38187'
2025-09-03 10:31:54,843 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:45451'
2025-09-03 10:31:54,848 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:34223'
2025-09-03 10:31:54,854 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:46177'
2025-09-03 10:31:54,859 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:36033'
2025-09-03 10:31:54,863 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:44773'
2025-09-03 10:31:54,865 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:39789'
2025-09-03 10:31:54,868 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:41493'
2025-09-03 10:31:54,872 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:44573'
2025-09-03 10:31:54,877 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:39431'
2025-09-03 10:31:54,884 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:37041'
2025-09-03 10:31:54,888 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:35141'
2025-09-03 10:31:54,892 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:38137'
2025-09-03 10:31:54,896 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:43617'
2025-09-03 10:31:54,900 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:36945'
2025-09-03 10:31:54,904 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:46531'
2025-09-03 10:31:54,912 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:34703'
2025-09-03 10:31:54,916 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:45585'
2025-09-03 10:31:54,920 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:44617'
2025-09-03 10:31:54,925 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:44421'
2025-09-03 10:31:54,930 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:43223'
2025-09-03 10:31:54,933 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:40705'
2025-09-03 10:31:54,938 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:45779'
2025-09-03 10:31:54,943 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:43751'
2025-09-03 10:31:54,948 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:38749'
2025-09-03 10:31:54,952 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:45573'
2025-09-03 10:31:54,955 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:42047'
2025-09-03 10:31:54,959 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:33521'
2025-09-03 10:31:54,962 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:34649'
2025-09-03 10:31:54,981 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:38655'
2025-09-03 10:31:54,987 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:46485'
2025-09-03 10:31:54,993 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:44587'
2025-09-03 10:31:54,998 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:42365'
2025-09-03 10:31:55,003 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:46475'
2025-09-03 10:31:55,009 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:43547'
2025-09-03 10:31:55,013 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:39159'
2025-09-03 10:31:55,018 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:45583'
2025-09-03 10:31:55,023 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:44251'
2025-09-03 10:31:55,027 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:38083'
2025-09-03 10:31:55,030 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:33403'
2025-09-03 10:31:55,035 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:41099'
2025-09-03 10:31:55,038 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:41387'
2025-09-03 10:31:55,043 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:35859'
2025-09-03 10:31:55,048 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:44065'
2025-09-03 10:31:55,053 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:44455'
2025-09-03 10:31:55,059 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:42233'
2025-09-03 10:31:55,064 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:44679'
2025-09-03 10:31:55,068 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:32781'
2025-09-03 10:31:55,073 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:33361'
2025-09-03 10:31:55,078 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:37897'
2025-09-03 10:31:55,083 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:46115'
2025-09-03 10:31:55,087 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:43067'
2025-09-03 10:31:55,092 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:33821'
2025-09-03 10:31:55,503 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:43597'
2025-09-03 10:31:55,507 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.33:42197'
2025-09-03 10:31:56,525 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:34841
2025-09-03 10:31:56,525 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:44859
2025-09-03 10:31:56,525 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:32787
2025-09-03 10:31:56,525 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:33685
2025-09-03 10:31:56,525 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:42963
2025-09-03 10:31:56,525 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:35905
2025-09-03 10:31:56,525 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:46447
2025-09-03 10:31:56,525 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:34841
2025-09-03 10:31:56,525 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:35209
2025-09-03 10:31:56,525 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:35165
2025-09-03 10:31:56,525 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:44859
2025-09-03 10:31:56,525 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:32955
2025-09-03 10:31:56,525 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:39633
2025-09-03 10:31:56,525 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:40833
2025-09-03 10:31:56,525 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:37939
2025-09-03 10:31:56,525 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:32787
2025-09-03 10:31:56,525 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:38445
2025-09-03 10:31:56,525 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:45839
2025-09-03 10:31:56,525 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:42759
2025-09-03 10:31:56,525 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:33685
2025-09-03 10:31:56,525 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:43501
2025-09-03 10:31:56,525 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:42963
2025-09-03 10:31:56,525 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:35905
2025-09-03 10:31:56,525 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:44807
2025-09-03 10:31:56,525 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:46447
2025-09-03 10:31:56,525 - distributed.worker - INFO -          dashboard at:          10.6.102.33:41735
2025-09-03 10:31:56,525 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:35209
2025-09-03 10:31:56,525 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:39745
2025-09-03 10:31:56,525 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:46879
2025-09-03 10:31:56,525 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:35165
2025-09-03 10:31:56,525 - distributed.worker - INFO -          dashboard at:          10.6.102.33:34037
2025-09-03 10:31:56,526 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:32955
2025-09-03 10:31:56,526 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:39633
2025-09-03 10:31:56,526 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:40833
2025-09-03 10:31:56,526 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:37939
2025-09-03 10:31:56,526 - distributed.worker - INFO -          dashboard at:          10.6.102.33:43601
2025-09-03 10:31:56,526 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:38445
2025-09-03 10:31:56,526 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:45839
2025-09-03 10:31:56,526 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:42759
2025-09-03 10:31:56,526 - distributed.worker - INFO -          dashboard at:          10.6.102.33:45153
2025-09-03 10:31:56,526 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:43501
2025-09-03 10:31:56,526 - distributed.worker - INFO -          dashboard at:          10.6.102.33:41329
2025-09-03 10:31:56,526 - distributed.worker - INFO -          dashboard at:          10.6.102.33:42221
2025-09-03 10:31:56,526 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:44807
2025-09-03 10:31:56,526 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,526 - distributed.worker - INFO -          dashboard at:          10.6.102.33:46479
2025-09-03 10:31:56,526 - distributed.worker - INFO -          dashboard at:          10.6.102.33:35171
2025-09-03 10:31:56,526 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:39745
2025-09-03 10:31:56,526 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:46879
2025-09-03 10:31:56,526 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,526 - distributed.worker - INFO -          dashboard at:          10.6.102.33:46099
2025-09-03 10:31:56,526 - distributed.worker - INFO -          dashboard at:          10.6.102.33:43525
2025-09-03 10:31:56,526 - distributed.worker - INFO -          dashboard at:          10.6.102.33:33731
2025-09-03 10:31:56,526 - distributed.worker - INFO -          dashboard at:          10.6.102.33:36463
2025-09-03 10:31:56,526 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,526 - distributed.worker - INFO -          dashboard at:          10.6.102.33:38023
2025-09-03 10:31:56,526 - distributed.worker - INFO -          dashboard at:          10.6.102.33:42075
2025-09-03 10:31:56,526 - distributed.worker - INFO -          dashboard at:          10.6.102.33:43415
2025-09-03 10:31:56,526 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,526 - distributed.worker - INFO -          dashboard at:          10.6.102.33:36539
2025-09-03 10:31:56,526 - distributed.worker - INFO -          dashboard at:          10.6.102.33:46391
2025-09-03 10:31:56,526 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,526 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,526 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,526 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:40971
2025-09-03 10:31:56,526 - distributed.worker - INFO -          dashboard at:          10.6.102.33:39757
2025-09-03 10:31:56,526 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,526 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,526 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,526 - distributed.worker - INFO -          dashboard at:          10.6.102.33:37287
2025-09-03 10:31:56,526 - distributed.worker - INFO -          dashboard at:          10.6.102.33:38323
2025-09-03 10:31:56,526 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,526 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,526 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,526 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,526 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,526 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,526 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,526 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,526 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,526 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,526 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,526 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,526 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,526 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,526 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,526 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,526 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:40971
2025-09-03 10:31:56,526 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,526 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,526 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,526 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,526 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,526 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,526 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,526 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,526 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,526 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:46073
2025-09-03 10:31:56,526 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,526 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,526 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,526 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,526 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,526 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,526 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,526 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,526 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,526 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,526 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,526 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,526 - distributed.worker - INFO -          dashboard at:          10.6.102.33:36111
2025-09-03 10:31:56,526 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,526 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,526 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,526 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,526 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,526 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,526 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,526 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,526 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,527 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,526 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,526 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:46073
2025-09-03 10:31:56,526 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,526 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,526 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,526 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,526 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,526 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,527 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,527 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-srw2oby6
2025-09-03 10:31:56,527 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,527 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,527 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,527 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,527 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,527 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-qp6fohe4
2025-09-03 10:31:56,527 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,527 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,527 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,527 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,527 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-a2xgo20o
2025-09-03 10:31:56,527 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,527 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-w3qc1p1o
2025-09-03 10:31:56,527 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,527 - distributed.worker - INFO -          dashboard at:          10.6.102.33:42573
2025-09-03 10:31:56,527 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,527 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-7m5m9rea
2025-09-03 10:31:56,527 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,527 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-vx3frxwd
2025-09-03 10:31:56,527 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-uz1qgwoj
2025-09-03 10:31:56,527 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ps_7qmic
2025-09-03 10:31:56,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,527 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-h1agfbj8
2025-09-03 10:31:56,527 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,527 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-xo4u43tt
2025-09-03 10:31:56,527 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,527 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,527 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-bo36npm5
2025-09-03 10:31:56,527 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-wd31iggd
2025-09-03 10:31:56,527 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-r72ck_oo
2025-09-03 10:31:56,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,527 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-3ju83ppn
2025-09-03 10:31:56,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,527 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-acwmvf56
2025-09-03 10:31:56,527 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,527 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ryk5fhgu
2025-09-03 10:31:56,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,527 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-n5ik9_zl
2025-09-03 10:31:56,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,527 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,527 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-4zqk0cvb
2025-09-03 10:31:56,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,527 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-wfgkbru7
2025-09-03 10:31:56,527 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-h9dc3220
2025-09-03 10:31:56,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,527 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,527 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,527 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-nnfztidg
2025-09-03 10:31:56,527 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,527 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-6j61lxce
2025-09-03 10:31:56,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,530 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:39625
2025-09-03 10:31:56,530 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:39625
2025-09-03 10:31:56,530 - distributed.worker - INFO -          dashboard at:          10.6.102.33:46627
2025-09-03 10:31:56,530 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,530 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,531 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,531 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,531 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-vytqgdgt
2025-09-03 10:31:56,531 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,538 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:33657
2025-09-03 10:31:56,538 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:33657
2025-09-03 10:31:56,539 - distributed.worker - INFO -          dashboard at:          10.6.102.33:43491
2025-09-03 10:31:56,539 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,539 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,539 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,539 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,539 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-3n9s4bel
2025-09-03 10:31:56,539 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,541 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:37995
2025-09-03 10:31:56,541 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:37995
2025-09-03 10:31:56,541 - distributed.worker - INFO -          dashboard at:          10.6.102.33:45183
2025-09-03 10:31:56,541 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,541 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,541 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,541 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,541 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-9r59nxdx
2025-09-03 10:31:56,541 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,542 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:40857
2025-09-03 10:31:56,542 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:40857
2025-09-03 10:31:56,542 - distributed.worker - INFO -          dashboard at:          10.6.102.33:37161
2025-09-03 10:31:56,542 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,543 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,543 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,543 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,543 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ktzhjg3k
2025-09-03 10:31:56,543 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,570 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:34753
2025-09-03 10:31:56,570 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:34753
2025-09-03 10:31:56,570 - distributed.worker - INFO -          dashboard at:          10.6.102.33:45635
2025-09-03 10:31:56,570 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,570 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,570 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,570 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,570 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-t2xvdfcn
2025-09-03 10:31:56,570 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,579 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:42291
2025-09-03 10:31:56,579 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:42291
2025-09-03 10:31:56,579 - distributed.worker - INFO -          dashboard at:          10.6.102.33:37557
2025-09-03 10:31:56,579 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,579 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,579 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,579 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,579 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-z4_y3ql0
2025-09-03 10:31:56,579 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,584 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:42437
2025-09-03 10:31:56,584 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:42437
2025-09-03 10:31:56,584 - distributed.worker - INFO -          dashboard at:          10.6.102.33:34297
2025-09-03 10:31:56,584 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,584 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,584 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,584 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,584 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-6yz69eo2
2025-09-03 10:31:56,584 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,591 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:45145
2025-09-03 10:31:56,591 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:45145
2025-09-03 10:31:56,591 - distributed.worker - INFO -          dashboard at:          10.6.102.33:39139
2025-09-03 10:31:56,591 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,591 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,591 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,591 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,591 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-k1t9k6p9
2025-09-03 10:31:56,591 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,593 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:32933
2025-09-03 10:31:56,594 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:32933
2025-09-03 10:31:56,594 - distributed.worker - INFO -          dashboard at:          10.6.102.33:41819
2025-09-03 10:31:56,594 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,594 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,594 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,594 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,594 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-li6p23e_
2025-09-03 10:31:56,594 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,642 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:40037
2025-09-03 10:31:56,642 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:40037
2025-09-03 10:31:56,642 - distributed.worker - INFO -          dashboard at:          10.6.102.33:33701
2025-09-03 10:31:56,642 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,642 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,642 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,642 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,643 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-1i6jc4_o
2025-09-03 10:31:56,643 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,686 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:36663
2025-09-03 10:31:56,686 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:36663
2025-09-03 10:31:56,686 - distributed.worker - INFO -          dashboard at:          10.6.102.33:43885
2025-09-03 10:31:56,686 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,686 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,686 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,686 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,686 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-dznbbd8m
2025-09-03 10:31:56,686 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,791 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:44319
2025-09-03 10:31:56,791 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:44319
2025-09-03 10:31:56,791 - distributed.worker - INFO -          dashboard at:          10.6.102.33:45381
2025-09-03 10:31:56,791 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,791 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,791 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,791 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,791 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-y1qwve4y
2025-09-03 10:31:56,791 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,852 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:39211
2025-09-03 10:31:56,852 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:39211
2025-09-03 10:31:56,852 - distributed.worker - INFO -          dashboard at:          10.6.102.33:36131
2025-09-03 10:31:56,852 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,852 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,852 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,852 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,852 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-vsq_wwzq
2025-09-03 10:31:56,852 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,989 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:43013
2025-09-03 10:31:56,989 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:43013
2025-09-03 10:31:56,989 - distributed.worker - INFO -          dashboard at:          10.6.102.33:37725
2025-09-03 10:31:56,989 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,989 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,989 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:56,989 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:56,989 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-lbw4v5yi
2025-09-03 10:31:56,989 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,007 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:42699
2025-09-03 10:31:57,007 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:42699
2025-09-03 10:31:57,007 - distributed.worker - INFO -          dashboard at:          10.6.102.33:40339
2025-09-03 10:31:57,007 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,007 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,007 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,007 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,008 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-nww4nbef
2025-09-03 10:31:57,008 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,008 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:43621
2025-09-03 10:31:57,008 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:43621
2025-09-03 10:31:57,008 - distributed.worker - INFO -          dashboard at:          10.6.102.33:43673
2025-09-03 10:31:57,008 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,008 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,008 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,008 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,008 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-50rayeom
2025-09-03 10:31:57,008 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,014 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:45789
2025-09-03 10:31:57,014 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:45789
2025-09-03 10:31:57,014 - distributed.worker - INFO -          dashboard at:          10.6.102.33:39421
2025-09-03 10:31:57,014 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,014 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,014 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,014 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,014 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-l5kru0xx
2025-09-03 10:31:57,014 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,020 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:36833
2025-09-03 10:31:57,020 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:36833
2025-09-03 10:31:57,020 - distributed.worker - INFO -          dashboard at:          10.6.102.33:45499
2025-09-03 10:31:57,020 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,020 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,020 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,020 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,020 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-2_rbnh50
2025-09-03 10:31:57,020 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,025 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:41613
2025-09-03 10:31:57,025 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:41613
2025-09-03 10:31:57,025 - distributed.worker - INFO -          dashboard at:          10.6.102.33:39773
2025-09-03 10:31:57,025 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,025 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,025 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,025 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,025 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-hxftfezg
2025-09-03 10:31:57,025 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,032 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:41665
2025-09-03 10:31:57,032 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:41665
2025-09-03 10:31:57,032 - distributed.worker - INFO -          dashboard at:          10.6.102.33:42151
2025-09-03 10:31:57,032 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,032 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,032 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,032 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,032 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-bxsxwq3t
2025-09-03 10:31:57,032 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,033 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:32993
2025-09-03 10:31:57,033 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:32993
2025-09-03 10:31:57,033 - distributed.worker - INFO -          dashboard at:          10.6.102.33:36379
2025-09-03 10:31:57,033 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,033 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,033 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,033 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,033 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-r5_8u10_
2025-09-03 10:31:57,033 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,041 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:33471
2025-09-03 10:31:57,041 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:33471
2025-09-03 10:31:57,041 - distributed.worker - INFO -          dashboard at:          10.6.102.33:38925
2025-09-03 10:31:57,041 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,041 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,041 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,041 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,041 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-el_60a6u
2025-09-03 10:31:57,041 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,044 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:44249
2025-09-03 10:31:57,044 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:44249
2025-09-03 10:31:57,044 - distributed.worker - INFO -          dashboard at:          10.6.102.33:41321
2025-09-03 10:31:57,044 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,044 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,044 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,044 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,044 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-vofz49an
2025-09-03 10:31:57,044 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,056 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:41541
2025-09-03 10:31:57,056 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:41541
2025-09-03 10:31:57,056 - distributed.worker - INFO -          dashboard at:          10.6.102.33:38871
2025-09-03 10:31:57,056 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,056 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,056 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,056 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,056 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-23ch4z66
2025-09-03 10:31:57,056 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,058 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:38777
2025-09-03 10:31:57,058 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:38777
2025-09-03 10:31:57,058 - distributed.worker - INFO -          dashboard at:          10.6.102.33:36059
2025-09-03 10:31:57,058 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,058 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,058 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,058 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,058 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-hwbf7m5n
2025-09-03 10:31:57,058 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,074 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:44231
2025-09-03 10:31:57,074 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:44231
2025-09-03 10:31:57,074 - distributed.worker - INFO -          dashboard at:          10.6.102.33:42857
2025-09-03 10:31:57,074 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,074 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,074 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,074 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,074 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ke9d0gx0
2025-09-03 10:31:57,074 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,084 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:36337
2025-09-03 10:31:57,084 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:36337
2025-09-03 10:31:57,084 - distributed.worker - INFO -          dashboard at:          10.6.102.33:35251
2025-09-03 10:31:57,084 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,084 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,084 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,084 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,084 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-j0wwe7t0
2025-09-03 10:31:57,084 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,109 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:57,110 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,110 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,112 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:57,123 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:57,125 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,125 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,126 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:57,134 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:40207
2025-09-03 10:31:57,134 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:40207
2025-09-03 10:31:57,134 - distributed.worker - INFO -          dashboard at:          10.6.102.33:42215
2025-09-03 10:31:57,134 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,134 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,134 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,134 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,134 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-vqz1r3ba
2025-09-03 10:31:57,134 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,141 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:35437
2025-09-03 10:31:57,141 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:35437
2025-09-03 10:31:57,141 - distributed.worker - INFO -          dashboard at:          10.6.102.33:34667
2025-09-03 10:31:57,141 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,141 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,141 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,141 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,141 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-vmdswn86
2025-09-03 10:31:57,142 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,145 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:46005
2025-09-03 10:31:57,145 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:46005
2025-09-03 10:31:57,145 - distributed.worker - INFO -          dashboard at:          10.6.102.33:35449
2025-09-03 10:31:57,145 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,145 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,145 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,145 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,145 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-hyrch05h
2025-09-03 10:31:57,145 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,152 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:57,153 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,153 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,155 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:57,185 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:34521
2025-09-03 10:31:57,185 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:34521
2025-09-03 10:31:57,185 - distributed.worker - INFO -          dashboard at:          10.6.102.33:44337
2025-09-03 10:31:57,185 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,185 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,185 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,185 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,185 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-72fheg5b
2025-09-03 10:31:57,185 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,222 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:41089
2025-09-03 10:31:57,222 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:41089
2025-09-03 10:31:57,222 - distributed.worker - INFO -          dashboard at:          10.6.102.33:44083
2025-09-03 10:31:57,222 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,222 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,222 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,222 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,222 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-4lgvpanh
2025-09-03 10:31:57,223 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,241 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:36929
2025-09-03 10:31:57,241 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:36929
2025-09-03 10:31:57,241 - distributed.worker - INFO -          dashboard at:          10.6.102.33:45657
2025-09-03 10:31:57,241 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,241 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,241 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,241 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,241 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-99elrdqa
2025-09-03 10:31:57,241 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,381 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:37479
2025-09-03 10:31:57,382 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:37479
2025-09-03 10:31:57,382 - distributed.worker - INFO -          dashboard at:          10.6.102.33:37857
2025-09-03 10:31:57,382 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,382 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,382 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,382 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,382 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-5zcv5240
2025-09-03 10:31:57,382 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,399 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:41843
2025-09-03 10:31:57,399 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:41843
2025-09-03 10:31:57,399 - distributed.worker - INFO -          dashboard at:          10.6.102.33:44511
2025-09-03 10:31:57,399 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,400 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,400 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,400 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,400 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-qy0thpin
2025-09-03 10:31:57,400 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,432 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:38579
2025-09-03 10:31:57,432 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:38579
2025-09-03 10:31:57,432 - distributed.worker - INFO -          dashboard at:          10.6.102.33:33111
2025-09-03 10:31:57,432 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,432 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,432 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,432 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,432 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-2seiyj79
2025-09-03 10:31:57,432 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,442 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:38141
2025-09-03 10:31:57,443 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:38141
2025-09-03 10:31:57,443 - distributed.worker - INFO -          dashboard at:          10.6.102.33:38603
2025-09-03 10:31:57,443 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,443 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,443 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:42719
2025-09-03 10:31:57,443 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,443 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,443 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:42719
2025-09-03 10:31:57,443 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-2v12pzvl
2025-09-03 10:31:57,443 - distributed.worker - INFO -          dashboard at:          10.6.102.33:39543
2025-09-03 10:31:57,443 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,443 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,443 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,443 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,443 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,443 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-uf0aiayx
2025-09-03 10:31:57,443 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,444 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:39073
2025-09-03 10:31:57,444 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:39073
2025-09-03 10:31:57,444 - distributed.worker - INFO -          dashboard at:          10.6.102.33:41063
2025-09-03 10:31:57,444 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,444 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,444 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,444 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,444 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-d_tzvqo7
2025-09-03 10:31:57,444 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,450 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:36521
2025-09-03 10:31:57,450 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:36521
2025-09-03 10:31:57,450 - distributed.worker - INFO -          dashboard at:          10.6.102.33:38353
2025-09-03 10:31:57,450 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,450 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,450 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,450 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,450 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-4_9f3n7q
2025-09-03 10:31:57,450 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,455 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:46661
2025-09-03 10:31:57,455 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:46661
2025-09-03 10:31:57,455 - distributed.worker - INFO -          dashboard at:          10.6.102.33:43129
2025-09-03 10:31:57,455 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,455 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,455 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,455 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,455 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-8vn234s8
2025-09-03 10:31:57,455 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,456 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:38461
2025-09-03 10:31:57,456 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:38461
2025-09-03 10:31:57,456 - distributed.worker - INFO -          dashboard at:          10.6.102.33:34643
2025-09-03 10:31:57,456 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,456 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,456 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,456 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,456 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-7s6jsb97
2025-09-03 10:31:57,457 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,482 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:36787
2025-09-03 10:31:57,483 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:36787
2025-09-03 10:31:57,483 - distributed.worker - INFO -          dashboard at:          10.6.102.33:46095
2025-09-03 10:31:57,483 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,483 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,483 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,483 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,483 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-_77hwlfr
2025-09-03 10:31:57,483 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,508 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:57,509 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:43479
2025-09-03 10:31:57,509 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:43479
2025-09-03 10:31:57,509 - distributed.worker - INFO -          dashboard at:          10.6.102.33:46007
2025-09-03 10:31:57,510 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,510 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,509 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,510 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,510 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,510 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-qxobogas
2025-09-03 10:31:57,510 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,510 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,511 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:57,518 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:44659
2025-09-03 10:31:57,518 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:44659
2025-09-03 10:31:57,518 - distributed.worker - INFO -          dashboard at:          10.6.102.33:37969
2025-09-03 10:31:57,518 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,518 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,518 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,518 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,518 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-5z51k9xi
2025-09-03 10:31:57,518 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,538 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:45939
2025-09-03 10:31:57,538 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:45939
2025-09-03 10:31:57,538 - distributed.worker - INFO -          dashboard at:          10.6.102.33:41887
2025-09-03 10:31:57,538 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,538 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,538 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,538 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,538 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-jbflughq
2025-09-03 10:31:57,538 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,597 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:36495
2025-09-03 10:31:57,597 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:36495
2025-09-03 10:31:57,597 - distributed.worker - INFO -          dashboard at:          10.6.102.33:40657
2025-09-03 10:31:57,597 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,597 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,597 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,597 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,597 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-hkvdipii
2025-09-03 10:31:57,597 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,621 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:38017
2025-09-03 10:31:57,622 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:38017
2025-09-03 10:31:57,622 - distributed.worker - INFO -          dashboard at:          10.6.102.33:46771
2025-09-03 10:31:57,622 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,622 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,622 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,622 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,622 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-l10solzh
2025-09-03 10:31:57,622 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,627 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:57,628 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,628 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,630 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:57,642 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:57,643 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,643 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,645 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:57,657 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:57,658 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,658 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,660 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:57,667 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:35367
2025-09-03 10:31:57,667 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:35367
2025-09-03 10:31:57,667 - distributed.worker - INFO -          dashboard at:          10.6.102.33:40089
2025-09-03 10:31:57,667 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,667 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,667 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,667 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,667 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-xvaikzi_
2025-09-03 10:31:57,667 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,679 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:34081
2025-09-03 10:31:57,680 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:34081
2025-09-03 10:31:57,680 - distributed.worker - INFO -          dashboard at:          10.6.102.33:32901
2025-09-03 10:31:57,680 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,680 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,680 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,680 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,680 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-2jz1uvhq
2025-09-03 10:31:57,680 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,714 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:57,715 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,715 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,717 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:57,721 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:46003
2025-09-03 10:31:57,721 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:46003
2025-09-03 10:31:57,721 - distributed.worker - INFO -          dashboard at:          10.6.102.33:34723
2025-09-03 10:31:57,721 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,721 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,721 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,721 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,721 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ox14hps6
2025-09-03 10:31:57,721 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,728 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:57,728 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,728 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,729 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:57,748 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:57,749 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,749 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,751 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:57,761 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:44393
2025-09-03 10:31:57,761 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:44393
2025-09-03 10:31:57,761 - distributed.worker - INFO -          dashboard at:          10.6.102.33:44549
2025-09-03 10:31:57,761 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,761 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,761 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,761 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,761 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-xb2cl4_s
2025-09-03 10:31:57,761 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,763 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:57,764 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,764 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,766 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:57,777 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:57,778 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,778 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,780 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:57,783 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:36847
2025-09-03 10:31:57,783 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:36847
2025-09-03 10:31:57,783 - distributed.worker - INFO -          dashboard at:          10.6.102.33:35857
2025-09-03 10:31:57,783 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,783 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,783 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,783 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,783 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-av9uhk16
2025-09-03 10:31:57,783 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,791 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:57,792 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,792 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,794 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:57,805 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:57,807 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,807 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,808 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:57,820 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:57,821 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,821 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,823 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:57,834 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:57,835 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,835 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,837 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:57,848 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:57,849 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,849 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,851 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:57,861 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:57,862 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,862 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,862 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:57,877 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:57,878 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,878 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,880 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:57,891 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:57,892 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,892 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,894 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:57,905 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:57,906 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,907 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,908 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:57,920 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:57,921 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,921 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,923 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:57,935 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:57,936 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,936 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,938 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:57,945 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:39239
2025-09-03 10:31:57,945 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:39239
2025-09-03 10:31:57,945 - distributed.worker - INFO -          dashboard at:          10.6.102.33:44949
2025-09-03 10:31:57,945 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,945 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,945 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,945 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,945 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-frun4w_3
2025-09-03 10:31:57,945 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,949 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:57,950 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,950 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,950 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:35041
2025-09-03 10:31:57,950 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:35041
2025-09-03 10:31:57,950 - distributed.worker - INFO -          dashboard at:          10.6.102.33:35705
2025-09-03 10:31:57,951 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,951 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,951 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,951 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,951 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-iikz_qvd
2025-09-03 10:31:57,951 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,951 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:40585
2025-09-03 10:31:57,951 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:40585
2025-09-03 10:31:57,951 - distributed.worker - INFO -          dashboard at:          10.6.102.33:33097
2025-09-03 10:31:57,951 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,951 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,951 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,951 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,951 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-sak9q9zn
2025-09-03 10:31:57,951 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,952 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:57,957 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:34277
2025-09-03 10:31:57,957 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:34277
2025-09-03 10:31:57,957 - distributed.worker - INFO -          dashboard at:          10.6.102.33:44079
2025-09-03 10:31:57,957 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,957 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,957 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,957 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,957 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ezy8b8xg
2025-09-03 10:31:57,957 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,961 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:45483
2025-09-03 10:31:57,961 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:45483
2025-09-03 10:31:57,961 - distributed.worker - INFO -          dashboard at:          10.6.102.33:42937
2025-09-03 10:31:57,961 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,961 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,961 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,961 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,961 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-hno31x5e
2025-09-03 10:31:57,961 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,984 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:44891
2025-09-03 10:31:57,984 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:44891
2025-09-03 10:31:57,984 - distributed.worker - INFO -          dashboard at:          10.6.102.33:38203
2025-09-03 10:31:57,984 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,984 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,984 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,984 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,984 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-15fo1t0i
2025-09-03 10:31:57,984 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,006 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,008 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,008 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,009 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,013 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:44747
2025-09-03 10:31:58,013 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:44747
2025-09-03 10:31:58,013 - distributed.worker - INFO -          dashboard at:          10.6.102.33:42583
2025-09-03 10:31:58,013 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,013 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,013 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,013 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,013 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-atap1ie6
2025-09-03 10:31:58,013 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,021 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,022 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,022 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,024 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,027 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:33553
2025-09-03 10:31:58,027 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:33553
2025-09-03 10:31:58,027 - distributed.worker - INFO -          dashboard at:          10.6.102.33:36859
2025-09-03 10:31:58,027 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,027 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,027 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,027 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,027 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-f0776yhf
2025-09-03 10:31:58,027 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,035 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,036 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,036 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,038 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,073 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:45185
2025-09-03 10:31:58,073 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:45185
2025-09-03 10:31:58,073 - distributed.worker - INFO -          dashboard at:          10.6.102.33:35681
2025-09-03 10:31:58,073 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,073 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,073 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,073 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,073 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-wxq3mqqs
2025-09-03 10:31:58,073 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,077 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:40921
2025-09-03 10:31:58,077 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:40921
2025-09-03 10:31:58,077 - distributed.worker - INFO -          dashboard at:          10.6.102.33:41419
2025-09-03 10:31:58,077 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,077 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,077 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,077 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,077 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-lnfitlfp
2025-09-03 10:31:58,077 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,078 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,079 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,079 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,080 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:41977
2025-09-03 10:31:58,080 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:41977
2025-09-03 10:31:58,080 - distributed.worker - INFO -          dashboard at:          10.6.102.33:32885
2025-09-03 10:31:58,080 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,080 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,080 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,080 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,080 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-osfv_oq3
2025-09-03 10:31:58,080 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,080 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,087 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:44981
2025-09-03 10:31:58,087 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:44981
2025-09-03 10:31:58,087 - distributed.worker - INFO -          dashboard at:          10.6.102.33:46399
2025-09-03 10:31:58,087 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,087 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,087 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,088 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,088 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-4ysr8yzj
2025-09-03 10:31:58,088 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,092 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:32873
2025-09-03 10:31:58,092 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:32873
2025-09-03 10:31:58,092 - distributed.worker - INFO -          dashboard at:          10.6.102.33:46667
2025-09-03 10:31:58,092 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,092 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,092 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,092 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,092 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-xd_zon0t
2025-09-03 10:31:58,092 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,095 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:43795
2025-09-03 10:31:58,095 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:43795
2025-09-03 10:31:58,095 - distributed.worker - INFO -          dashboard at:          10.6.102.33:37565
2025-09-03 10:31:58,095 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,095 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,095 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,095 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,095 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-adfak0eu
2025-09-03 10:31:58,095 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,097 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:35537
2025-09-03 10:31:58,097 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:35537
2025-09-03 10:31:58,097 - distributed.worker - INFO -          dashboard at:          10.6.102.33:36565
2025-09-03 10:31:58,097 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,097 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,097 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,097 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,097 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-v3amwxh3
2025-09-03 10:31:58,097 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,098 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:35263
2025-09-03 10:31:58,098 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:35263
2025-09-03 10:31:58,098 - distributed.worker - INFO -          dashboard at:          10.6.102.33:41725
2025-09-03 10:31:58,098 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,098 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,098 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,098 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,098 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-bcyv3pe4
2025-09-03 10:31:58,098 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,101 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:32949
2025-09-03 10:31:58,101 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:32949
2025-09-03 10:31:58,101 - distributed.worker - INFO -          dashboard at:          10.6.102.33:38919
2025-09-03 10:31:58,101 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:42053
2025-09-03 10:31:58,101 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,101 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,101 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:42053
2025-09-03 10:31:58,101 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,101 - distributed.worker - INFO -          dashboard at:          10.6.102.33:36215
2025-09-03 10:31:58,101 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,101 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,101 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-0jyd9t8u
2025-09-03 10:31:58,101 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,101 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,101 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,101 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,101 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-982fsla5
2025-09-03 10:31:58,101 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,102 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:41299
2025-09-03 10:31:58,102 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:41299
2025-09-03 10:31:58,102 - distributed.worker - INFO -          dashboard at:          10.6.102.33:41585
2025-09-03 10:31:58,102 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,102 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,102 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,102 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,102 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-bhfmp5ea
2025-09-03 10:31:58,102 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,105 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:33205
2025-09-03 10:31:58,105 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:33205
2025-09-03 10:31:58,105 - distributed.worker - INFO -          dashboard at:          10.6.102.33:42163
2025-09-03 10:31:58,105 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,105 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,105 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,105 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,105 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-smruu83h
2025-09-03 10:31:58,105 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,107 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,108 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,108 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,109 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:41903
2025-09-03 10:31:58,109 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:41903
2025-09-03 10:31:58,109 - distributed.worker - INFO -          dashboard at:          10.6.102.33:38731
2025-09-03 10:31:58,109 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,109 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,109 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,109 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,109 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-zjsv8l5v
2025-09-03 10:31:58,109 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,109 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,111 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:34795
2025-09-03 10:31:58,111 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:34795
2025-09-03 10:31:58,111 - distributed.worker - INFO -          dashboard at:          10.6.102.33:41073
2025-09-03 10:31:58,111 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,111 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,111 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,111 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,111 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-q6mhhczn
2025-09-03 10:31:58,111 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,111 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:33309
2025-09-03 10:31:58,111 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:33309
2025-09-03 10:31:58,111 - distributed.worker - INFO -          dashboard at:          10.6.102.33:35149
2025-09-03 10:31:58,111 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:35461
2025-09-03 10:31:58,112 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,112 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,112 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:35461
2025-09-03 10:31:58,112 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,112 - distributed.worker - INFO -          dashboard at:          10.6.102.33:35585
2025-09-03 10:31:58,112 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,112 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,112 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-d7hzd0x_
2025-09-03 10:31:58,112 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,112 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,112 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,112 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,112 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-avfvpbm0
2025-09-03 10:31:58,112 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,113 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:43919
2025-09-03 10:31:58,113 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:43919
2025-09-03 10:31:58,113 - distributed.worker - INFO -          dashboard at:          10.6.102.33:35127
2025-09-03 10:31:58,113 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,113 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,113 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,113 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,113 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-82chgcjh
2025-09-03 10:31:58,113 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,115 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:44821
2025-09-03 10:31:58,115 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:44821
2025-09-03 10:31:58,115 - distributed.worker - INFO -          dashboard at:          10.6.102.33:40231
2025-09-03 10:31:58,115 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,115 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,115 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,115 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,115 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-tduyvl0z
2025-09-03 10:31:58,115 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,117 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:40489
2025-09-03 10:31:58,117 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:40489
2025-09-03 10:31:58,117 - distributed.worker - INFO -          dashboard at:          10.6.102.33:37375
2025-09-03 10:31:58,117 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,117 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,117 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,117 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,117 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-0lpul0wr
2025-09-03 10:31:58,117 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,123 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:33741
2025-09-03 10:31:58,123 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:33741
2025-09-03 10:31:58,123 - distributed.worker - INFO -          dashboard at:          10.6.102.33:39345
2025-09-03 10:31:58,123 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,123 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,123 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,123 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,123 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-_azgflcv
2025-09-03 10:31:58,123 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,132 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.33:39137
2025-09-03 10:31:58,132 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.33:39137
2025-09-03 10:31:58,132 - distributed.worker - INFO -          dashboard at:          10.6.102.33:38715
2025-09-03 10:31:58,132 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,132 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,132 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,132 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,132 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-lk1zjfv8
2025-09-03 10:31:58,132 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,135 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,137 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,137 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,139 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,150 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,151 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,151 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,152 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,165 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,167 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,167 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,171 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,206 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,208 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,208 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,209 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,280 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,281 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,281 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,282 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,294 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,295 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,296 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,298 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,338 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,340 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,340 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,341 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,469 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,471 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,471 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,475 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,511 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,512 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,513 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,514 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,525 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,527 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,529 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,539 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,541 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,541 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,543 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,554 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,555 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,556 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,557 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,873 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,874 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,874 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,876 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,975 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,976 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,977 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,979 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:00,865 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:00,867 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,867 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,869 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:00,879 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:00,881 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,881 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,883 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:00,896 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:00,898 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,898 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,903 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:00,909 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:00,910 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,910 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,912 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:00,924 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:00,925 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,926 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,928 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,725 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,726 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,727 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,729 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,740 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,741 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,742 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,744 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,755 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,756 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,756 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,758 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,591 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,592 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,593 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,595 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,606 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,607 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,607 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,609 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,621 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,622 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,622 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,624 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,636 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,637 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,637 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,639 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,650 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,652 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,652 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,654 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,665 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,667 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,667 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,669 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:03,846 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:03,847 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:03,847 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:03,849 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,197 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,199 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,199 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,201 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,504 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,505 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,505 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,507 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,550 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,552 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,552 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,554 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,566 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,567 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,567 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,570 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,683 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,685 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,685 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,687 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,699 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,701 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,701 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,703 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,730 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,732 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,732 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,734 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,745 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,747 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,747 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,748 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,776 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,777 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,777 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,779 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,839 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,841 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,841 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,843 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:05,212 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:05,214 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:05,214 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:05,216 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:26,990 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,009 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,009 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,015 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,021 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,026 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,033 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,035 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,043 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,046 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,057 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,060 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,075 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,085 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,186 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,224 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,242 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:28,089 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:28,093 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:28,097 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:28,099 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:28,099 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:28,103 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:28,103 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:28,104 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:28,107 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:28,110 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:28,112 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:28,113 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:28,113 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:28,114 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:28,117 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:28,119 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:28,124 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:28,115 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:28,134 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:28,128 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:28,157 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:29,211 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:29,286 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:29,301 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:29,345 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:29,455 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:29,457 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,457 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:29,459 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:29,780 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:29,781 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,781 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:29,783 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:32,238 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:32,240 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:32,240 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:32,241 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:32,255 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:32,256 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:32,256 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:32,258 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:32,272 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:32,274 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:32,274 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:32,276 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:32,289 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:32,291 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:32,291 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:32,292 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:32,306 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:32,308 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:32,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:32,310 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:32,323 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:32,325 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:32,325 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:32,327 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:32,340 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:32,342 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:32,342 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:32,343 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:32,357 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:32,359 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:32,359 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:32,361 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:32,374 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:32,376 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:32,376 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:32,378 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:32,392 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:32,393 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:32,394 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:32,395 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:32,408 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:32,410 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:32,410 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:32,412 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:32,425 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:32,427 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:32,427 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:32,429 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:32,443 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:32,444 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:32,445 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:32,446 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:32,459 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:32,461 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:32,461 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:32,463 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:32,477 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:32,478 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:32,479 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:32,480 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:32,494 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:32,495 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:32,496 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:32,497 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:32,511 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:32,513 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:32,513 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:32,515 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:32,528 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:32,530 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:32,530 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:32,532 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:35,689 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:35,707 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:35,737 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:35,751 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:51,475 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,480 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,480 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,486 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,494 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,499 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,499 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,505 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,512 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,517 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,517 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,523 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,530 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,536 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,537 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,543 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,545 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,550 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,550 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,555 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,560 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,563 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,563 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,568 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,579 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,583 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,583 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,590 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,596 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,601 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,601 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,606 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,613 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,617 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,618 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,624 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,628 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,631 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,631 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,636 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,647 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,651 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,652 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,657 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,663 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,667 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,667 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,672 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,682 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,686 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,686 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,691 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,696 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,697 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,697 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,699 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,718 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,722 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,722 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,727 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:59,114 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:59,128 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:59,156 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:35:51,382 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,389 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,592 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,598 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,664 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,668 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,878 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,885 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,886 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,888 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,950 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,954 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,955 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,960 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,967 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,971 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,074 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,076 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,076 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,077 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,086 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,091 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,093 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,095 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,143 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,147 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,232 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,233 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,323 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,327 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,379 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,380 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,379 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,381 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,385 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,387 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,411 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,416 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,430 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,435 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,436 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,442 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,452 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,456 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,558 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,559 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,986 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,987 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,034 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,035 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,238 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,242 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,243 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,246 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,286 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,291 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,300 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,301 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,365 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,371 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,787 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,788 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,841 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,844 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,925 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,930 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,988 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,995 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,146 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,149 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,171 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,176 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,341 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,346 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,402 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,407 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,461 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,464 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,727 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,732 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,740 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,741 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,745 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,750 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,756 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,760 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,812 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,814 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,955 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,960 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,968 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,971 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,112 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,113 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,126 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,129 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,151 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,154 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,165 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,167 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,368 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,373 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,616 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,617 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,618 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,620 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,638 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,640 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,646 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,646 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,778 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,779 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,801 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,805 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,830 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,834 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,875 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,881 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,988 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,990 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,999 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,001 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,137 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,142 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,157 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,162 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,288 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,291 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,301 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,302 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,408 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,409 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,420 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,425 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,461 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,463 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,636 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,639 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,666 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,667 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,783 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,784 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,790 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,793 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,913 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,919 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,962 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,966 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,074 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,076 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,113 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,114 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,161 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,162 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,225 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,226 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,227 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,228 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,331 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,332 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,355 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,356 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,370 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,371 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,625 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,627 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,711 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,716 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,072 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,073 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,264 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,267 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,021 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,026 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,166 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,169 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,266 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,267 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,388 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,389 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,777 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,780 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,232 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,233 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,643 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,648 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,056 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,062 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,803 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,808 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,810 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,816 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:03,646 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:03,651 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:04,034 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:04,035 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:04,389 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:04,395 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:04,866 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:04,871 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:05,508 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:05,515 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:05,964 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:05,967 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:13,406 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:13,408 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:17,114 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:17,121 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:22,889 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,891 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,892 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,893 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,894 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,895 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,897 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,898 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,898 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,900 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,900 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,901 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,903 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,907 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,910 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,912 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,912 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,914 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,914 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,916 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,917 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,919 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,925 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,927 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,927 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,929 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,930 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,932 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,948 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,950 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,952 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,956 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,957 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,958 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,386 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,387 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,388 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,389 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,390 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,392 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,399 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,401 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,403 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,405 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,517 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,519 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,522 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,524 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,526 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,528 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,535 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,536 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,537 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,538 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,550 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,551 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,552 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,554 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,787 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,789 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,801 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,804 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,806 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,806 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,834 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,836 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,838 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,838 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,839 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,840 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,840 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,841 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,852 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,854 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,863 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,865 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,878 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,880 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,885 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,886 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,887 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,888 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,887 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,892 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,892 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,894 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,021 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,023 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,030 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,033 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,064 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,066 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,065 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,066 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,066 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,067 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,068 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,068 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,069 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,070 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,071 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,075 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,076 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,079 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,090 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,092 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,124 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,126 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,242 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,244 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,334 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,336 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,388 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,391 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,400 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,403 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,409 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,414 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,429 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,434 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,450 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,455 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,458 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,460 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,460 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,460 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,462 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,463 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,586 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,588 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,624 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,625 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,626 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,627 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,627 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,629 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,632 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,634 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,642 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,644 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,663 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,665 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,676 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,678 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,685 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,690 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,693 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,695 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,740 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,742 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,750 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,752 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,757 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,760 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,790 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,789 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,789 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,791 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,792 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,793 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,794 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,795 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,855 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,857 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,858 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,860 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,864 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,866 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,891 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,893 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,922 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,924 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,946 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,948 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,991 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,992 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,993 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,994 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,019 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,021 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,061 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,063 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,085 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,088 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,143 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,148 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,444 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,446 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,708 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,710 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,753 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,755 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,793 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,796 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,834 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,836 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,928 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,930 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,022 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,024 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,030 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,038 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,041 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,043 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,387 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,389 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,417 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,419 - distributed.utils - INFO - Reload module qme_vars from .py file
