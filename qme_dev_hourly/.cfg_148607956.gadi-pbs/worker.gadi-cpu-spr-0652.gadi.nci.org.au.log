Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-03 10:52:37,654 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:35693'
2025-09-03 10:52:37,663 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:46651'
2025-09-03 10:52:37,669 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:41641'
2025-09-03 10:52:37,674 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:42451'
2025-09-03 10:52:37,679 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:33619'
2025-09-03 10:52:37,805 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:33989'
2025-09-03 10:52:37,810 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:35149'
2025-09-03 10:52:37,813 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:37451'
2025-09-03 10:52:37,818 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:33247'
2025-09-03 10:52:37,823 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:36085'
2025-09-03 10:52:37,827 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:46175'
2025-09-03 10:52:37,831 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:41885'
2025-09-03 10:52:37,835 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:45589'
2025-09-03 10:52:37,839 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:35717'
2025-09-03 10:52:37,844 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:36789'
2025-09-03 10:52:37,848 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:42589'
2025-09-03 10:52:37,853 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:39129'
2025-09-03 10:52:37,859 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:43195'
2025-09-03 10:52:37,864 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:34699'
2025-09-03 10:52:37,868 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:37335'
2025-09-03 10:52:37,872 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:36001'
2025-09-03 10:52:37,876 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:44093'
2025-09-03 10:52:37,880 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:39385'
2025-09-03 10:52:37,882 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:36841'
2025-09-03 10:52:37,887 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:41325'
2025-09-03 10:52:37,890 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:38109'
2025-09-03 10:52:37,893 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:37807'
2025-09-03 10:52:37,898 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:44769'
2025-09-03 10:52:37,903 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:36731'
2025-09-03 10:52:37,907 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:37759'
2025-09-03 10:52:37,911 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:35505'
2025-09-03 10:52:37,916 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:40709'
2025-09-03 10:52:37,921 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:35355'
2025-09-03 10:52:37,926 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:40539'
2025-09-03 10:52:37,931 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:45855'
2025-09-03 10:52:37,935 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:45117'
2025-09-03 10:52:37,939 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:41923'
2025-09-03 10:52:37,944 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:46109'
2025-09-03 10:52:37,947 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:39753'
2025-09-03 10:52:37,952 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:37521'
2025-09-03 10:52:37,958 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:36505'
2025-09-03 10:52:38,086 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:43549'
2025-09-03 10:52:38,090 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:43857'
2025-09-03 10:52:38,095 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:45149'
2025-09-03 10:52:38,100 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:37371'
2025-09-03 10:52:38,104 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:44195'
2025-09-03 10:52:38,108 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:36637'
2025-09-03 10:52:38,119 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:44493'
2025-09-03 10:52:38,124 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:45527'
2025-09-03 10:52:38,128 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:33753'
2025-09-03 10:52:38,133 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:41715'
2025-09-03 10:52:38,137 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:43927'
2025-09-03 10:52:38,142 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:43149'
2025-09-03 10:52:38,149 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:38873'
2025-09-03 10:52:38,153 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:41561'
2025-09-03 10:52:38,158 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:33569'
2025-09-03 10:52:38,162 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:43839'
2025-09-03 10:52:38,165 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:42493'
2025-09-03 10:52:38,170 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:42667'
2025-09-03 10:52:38,176 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:38753'
2025-09-03 10:52:38,179 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:34693'
2025-09-03 10:52:38,183 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:41019'
2025-09-03 10:52:38,187 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:42895'
2025-09-03 10:52:38,191 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:39513'
2025-09-03 10:52:38,196 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:37647'
2025-09-03 10:52:38,202 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:37379'
2025-09-03 10:52:38,208 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:42187'
2025-09-03 10:52:38,213 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:42477'
2025-09-03 10:52:38,216 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:45831'
2025-09-03 10:52:38,219 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:39935'
2025-09-03 10:52:38,224 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:45717'
2025-09-03 10:52:38,228 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:39909'
2025-09-03 10:52:38,235 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:37263'
2025-09-03 10:52:38,250 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:46455'
2025-09-03 10:52:38,581 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:38711'
2025-09-03 10:52:38,586 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:39925'
2025-09-03 10:52:38,591 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:43307'
2025-09-03 10:52:38,596 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:35633'
2025-09-03 10:52:38,600 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:33793'
2025-09-03 10:52:38,605 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:46467'
2025-09-03 10:52:38,610 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:37889'
2025-09-03 10:52:38,614 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:38887'
2025-09-03 10:52:38,619 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:41085'
2025-09-03 10:52:38,623 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:33485'
2025-09-03 10:52:38,642 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:34019'
2025-09-03 10:52:38,647 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:43833'
2025-09-03 10:52:38,653 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:39251'
2025-09-03 10:52:38,658 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:38877'
2025-09-03 10:52:38,663 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:43845'
2025-09-03 10:52:38,667 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:38693'
2025-09-03 10:52:38,672 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:33501'
2025-09-03 10:52:38,676 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:37933'
2025-09-03 10:52:38,681 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:41167'
2025-09-03 10:52:38,686 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:39681'
2025-09-03 10:52:38,691 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:46875'
2025-09-03 10:52:38,698 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:44059'
2025-09-03 10:52:38,703 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:33157'
2025-09-03 10:52:38,706 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:45237'
2025-09-03 10:52:38,709 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:44843'
2025-09-03 10:52:39,241 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:33113
2025-09-03 10:52:39,241 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:33131
2025-09-03 10:52:39,241 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:33113
2025-09-03 10:52:39,241 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:41445
2025-09-03 10:52:39,241 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:33131
2025-09-03 10:52:39,241 - distributed.worker - INFO -          dashboard at:           10.6.105.4:35883
2025-09-03 10:52:39,241 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:41445
2025-09-03 10:52:39,241 - distributed.worker - INFO -          dashboard at:           10.6.105.4:46011
2025-09-03 10:52:39,241 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,241 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:39233
2025-09-03 10:52:39,241 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:34583
2025-09-03 10:52:39,241 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:42745
2025-09-03 10:52:39,241 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,241 - distributed.worker - INFO -          dashboard at:           10.6.105.4:36977
2025-09-03 10:52:39,241 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,241 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:39233
2025-09-03 10:52:39,241 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:34583
2025-09-03 10:52:39,241 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,241 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:42745
2025-09-03 10:52:39,241 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,241 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,241 - distributed.worker - INFO -          dashboard at:           10.6.105.4:43485
2025-09-03 10:52:39,241 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,241 - distributed.worker - INFO -          dashboard at:           10.6.105.4:45313
2025-09-03 10:52:39,241 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,241 - distributed.worker - INFO -          dashboard at:           10.6.105.4:46417
2025-09-03 10:52:39,241 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,241 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,241 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-56qox4h2
2025-09-03 10:52:39,241 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,241 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,241 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,241 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,241 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,241 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,241 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,241 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,241 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,241 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-xnsizkhk
2025-09-03 10:52:39,241 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,241 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,241 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,241 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-okpyp0_i
2025-09-03 10:52:39,241 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,241 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,241 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,241 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,241 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-gk8xf83q
2025-09-03 10:52:39,241 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,241 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ny1bq4nr
2025-09-03 10:52:39,241 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-_3xg2a1k
2025-09-03 10:52:39,241 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,241 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,241 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,695 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:33827
2025-09-03 10:52:39,696 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:33827
2025-09-03 10:52:39,696 - distributed.worker - INFO -          dashboard at:           10.6.105.4:38621
2025-09-03 10:52:39,696 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,696 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,696 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,696 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,696 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-w30ksm3t
2025-09-03 10:52:39,696 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,736 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:39,736 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:46269
2025-09-03 10:52:39,736 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:46269
2025-09-03 10:52:39,736 - distributed.worker - INFO -          dashboard at:           10.6.105.4:44559
2025-09-03 10:52:39,736 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,736 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,736 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,736 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,736 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-gbywc8aw
2025-09-03 10:52:39,736 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,737 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,737 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,739 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:39,749 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:39,750 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,751 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:35493
2025-09-03 10:52:39,751 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,751 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:35493
2025-09-03 10:52:39,751 - distributed.worker - INFO -          dashboard at:           10.6.105.4:40497
2025-09-03 10:52:39,751 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,751 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,751 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,751 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,751 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-vdh05589
2025-09-03 10:52:39,751 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,752 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:39,756 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:41251
2025-09-03 10:52:39,756 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:41251
2025-09-03 10:52:39,756 - distributed.worker - INFO -          dashboard at:           10.6.105.4:46877
2025-09-03 10:52:39,756 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,756 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,756 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,756 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,756 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-q_pokl7a
2025-09-03 10:52:39,756 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,757 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:36801
2025-09-03 10:52:39,757 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:36801
2025-09-03 10:52:39,757 - distributed.worker - INFO -          dashboard at:           10.6.105.4:32917
2025-09-03 10:52:39,757 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,757 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,757 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,757 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,757 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-5ounpi4r
2025-09-03 10:52:39,757 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,764 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:39,765 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,765 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,767 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:39,778 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:39,779 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:46165
2025-09-03 10:52:39,779 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:46165
2025-09-03 10:52:39,779 - distributed.worker - INFO -          dashboard at:           10.6.105.4:38025
2025-09-03 10:52:39,779 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,779 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,779 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,779 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,779 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-i6vtk_xo
2025-09-03 10:52:39,779 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,779 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,780 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,781 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:39,803 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:45439
2025-09-03 10:52:39,804 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:45439
2025-09-03 10:52:39,804 - distributed.worker - INFO -          dashboard at:           10.6.105.4:43243
2025-09-03 10:52:39,804 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,804 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,804 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,804 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,804 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-qip0b0w5
2025-09-03 10:52:39,804 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,832 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:43047
2025-09-03 10:52:39,833 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:43047
2025-09-03 10:52:39,833 - distributed.worker - INFO -          dashboard at:           10.6.105.4:43051
2025-09-03 10:52:39,833 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,833 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,833 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,833 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,833 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ht8ai77m
2025-09-03 10:52:39,833 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,840 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:39287
2025-09-03 10:52:39,841 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:39287
2025-09-03 10:52:39,841 - distributed.worker - INFO -          dashboard at:           10.6.105.4:38053
2025-09-03 10:52:39,841 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,841 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,841 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,841 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,841 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-6x7oeyvm
2025-09-03 10:52:39,841 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,849 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:45477
2025-09-03 10:52:39,849 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:45477
2025-09-03 10:52:39,849 - distributed.worker - INFO -          dashboard at:           10.6.105.4:42947
2025-09-03 10:52:39,849 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,849 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,849 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,849 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,849 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-rj33zo7e
2025-09-03 10:52:39,849 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,853 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:37833
2025-09-03 10:52:39,853 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:37833
2025-09-03 10:52:39,853 - distributed.worker - INFO -          dashboard at:           10.6.105.4:37179
2025-09-03 10:52:39,853 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,853 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,853 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,853 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,854 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-qd1wmo3o
2025-09-03 10:52:39,854 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,877 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:46463
2025-09-03 10:52:39,877 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:46463
2025-09-03 10:52:39,877 - distributed.worker - INFO -          dashboard at:           10.6.105.4:38389
2025-09-03 10:52:39,877 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,877 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,877 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,877 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,877 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-1q5wsfc0
2025-09-03 10:52:39,877 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,886 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:33315
2025-09-03 10:52:39,886 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:33315
2025-09-03 10:52:39,886 - distributed.worker - INFO -          dashboard at:           10.6.105.4:41717
2025-09-03 10:52:39,886 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,886 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,886 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,886 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,886 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-wkzmk8rw
2025-09-03 10:52:39,886 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,889 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:36043
2025-09-03 10:52:39,889 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:36043
2025-09-03 10:52:39,889 - distributed.worker - INFO -          dashboard at:           10.6.105.4:39981
2025-09-03 10:52:39,889 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,889 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,889 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,889 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,889 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-v6x_jt7f
2025-09-03 10:52:39,889 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,894 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:35959
2025-09-03 10:52:39,894 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:35959
2025-09-03 10:52:39,894 - distributed.worker - INFO -          dashboard at:           10.6.105.4:35439
2025-09-03 10:52:39,894 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,894 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,894 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,894 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,894 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-q9q0nh43
2025-09-03 10:52:39,894 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,897 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:35919
2025-09-03 10:52:39,897 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:35919
2025-09-03 10:52:39,897 - distributed.worker - INFO -          dashboard at:           10.6.105.4:44283
2025-09-03 10:52:39,897 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,897 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,897 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,897 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,897 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-15i_3r17
2025-09-03 10:52:39,897 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,900 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:43119
2025-09-03 10:52:39,900 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:43119
2025-09-03 10:52:39,900 - distributed.worker - INFO -          dashboard at:           10.6.105.4:41905
2025-09-03 10:52:39,900 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,900 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,900 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,900 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,900 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-3q0p8wai
2025-09-03 10:52:39,900 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,902 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:42751
2025-09-03 10:52:39,902 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:42751
2025-09-03 10:52:39,902 - distributed.worker - INFO -          dashboard at:           10.6.105.4:43059
2025-09-03 10:52:39,902 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,902 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,902 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,902 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,902 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-sfbxg3h5
2025-09-03 10:52:39,902 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,905 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:43539
2025-09-03 10:52:39,905 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:43539
2025-09-03 10:52:39,905 - distributed.worker - INFO -          dashboard at:           10.6.105.4:43265
2025-09-03 10:52:39,905 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,905 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,905 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,905 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,905 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-qiukm2rh
2025-09-03 10:52:39,905 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,908 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:44569
2025-09-03 10:52:39,908 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:44569
2025-09-03 10:52:39,908 - distributed.worker - INFO -          dashboard at:           10.6.105.4:37277
2025-09-03 10:52:39,908 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,908 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,908 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,908 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,908 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-8m0hs9ov
2025-09-03 10:52:39,908 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,912 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:37787
2025-09-03 10:52:39,912 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:37787
2025-09-03 10:52:39,912 - distributed.worker - INFO -          dashboard at:           10.6.105.4:38733
2025-09-03 10:52:39,912 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,912 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,912 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,912 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,912 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-8wasigkc
2025-09-03 10:52:39,912 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,917 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:37197
2025-09-03 10:52:39,917 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:37197
2025-09-03 10:52:39,917 - distributed.worker - INFO -          dashboard at:           10.6.105.4:45505
2025-09-03 10:52:39,917 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,917 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,917 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,917 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,917 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-5qpe52z4
2025-09-03 10:52:39,917 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,919 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:35131
2025-09-03 10:52:39,919 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:35131
2025-09-03 10:52:39,919 - distributed.worker - INFO -          dashboard at:           10.6.105.4:39333
2025-09-03 10:52:39,919 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,919 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,919 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,919 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,919 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-b7j8o2aw
2025-09-03 10:52:39,919 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,920 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:42693
2025-09-03 10:52:39,921 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:42693
2025-09-03 10:52:39,921 - distributed.worker - INFO -          dashboard at:           10.6.105.4:33791
2025-09-03 10:52:39,921 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,921 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,921 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,921 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,921 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-82wpdn7t
2025-09-03 10:52:39,921 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,922 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:38247
2025-09-03 10:52:39,922 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:38247
2025-09-03 10:52:39,922 - distributed.worker - INFO -          dashboard at:           10.6.105.4:34659
2025-09-03 10:52:39,922 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,922 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,922 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,922 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,922 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-i2lv66fi
2025-09-03 10:52:39,922 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,923 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:37471
2025-09-03 10:52:39,923 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:37471
2025-09-03 10:52:39,923 - distributed.worker - INFO -          dashboard at:           10.6.105.4:38571
2025-09-03 10:52:39,923 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,923 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,923 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,923 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,923 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-7erwlmv9
2025-09-03 10:52:39,923 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,924 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:37013
2025-09-03 10:52:39,924 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:37013
2025-09-03 10:52:39,924 - distributed.worker - INFO -          dashboard at:           10.6.105.4:38219
2025-09-03 10:52:39,924 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,924 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,924 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,924 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,924 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-rlzysfs1
2025-09-03 10:52:39,925 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,926 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:36989
2025-09-03 10:52:39,926 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:36989
2025-09-03 10:52:39,927 - distributed.worker - INFO -          dashboard at:           10.6.105.4:37903
2025-09-03 10:52:39,927 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,927 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,927 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,927 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,927 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-9offubly
2025-09-03 10:52:39,927 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,931 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:46013
2025-09-03 10:52:39,931 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:46013
2025-09-03 10:52:39,931 - distributed.worker - INFO -          dashboard at:           10.6.105.4:34835
2025-09-03 10:52:39,931 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,931 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,931 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,931 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,931 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ay70ry3v
2025-09-03 10:52:39,931 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,932 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:39,933 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,933 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,934 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:43363
2025-09-03 10:52:39,934 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:43363
2025-09-03 10:52:39,934 - distributed.worker - INFO -          dashboard at:           10.6.105.4:37855
2025-09-03 10:52:39,934 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,934 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,934 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,934 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,934 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-l2045eog
2025-09-03 10:52:39,934 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,935 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:39,935 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:45255
2025-09-03 10:52:39,935 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:45255
2025-09-03 10:52:39,935 - distributed.worker - INFO -          dashboard at:           10.6.105.4:39537
2025-09-03 10:52:39,935 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,935 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,935 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:39,935 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:39,936 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-d55h2mbf
2025-09-03 10:52:39,936 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,961 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:39,962 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:39,962 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:39,964 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:40,125 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:35567
2025-09-03 10:52:40,125 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:35567
2025-09-03 10:52:40,125 - distributed.worker - INFO -          dashboard at:           10.6.105.4:42169
2025-09-03 10:52:40,125 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,125 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,125 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:40,125 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:40,125 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-yf033jns
2025-09-03 10:52:40,125 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,168 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:43427
2025-09-03 10:52:40,168 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:43427
2025-09-03 10:52:40,168 - distributed.worker - INFO -          dashboard at:           10.6.105.4:39671
2025-09-03 10:52:40,168 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,169 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,169 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:40,169 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:40,169 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-yudjuan6
2025-09-03 10:52:40,169 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,409 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:40607
2025-09-03 10:52:40,409 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:40607
2025-09-03 10:52:40,409 - distributed.worker - INFO -          dashboard at:           10.6.105.4:45867
2025-09-03 10:52:40,409 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,409 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,409 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:40,409 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:40,410 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-a8s4eq_a
2025-09-03 10:52:40,410 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,451 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:37781
2025-09-03 10:52:40,451 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:37781
2025-09-03 10:52:40,451 - distributed.worker - INFO -          dashboard at:           10.6.105.4:45535
2025-09-03 10:52:40,451 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,451 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,451 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:40,451 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:40,451 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ea3iyjpn
2025-09-03 10:52:40,451 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,517 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:40,517 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:44117
2025-09-03 10:52:40,517 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:44117
2025-09-03 10:52:40,517 - distributed.worker - INFO -          dashboard at:           10.6.105.4:37161
2025-09-03 10:52:40,518 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,518 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,518 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:40,518 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:40,518 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-n3isqfen
2025-09-03 10:52:40,518 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,518 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,518 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,520 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:40,523 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:36373
2025-09-03 10:52:40,523 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:36373
2025-09-03 10:52:40,524 - distributed.worker - INFO -          dashboard at:           10.6.105.4:44119
2025-09-03 10:52:40,524 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,524 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:40,524 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:40,524 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-2vilaju6
2025-09-03 10:52:40,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,534 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:43151
2025-09-03 10:52:40,534 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:43151
2025-09-03 10:52:40,534 - distributed.worker - INFO -          dashboard at:           10.6.105.4:42873
2025-09-03 10:52:40,534 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,535 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,535 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:40,535 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:40,535 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-vb6blf53
2025-09-03 10:52:40,535 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,540 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:43723
2025-09-03 10:52:40,540 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:43723
2025-09-03 10:52:40,540 - distributed.worker - INFO -          dashboard at:           10.6.105.4:35565
2025-09-03 10:52:40,540 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,541 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,541 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:40,541 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:40,541 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-jqhwooun
2025-09-03 10:52:40,541 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,545 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:40,546 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,546 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,548 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:40,560 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:40,561 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,561 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,563 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:40,569 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:43617
2025-09-03 10:52:40,569 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:43617
2025-09-03 10:52:40,569 - distributed.worker - INFO -          dashboard at:           10.6.105.4:43895
2025-09-03 10:52:40,569 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,569 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,569 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:40,569 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:40,569 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-d_2bm664
2025-09-03 10:52:40,569 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,574 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:46341
2025-09-03 10:52:40,574 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:46341
2025-09-03 10:52:40,574 - distributed.worker - INFO -          dashboard at:           10.6.105.4:39685
2025-09-03 10:52:40,574 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,574 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,574 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:40,574 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:40,574 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-2_h9vwo1
2025-09-03 10:52:40,574 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,579 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:33051
2025-09-03 10:52:40,579 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:33051
2025-09-03 10:52:40,579 - distributed.worker - INFO -          dashboard at:           10.6.105.4:34449
2025-09-03 10:52:40,579 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,579 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,579 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:40,579 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:40,579 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-qda5lx1p
2025-09-03 10:52:40,579 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,588 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:40,589 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,590 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,591 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:40,602 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:40,604 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,604 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,605 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:40,630 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:40,632 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,632 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,633 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:40,643 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:39057
2025-09-03 10:52:40,643 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:39057
2025-09-03 10:52:40,643 - distributed.worker - INFO -          dashboard at:           10.6.105.4:34063
2025-09-03 10:52:40,643 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,643 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,643 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:40,643 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:40,643 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-gtntrmt9
2025-09-03 10:52:40,643 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,715 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:40,716 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,716 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,718 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:40,799 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:40,801 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,801 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,802 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:40,808 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:45481
2025-09-03 10:52:40,808 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:45481
2025-09-03 10:52:40,808 - distributed.worker - INFO -          dashboard at:           10.6.105.4:36797
2025-09-03 10:52:40,808 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,808 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,808 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:40,808 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:40,808 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-qjvf8j2g
2025-09-03 10:52:40,808 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,856 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:37845
2025-09-03 10:52:40,856 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:37845
2025-09-03 10:52:40,856 - distributed.worker - INFO -          dashboard at:           10.6.105.4:44847
2025-09-03 10:52:40,856 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,856 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,856 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:40,856 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:40,856 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-4jabiyut
2025-09-03 10:52:40,856 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,869 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:40,869 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:44999
2025-09-03 10:52:40,869 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:44999
2025-09-03 10:52:40,870 - distributed.worker - INFO -          dashboard at:           10.6.105.4:39165
2025-09-03 10:52:40,870 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,870 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,870 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:40,870 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:40,870 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-tiu9e428
2025-09-03 10:52:40,870 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,870 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,871 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,872 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:40,887 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:40637
2025-09-03 10:52:40,887 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:40637
2025-09-03 10:52:40,887 - distributed.worker - INFO -          dashboard at:           10.6.105.4:40761
2025-09-03 10:52:40,887 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,887 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,887 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:40,887 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:40,887 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-nd9l_go8
2025-09-03 10:52:40,887 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,890 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:36105
2025-09-03 10:52:40,890 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:36105
2025-09-03 10:52:40,890 - distributed.worker - INFO -          dashboard at:           10.6.105.4:38221
2025-09-03 10:52:40,890 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,890 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,890 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:40,890 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:40,890 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-w5trn98_
2025-09-03 10:52:40,890 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,895 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:42953
2025-09-03 10:52:40,896 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:42953
2025-09-03 10:52:40,896 - distributed.worker - INFO -          dashboard at:           10.6.105.4:38519
2025-09-03 10:52:40,896 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,896 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,896 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:40,896 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:40,896 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-lzi_k0hj
2025-09-03 10:52:40,896 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,907 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:43849
2025-09-03 10:52:40,907 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:43849
2025-09-03 10:52:40,907 - distributed.worker - INFO -          dashboard at:           10.6.105.4:37913
2025-09-03 10:52:40,907 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,908 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,908 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:40,908 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:40,908 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-1fcgipcm
2025-09-03 10:52:40,908 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,912 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:34003
2025-09-03 10:52:40,912 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:34003
2025-09-03 10:52:40,912 - distributed.worker - INFO -          dashboard at:           10.6.105.4:38149
2025-09-03 10:52:40,912 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,912 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,912 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:40,912 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:40,912 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-5zjyy001
2025-09-03 10:52:40,912 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,917 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:36907
2025-09-03 10:52:40,917 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:36907
2025-09-03 10:52:40,917 - distributed.worker - INFO -          dashboard at:           10.6.105.4:34723
2025-09-03 10:52:40,917 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,917 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,917 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:40,917 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:40,917 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-94egqhzi
2025-09-03 10:52:40,917 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,942 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:34833
2025-09-03 10:52:40,942 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:34833
2025-09-03 10:52:40,942 - distributed.worker - INFO -          dashboard at:           10.6.105.4:32829
2025-09-03 10:52:40,942 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,942 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,942 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:40,942 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:40,942 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-33mq756y
2025-09-03 10:52:40,942 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,944 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:37719
2025-09-03 10:52:40,944 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:37719
2025-09-03 10:52:40,944 - distributed.worker - INFO -          dashboard at:           10.6.105.4:35569
2025-09-03 10:52:40,944 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,944 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,944 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:40,944 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:40,944 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-uo9ic3n9
2025-09-03 10:52:40,944 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,945 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:36251
2025-09-03 10:52:40,945 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:36251
2025-09-03 10:52:40,945 - distributed.worker - INFO -          dashboard at:           10.6.105.4:39697
2025-09-03 10:52:40,945 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,945 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,945 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:40,945 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:40,945 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-gj3mhohg
2025-09-03 10:52:40,945 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,953 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:40,954 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,954 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,956 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:40,966 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:34111
2025-09-03 10:52:40,966 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:34111
2025-09-03 10:52:40,966 - distributed.worker - INFO -          dashboard at:           10.6.105.4:42633
2025-09-03 10:52:40,966 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,966 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,966 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:40,966 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:40,966 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-5x10f5c9
2025-09-03 10:52:40,966 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,981 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:40,982 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:40,982 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:40,984 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:41,009 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:41,010 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,010 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,012 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:41,021 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:35125
2025-09-03 10:52:41,021 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:35125
2025-09-03 10:52:41,021 - distributed.worker - INFO -          dashboard at:           10.6.105.4:40335
2025-09-03 10:52:41,021 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,021 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,021 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,021 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,021 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-s_ni94ul
2025-09-03 10:52:41,021 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,041 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:45215
2025-09-03 10:52:41,042 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:45215
2025-09-03 10:52:41,042 - distributed.worker - INFO -          dashboard at:           10.6.105.4:41669
2025-09-03 10:52:41,042 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,042 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,042 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,042 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,042 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-smvj5yfh
2025-09-03 10:52:41,042 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,049 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:37279
2025-09-03 10:52:41,049 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:37279
2025-09-03 10:52:41,049 - distributed.worker - INFO -          dashboard at:           10.6.105.4:34141
2025-09-03 10:52:41,049 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,049 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,049 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,049 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,049 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-xdd9on4g
2025-09-03 10:52:41,049 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,052 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:41,053 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,054 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,055 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:41,066 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:41,067 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,068 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,069 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:41,081 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:41,082 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:38677
2025-09-03 10:52:41,082 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:38677
2025-09-03 10:52:41,082 - distributed.worker - INFO -          dashboard at:           10.6.105.4:40633
2025-09-03 10:52:41,082 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,082 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,082 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,082 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,082 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-y2yngk5q
2025-09-03 10:52:41,082 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,082 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,082 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,084 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:41,087 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:38245
2025-09-03 10:52:41,087 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:38245
2025-09-03 10:52:41,087 - distributed.worker - INFO -          dashboard at:           10.6.105.4:35889
2025-09-03 10:52:41,087 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,087 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,087 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,087 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,087 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-q3a94d66
2025-09-03 10:52:41,087 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,097 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:44471
2025-09-03 10:52:41,097 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:44471
2025-09-03 10:52:41,097 - distributed.worker - INFO -          dashboard at:           10.6.105.4:46041
2025-09-03 10:52:41,097 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,097 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,097 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,097 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,097 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-s8c9oitc
2025-09-03 10:52:41,097 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,098 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:42239
2025-09-03 10:52:41,098 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:42239
2025-09-03 10:52:41,098 - distributed.worker - INFO -          dashboard at:           10.6.105.4:33553
2025-09-03 10:52:41,098 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,098 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,098 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,098 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,098 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-2sktapk0
2025-09-03 10:52:41,098 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,109 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:41,110 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,110 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,112 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:41,137 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:41,138 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,139 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,140 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:41,152 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:41,153 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,153 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,155 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:41,166 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:41,167 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,167 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,169 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:41,180 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:41,181 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,181 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,183 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:41,194 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:41,195 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,196 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,197 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:41,209 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:41,210 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,210 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,211 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:41,223 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:41,224 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,224 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,226 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:41,237 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:41,238 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,238 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,240 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:41,251 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:41,252 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,253 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,254 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:41,264 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:34765
2025-09-03 10:52:41,264 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:34765
2025-09-03 10:52:41,264 - distributed.worker - INFO -          dashboard at:           10.6.105.4:43853
2025-09-03 10:52:41,264 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,264 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,264 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,264 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,264 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-skx_caht
2025-09-03 10:52:41,264 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,266 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:41,267 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,267 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,268 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:41,280 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:41,281 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,281 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,282 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:41,294 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:41,295 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,295 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,297 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:41,305 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:38523
2025-09-03 10:52:41,305 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:38523
2025-09-03 10:52:41,305 - distributed.worker - INFO -          dashboard at:           10.6.105.4:40379
2025-09-03 10:52:41,305 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,305 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,305 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,305 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,305 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-zexaqb6a
2025-09-03 10:52:41,305 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,323 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:41,324 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,324 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,326 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:41,334 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:43095
2025-09-03 10:52:41,334 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:43095
2025-09-03 10:52:41,334 - distributed.worker - INFO -          dashboard at:           10.6.105.4:42313
2025-09-03 10:52:41,334 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,334 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,334 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,335 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-b19ii6xc
2025-09-03 10:52:41,335 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,337 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:41,338 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,338 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,340 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:41,351 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:41,352 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,352 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,354 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:41,405 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:42069
2025-09-03 10:52:41,405 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:42069
2025-09-03 10:52:41,405 - distributed.worker - INFO -          dashboard at:           10.6.105.4:43343
2025-09-03 10:52:41,405 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,405 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,405 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,405 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,405 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ksf3k69j
2025-09-03 10:52:41,405 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,409 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:44213
2025-09-03 10:52:41,409 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:44213
2025-09-03 10:52:41,409 - distributed.worker - INFO -          dashboard at:           10.6.105.4:34577
2025-09-03 10:52:41,409 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,409 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,409 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,409 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,409 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-nbn3q2ib
2025-09-03 10:52:41,409 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,409 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:42813
2025-09-03 10:52:41,409 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:42813
2025-09-03 10:52:41,409 - distributed.worker - INFO -          dashboard at:           10.6.105.4:46705
2025-09-03 10:52:41,409 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,409 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,409 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,409 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,409 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-t6or6wcw
2025-09-03 10:52:41,409 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,410 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:36523
2025-09-03 10:52:41,411 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:36523
2025-09-03 10:52:41,411 - distributed.worker - INFO -          dashboard at:           10.6.105.4:33929
2025-09-03 10:52:41,411 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,411 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,411 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,411 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,411 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-u2212aja
2025-09-03 10:52:41,411 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,426 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:40023
2025-09-03 10:52:41,426 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:40023
2025-09-03 10:52:41,426 - distributed.worker - INFO -          dashboard at:           10.6.105.4:40401
2025-09-03 10:52:41,426 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,426 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,426 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,426 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,426 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-xh0xs3lm
2025-09-03 10:52:41,426 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,427 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:38659
2025-09-03 10:52:41,427 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:38659
2025-09-03 10:52:41,427 - distributed.worker - INFO -          dashboard at:           10.6.105.4:42277
2025-09-03 10:52:41,427 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,427 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,427 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,427 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,427 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-gh7r63g7
2025-09-03 10:52:41,427 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,441 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:37519
2025-09-03 10:52:41,441 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:37519
2025-09-03 10:52:41,441 - distributed.worker - INFO -          dashboard at:           10.6.105.4:37001
2025-09-03 10:52:41,441 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,441 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,441 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,441 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,441 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-szhruu0u
2025-09-03 10:52:41,441 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,441 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:40161
2025-09-03 10:52:41,441 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:40161
2025-09-03 10:52:41,441 - distributed.worker - INFO -          dashboard at:           10.6.105.4:45489
2025-09-03 10:52:41,441 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,441 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,441 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,441 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,441 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-tzcflxtn
2025-09-03 10:52:41,441 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,442 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:39179
2025-09-03 10:52:41,442 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:39179
2025-09-03 10:52:41,442 - distributed.worker - INFO -          dashboard at:           10.6.105.4:37803
2025-09-03 10:52:41,442 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,442 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,442 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,442 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,442 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-cvmq8kvm
2025-09-03 10:52:41,442 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,452 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:42387
2025-09-03 10:52:41,452 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:42387
2025-09-03 10:52:41,452 - distributed.worker - INFO -          dashboard at:           10.6.105.4:40121
2025-09-03 10:52:41,452 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,452 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,452 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,452 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,452 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-y5tf__sg
2025-09-03 10:52:41,452 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,452 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:41355
2025-09-03 10:52:41,453 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:41355
2025-09-03 10:52:41,453 - distributed.worker - INFO -          dashboard at:           10.6.105.4:38671
2025-09-03 10:52:41,453 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,453 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,453 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,453 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,453 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:37993
2025-09-03 10:52:41,453 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-y95lmt1e
2025-09-03 10:52:41,453 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:37993
2025-09-03 10:52:41,453 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,453 - distributed.worker - INFO -          dashboard at:           10.6.105.4:37867
2025-09-03 10:52:41,453 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,453 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,453 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,453 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,453 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-vssf8flc
2025-09-03 10:52:41,453 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,454 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:46847
2025-09-03 10:52:41,454 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:46847
2025-09-03 10:52:41,454 - distributed.worker - INFO -          dashboard at:           10.6.105.4:44431
2025-09-03 10:52:41,454 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,454 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,454 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,454 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,454 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-z5w36cbu
2025-09-03 10:52:41,454 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,455 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:34419
2025-09-03 10:52:41,455 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:34419
2025-09-03 10:52:41,455 - distributed.worker - INFO -          dashboard at:           10.6.105.4:37307
2025-09-03 10:52:41,455 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,455 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,455 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,455 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,455 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-6z72p8_2
2025-09-03 10:52:41,455 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,456 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:37829
2025-09-03 10:52:41,456 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:37829
2025-09-03 10:52:41,456 - distributed.worker - INFO -          dashboard at:           10.6.105.4:39259
2025-09-03 10:52:41,456 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,456 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,456 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,456 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,456 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-6sthaf01
2025-09-03 10:52:41,456 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,459 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:33413
2025-09-03 10:52:41,459 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:33413
2025-09-03 10:52:41,459 - distributed.worker - INFO -          dashboard at:           10.6.105.4:45641
2025-09-03 10:52:41,459 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,459 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,459 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,459 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,459 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-9z9wla4s
2025-09-03 10:52:41,459 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,463 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:37305
2025-09-03 10:52:41,463 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:37305
2025-09-03 10:52:41,463 - distributed.worker - INFO -          dashboard at:           10.6.105.4:41839
2025-09-03 10:52:41,463 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,463 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,463 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,463 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,463 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-jc719_j_
2025-09-03 10:52:41,463 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:45521
2025-09-03 10:52:41,463 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,463 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:45521
2025-09-03 10:52:41,463 - distributed.worker - INFO -          dashboard at:           10.6.105.4:37287
2025-09-03 10:52:41,463 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,463 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:37427
2025-09-03 10:52:41,463 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,463 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,463 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:37427
2025-09-03 10:52:41,463 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,463 - distributed.worker - INFO -          dashboard at:           10.6.105.4:46751
2025-09-03 10:52:41,463 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-7tjqn8z6
2025-09-03 10:52:41,463 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,463 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,463 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,463 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,463 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,463 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-topse575
2025-09-03 10:52:41,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,465 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:43875
2025-09-03 10:52:41,465 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:43875
2025-09-03 10:52:41,465 - distributed.worker - INFO -          dashboard at:           10.6.105.4:36685
2025-09-03 10:52:41,465 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,465 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,465 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,465 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,465 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-q5o066jq
2025-09-03 10:52:41,465 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,465 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:32987
2025-09-03 10:52:41,466 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:32987
2025-09-03 10:52:41,466 - distributed.worker - INFO -          dashboard at:           10.6.105.4:44041
2025-09-03 10:52:41,466 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,466 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,466 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,466 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,466 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-nmbtmxo6
2025-09-03 10:52:41,466 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,467 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:45967
2025-09-03 10:52:41,467 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:45967
2025-09-03 10:52:41,467 - distributed.worker - INFO -          dashboard at:           10.6.105.4:40843
2025-09-03 10:52:41,467 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,467 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,467 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,467 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,467 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ilk_jh72
2025-09-03 10:52:41,467 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,469 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:37943
2025-09-03 10:52:41,469 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:37943
2025-09-03 10:52:41,469 - distributed.worker - INFO -          dashboard at:           10.6.105.4:36257
2025-09-03 10:52:41,469 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,469 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,469 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,469 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,469 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-suvqr7oz
2025-09-03 10:52:41,469 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,471 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:42755
2025-09-03 10:52:41,471 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:42755
2025-09-03 10:52:41,471 - distributed.worker - INFO -          dashboard at:           10.6.105.4:43545
2025-09-03 10:52:41,471 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,471 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,471 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,471 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,471 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-e7o7dof2
2025-09-03 10:52:41,471 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,471 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:43359
2025-09-03 10:52:41,471 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:43359
2025-09-03 10:52:41,471 - distributed.worker - INFO -          dashboard at:           10.6.105.4:41897
2025-09-03 10:52:41,471 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,471 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,471 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,471 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,471 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-8ck2ycla
2025-09-03 10:52:41,471 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,476 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:46375
2025-09-03 10:52:41,476 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:46375
2025-09-03 10:52:41,476 - distributed.worker - INFO -          dashboard at:           10.6.105.4:41595
2025-09-03 10:52:41,476 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,476 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,476 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,476 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,476 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-nzkwn59e
2025-09-03 10:52:41,476 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,477 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:36657
2025-09-03 10:52:41,478 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:36657
2025-09-03 10:52:41,478 - distributed.worker - INFO -          dashboard at:           10.6.105.4:37357
2025-09-03 10:52:41,478 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,478 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,478 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:41,478 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:41,478 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-kc7mbrq5
2025-09-03 10:52:41,478 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,673 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:41,674 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,674 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,676 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:41,716 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:41,717 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:41,718 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:41,719 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:42,349 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:42,351 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:42,351 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:42,353 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:42,421 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:41939'
2025-09-03 10:52:42,426 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:41117'
2025-09-03 10:52:42,431 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:41219'
2025-09-03 10:52:42,436 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:33881'
2025-09-03 10:52:42,825 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:42,827 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:42,827 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:42,828 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:42,870 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:42,871 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:42,871 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:42,872 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:42,884 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:42,886 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:42,886 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:42,887 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:42,899 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:42,900 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:42,900 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:42,902 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:43,018 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:43,019 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,019 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,021 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:43,061 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:43,062 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,063 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,064 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:43,076 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:43,077 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,077 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,078 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:43,090 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:43,091 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,091 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,093 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:43,119 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:43,120 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,120 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,121 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:43,133 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:43,134 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,134 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,136 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:43,162 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:43,163 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,163 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,164 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:43,177 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:43,178 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,178 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,179 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:43,191 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:43,192 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,192 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,194 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:43,194 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:42933
2025-09-03 10:52:43,194 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:42933
2025-09-03 10:52:43,194 - distributed.worker - INFO -          dashboard at:           10.6.105.4:37741
2025-09-03 10:52:43,194 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,194 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,194 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:43,195 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:43,195 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-f0aae9ca
2025-09-03 10:52:43,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,201 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:42805
2025-09-03 10:52:43,201 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:42805
2025-09-03 10:52:43,201 - distributed.worker - INFO -          dashboard at:           10.6.105.4:45181
2025-09-03 10:52:43,201 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,202 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,202 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:43,202 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:43,202 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-gq8ozzkp
2025-09-03 10:52:43,202 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,205 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:43383
2025-09-03 10:52:43,205 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:43383
2025-09-03 10:52:43,205 - distributed.worker - INFO -          dashboard at:           10.6.105.4:33575
2025-09-03 10:52:43,205 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,205 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,205 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:43,205 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:43,205 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-_g8qntca
2025-09-03 10:52:43,205 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,206 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:43,207 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,207 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,209 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:43,235 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:43,236 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,238 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:43,250 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:43,251 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,251 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,253 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:43,264 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:43,265 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,266 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,267 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:43,273 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:43801
2025-09-03 10:52:43,273 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:43801
2025-09-03 10:52:43,273 - distributed.worker - INFO -          dashboard at:           10.6.105.4:38357
2025-09-03 10:52:43,274 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,274 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,274 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:43,274 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:43,274 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-tfeeuqbj
2025-09-03 10:52:43,274 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,279 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:43,280 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,280 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,282 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:43,293 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:43,294 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,295 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,296 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:43,307 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.4:44311'
2025-09-03 10:52:43,308 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:43,309 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,309 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,310 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:43,322 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:43,323 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,323 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,325 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:43,337 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:43,338 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,338 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,340 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:43,352 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:43,353 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,353 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,354 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:43,993 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:43,994 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:43,994 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:43,996 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:44,008 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:44,009 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,009 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,011 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:44,022 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:44,024 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,024 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,025 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:44,096 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.4:46661
2025-09-03 10:52:44,096 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.4:46661
2025-09-03 10:52:44,096 - distributed.worker - INFO -          dashboard at:           10.6.105.4:41405
2025-09-03 10:52:44,096 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,096 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,096 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:44,096 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:44,096 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-xzfm54o9
2025-09-03 10:52:44,096 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,242 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:44,243 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,243 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,245 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:44,256 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:44,258 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,258 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,259 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:44,271 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:44,272 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,273 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,274 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:44,360 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:44,361 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:44,361 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:44,363 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:45,165 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:45,169 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,169 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,175 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:45,178 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:45,179 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,180 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,181 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:45,193 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:45,194 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,194 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,196 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:45,520 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:45,522 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:45,522 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:45,524 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:46,343 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:46,345 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,345 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,347 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:46,358 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:46,359 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,359 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,361 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:46,373 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:46,374 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,374 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,376 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:46,388 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:46,389 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,389 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,391 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:46,403 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:46,404 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,404 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,406 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:46,418 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:46,419 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:46,419 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:46,421 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:47,811 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:47,813 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:47,813 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:47,815 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:48,477 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:48,479 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:48,479 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,481 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:48,862 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:48,864 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:48,864 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,866 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:48,878 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:48,880 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:48,880 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,882 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:48,893 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:48,895 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:48,895 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,897 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:48,908 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:48,910 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:48,910 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:48,925 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,000 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,002 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,002 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,004 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,016 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,017 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,017 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,019 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,387 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,389 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,389 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,391 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:49,402 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:49,404 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:49,404 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:49,406 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:50,018 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:50,019 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:50,019 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:50,021 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:51,892 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:51,893 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:51,893 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:51,895 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:55,619 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:55,621 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,621 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,623 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:55,636 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:55,637 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,637 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,639 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:55,652 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:55,653 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,653 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,655 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:10,871 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:10,889 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:10,892 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:10,897 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:10,909 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:10,914 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:10,918 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:10,937 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:10,943 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:10,945 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:11,595 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:11,609 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:11,635 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:11,805 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:19,915 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:20,008 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:20,022 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:22,336 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:22,337 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:22,337 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:22,339 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:22,354 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:22,355 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:22,355 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:22,357 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:22,371 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:22,373 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:22,373 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:22,375 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:22,389 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:22,391 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:22,391 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:22,392 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:22,410 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:22,414 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:22,414 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:22,419 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:22,424 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:22,425 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:22,426 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:22,428 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:22,442 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:22,443 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:22,443 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:22,445 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:22,898 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:26,626 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:26,640 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:26,658 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:33,566 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:33,567 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:33,568 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:33,569 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:33,583 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:33,584 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:33,584 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:33,586 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:42,806 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:56:38,108 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,115 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,686 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,687 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,702 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,710 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,738 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,739 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,186 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,187 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,557 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,562 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,570 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,579 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,959 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,960 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,209 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,210 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,558 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,560 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,605 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,611 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,676 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,678 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,746 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,752 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,834 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,838 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,849 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,850 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,008 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,010 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,077 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,080 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,332 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,335 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,373 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,374 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,754 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,757 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,889 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,890 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,025 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,027 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,148 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,150 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,182 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,187 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,214 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,219 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,286 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,288 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,443 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,448 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,520 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,525 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,943 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,944 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,975 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,980 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,264 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,266 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,383 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,388 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,475 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,476 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,486 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,487 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,508 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,511 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,869 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,874 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,003 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,005 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,246 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,252 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,317 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,319 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,462 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,464 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,561 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,565 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,680 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,690 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,742 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,743 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,768 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,775 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,829 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,834 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,845 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,850 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,198 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,203 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,346 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,348 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,688 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,693 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,919 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,921 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,010 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,016 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,158 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,163 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,191 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,193 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,299 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,305 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,323 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,325 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,418 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,424 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,545 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,547 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,598 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,599 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,701 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,702 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,977 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,979 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,247 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,253 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,397 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,398 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,548 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,552 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,590 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,595 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,172 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,174 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,412 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,416 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,418 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,424 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,639 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,641 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,696 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,698 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,938 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,940 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,213 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,215 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,757 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,763 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,054 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,055 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,794 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,799 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,023 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,024 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,175 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,176 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,385 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,387 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,476 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,477 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,527 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,535 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,677 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,679 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,987 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,988 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:53,137 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:53,138 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:53,247 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:53,249 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:53,364 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:53,370 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:53,391 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:53,397 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:53,411 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:53,413 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:53,709 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:53,715 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:53,919 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:53,924 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:54,981 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:54,987 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:55,000 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:55,012 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:55,044 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:55,046 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:56,817 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:56,825 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:57,623 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:57,625 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:59,212 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:59,219 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:00,096 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:00,098 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:00,536 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:00,539 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:01,557 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:01,566 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:02,249 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:02,251 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:02,253 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:02,258 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:02,579 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:02,581 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:02,608 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:02,610 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:02,685 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:02,686 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:03,538 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:03,539 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:03,960 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:03,962 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:08,458 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,459 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,459 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,459 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,461 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,461 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,461 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,462 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,461 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,462 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,463 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,464 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,462 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,465 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,464 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,461 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,465 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,465 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,466 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,465 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,466 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,466 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,466 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,467 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,467 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,467 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,467 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,467 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,467 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,465 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,468 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,468 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,466 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,469 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,469 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,470 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,468 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,470 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,470 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,470 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,471 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,471 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,472 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,473 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,473 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,474 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,474 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,474 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,472 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,475 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,475 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,476 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,476 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,476 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,475 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,476 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,477 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,475 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,478 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,477 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,478 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,478 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,478 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,478 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,478 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,478 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,479 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,479 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,479 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,479 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,480 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,479 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,480 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,480 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,480 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,480 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,481 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,482 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,482 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,482 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,482 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,482 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,483 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,484 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,484 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,485 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:08,485 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:08,487 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,021 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,023 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,333 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,334 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,335 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,336 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,341 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,343 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,348 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,350 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,501 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,506 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,526 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,528 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,529 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,531 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,542 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,544 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,544 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,547 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,632 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,634 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,638 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,640 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,657 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,659 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,659 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,661 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,734 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,736 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,872 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,874 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,931 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,933 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,105 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,107 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,116 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,119 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,122 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,124 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,170 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,175 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,277 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,280 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,279 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,281 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,392 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,393 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,395 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,396 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,397 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,399 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,416 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,418 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,435 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,437 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,477 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,479 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,485 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,488 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,489 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,490 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,491 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,493 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,493 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,495 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,530 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,530 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,532 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,533 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,566 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,568 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,589 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,590 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,591 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,599 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,638 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,640 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,685 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,687 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,012 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,014 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,097 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,099 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,106 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,108 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,166 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,169 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,251 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,253 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,315 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,317 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,324 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,326 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,331 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,333 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,386 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,388 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,391 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,393 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,550 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,552 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,603 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,605 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,749 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,757 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,957 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,959 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,024 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,026 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,215 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,220 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,794 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,797 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:14,971 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:14,973 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:15,051 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:15,053 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:15,471 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:15,473 - distributed.utils - INFO - Reload module qme_vars from .py file
