Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-03 16:27:09,873 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:37511'
2025-09-03 16:27:09,881 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:41385'
2025-09-03 16:27:09,884 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:40483'
2025-09-03 16:27:09,888 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:38055'
2025-09-03 16:27:09,892 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:34721'
2025-09-03 16:27:09,895 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:44103'
2025-09-03 16:27:09,900 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:43245'
2025-09-03 16:27:09,904 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:34981'
2025-09-03 16:27:09,907 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:35157'
2025-09-03 16:27:09,912 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:33025'
2025-09-03 16:27:09,916 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:42825'
2025-09-03 16:27:09,920 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:44851'
2025-09-03 16:27:09,923 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:43547'
2025-09-03 16:27:09,927 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:39211'
2025-09-03 16:27:09,931 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:34191'
2025-09-03 16:27:09,934 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:46335'
2025-09-03 16:27:09,938 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:40253'
2025-09-03 16:27:09,942 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:33165'
2025-09-03 16:27:09,947 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:45021'
2025-09-03 16:27:09,951 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:37455'
2025-09-03 16:27:10,081 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:43697'
2025-09-03 16:27:10,086 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:33747'
2025-09-03 16:27:10,090 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:42545'
2025-09-03 16:27:10,095 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:45701'
2025-09-03 16:27:10,100 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:36119'
2025-09-03 16:27:10,104 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:37367'
2025-09-03 16:27:10,109 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:39817'
2025-09-03 16:27:10,114 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:46365'
2025-09-03 16:27:10,119 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:41667'
2025-09-03 16:27:10,123 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:43565'
2025-09-03 16:27:10,128 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:38921'
2025-09-03 16:27:10,132 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:43617'
2025-09-03 16:27:10,137 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:38413'
2025-09-03 16:27:10,140 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:46415'
2025-09-03 16:27:10,142 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:34929'
2025-09-03 16:27:10,151 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:43287'
2025-09-03 16:27:10,155 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:41097'
2025-09-03 16:27:10,159 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:46499'
2025-09-03 16:27:10,164 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:44883'
2025-09-03 16:27:10,168 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:36647'
2025-09-03 16:27:10,173 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:44953'
2025-09-03 16:27:10,176 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:38393'
2025-09-03 16:27:10,181 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:34329'
2025-09-03 16:27:10,185 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:39383'
2025-09-03 16:27:10,189 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:33455'
2025-09-03 16:27:10,192 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:46165'
2025-09-03 16:27:10,196 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:38667'
2025-09-03 16:27:10,200 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:37351'
2025-09-03 16:27:10,205 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:41769'
2025-09-03 16:27:10,208 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:46027'
2025-09-03 16:27:10,214 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:34263'
2025-09-03 16:27:10,218 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.14:45233'
2025-09-03 16:27:11,017 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:33421
2025-09-03 16:27:11,017 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:33483
2025-09-03 16:27:11,017 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:40221
2025-09-03 16:27:11,017 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:42845
2025-09-03 16:27:11,017 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:40083
2025-09-03 16:27:11,017 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:42541
2025-09-03 16:27:11,017 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:37235
2025-09-03 16:27:11,017 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:33421
2025-09-03 16:27:11,017 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:45689
2025-09-03 16:27:11,017 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:41875
2025-09-03 16:27:11,017 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:33139
2025-09-03 16:27:11,017 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:35775
2025-09-03 16:27:11,017 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:33483
2025-09-03 16:27:11,017 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:32789
2025-09-03 16:27:11,017 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:44281
2025-09-03 16:27:11,017 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:40221
2025-09-03 16:27:11,017 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:42845
2025-09-03 16:27:11,017 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:40083
2025-09-03 16:27:11,017 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:42541
2025-09-03 16:27:11,017 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:37235
2025-09-03 16:27:11,017 - distributed.worker - INFO -          dashboard at:          10.6.102.14:39691
2025-09-03 16:27:11,017 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:36709
2025-09-03 16:27:11,017 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:45689
2025-09-03 16:27:11,017 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:34831
2025-09-03 16:27:11,017 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:41875
2025-09-03 16:27:11,017 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:33139
2025-09-03 16:27:11,017 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:35775
2025-09-03 16:27:11,017 - distributed.worker - INFO -          dashboard at:          10.6.102.14:34971
2025-09-03 16:27:11,017 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:32789
2025-09-03 16:27:11,017 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:44281
2025-09-03 16:27:11,017 - distributed.worker - INFO -          dashboard at:          10.6.102.14:46771
2025-09-03 16:27:11,017 - distributed.worker - INFO -          dashboard at:          10.6.102.14:45699
2025-09-03 16:27:11,017 - distributed.worker - INFO -          dashboard at:          10.6.102.14:45407
2025-09-03 16:27:11,017 - distributed.worker - INFO -          dashboard at:          10.6.102.14:38245
2025-09-03 16:27:11,017 - distributed.worker - INFO -          dashboard at:          10.6.102.14:41039
2025-09-03 16:27:11,017 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,017 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:36709
2025-09-03 16:27:11,017 - distributed.worker - INFO -          dashboard at:          10.6.102.14:42495
2025-09-03 16:27:11,017 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:34831
2025-09-03 16:27:11,017 - distributed.worker - INFO -          dashboard at:          10.6.102.14:41797
2025-09-03 16:27:11,017 - distributed.worker - INFO -          dashboard at:          10.6.102.14:37441
2025-09-03 16:27:11,017 - distributed.worker - INFO -          dashboard at:          10.6.102.14:44903
2025-09-03 16:27:11,017 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,017 - distributed.worker - INFO -          dashboard at:          10.6.102.14:42331
2025-09-03 16:27:11,017 - distributed.worker - INFO -          dashboard at:          10.6.102.14:36533
2025-09-03 16:27:11,017 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,017 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,017 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,017 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,017 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,017 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,017 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,017 - distributed.worker - INFO -          dashboard at:          10.6.102.14:35721
2025-09-03 16:27:11,017 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,017 - distributed.worker - INFO -          dashboard at:          10.6.102.14:38591
2025-09-03 16:27:11,017 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,017 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,017 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,017 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,017 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,017 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,017 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,017 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,017 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,017 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,017 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,017 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,017 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,017 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,017 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,017 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,017 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,017 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,017 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,017 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,017 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,017 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,017 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,017 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,017 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,017 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,017 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,017 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,018 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,018 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,018 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,018 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,018 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,018 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,018 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,018 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,018 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,018 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,018 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,018 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,018 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-hls1loqt
2025-09-03 16:27:11,018 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,018 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,018 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,018 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,018 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,018 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,018 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-7rq2oclu
2025-09-03 16:27:11,018 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,018 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,018 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-e4on7czb
2025-09-03 16:27:11,018 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-1cv5ifn2
2025-09-03 16:27:11,018 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-sgjzl5v1
2025-09-03 16:27:11,018 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-w2dbq96i
2025-09-03 16:27:11,018 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-vbt36bs6
2025-09-03 16:27:11,018 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,018 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-3j30ccoc
2025-09-03 16:27:11,018 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,018 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-voe68vjm
2025-09-03 16:27:11,018 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,018 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-nohxhg2h
2025-09-03 16:27:11,018 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-ugmb2j1j
2025-09-03 16:27:11,018 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,018 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-ky19l9b1
2025-09-03 16:27:11,018 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-nq1ucder
2025-09-03 16:27:11,018 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,018 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,018 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,018 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,018 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,018 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,018 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-7m3pm_d7
2025-09-03 16:27:11,018 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,018 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-w3g2pqy5
2025-09-03 16:27:11,018 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,018 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,018 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,018 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,018 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,018 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,020 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:40815
2025-09-03 16:27:11,020 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:40815
2025-09-03 16:27:11,020 - distributed.worker - INFO -          dashboard at:          10.6.102.14:44843
2025-09-03 16:27:11,020 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,020 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,020 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,020 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,021 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-_u794hgd
2025-09-03 16:27:11,021 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,025 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:41245
2025-09-03 16:27:11,025 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:41245
2025-09-03 16:27:11,025 - distributed.worker - INFO -          dashboard at:          10.6.102.14:37685
2025-09-03 16:27:11,025 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,025 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,025 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,025 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,026 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-z65o5egf
2025-09-03 16:27:11,026 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,035 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,036 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,036 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,037 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,039 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,039 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,040 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,040 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,044 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,044 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,045 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,045 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,048 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,050 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,050 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,050 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,051 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,051 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,052 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,053 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,053 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,054 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,054 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,055 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,055 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,056 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,056 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,056 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,057 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,057 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,058 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,058 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,059 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,059 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,059 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,059 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,060 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,061 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,061 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,061 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,062 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,062 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,062 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,062 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,064 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,064 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,064 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,064 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,065 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,066 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,066 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,066 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,067 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,067 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,067 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,067 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,068 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,068 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,068 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,069 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,069 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,070 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,070 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,071 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,071 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,072 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,072 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,074 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,090 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:33745
2025-09-03 16:27:11,090 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:33745
2025-09-03 16:27:11,090 - distributed.worker - INFO -          dashboard at:          10.6.102.14:36653
2025-09-03 16:27:11,090 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,090 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,090 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,090 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,090 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-ku9n9sld
2025-09-03 16:27:11,090 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,112 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,112 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,113 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,114 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,127 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:46509
2025-09-03 16:27:11,127 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:46509
2025-09-03 16:27:11,127 - distributed.worker - INFO -          dashboard at:          10.6.102.14:44869
2025-09-03 16:27:11,127 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,127 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,127 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,127 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,127 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-pzednxc2
2025-09-03 16:27:11,127 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,153 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:33163
2025-09-03 16:27:11,153 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:33163
2025-09-03 16:27:11,153 - distributed.worker - INFO -          dashboard at:          10.6.102.14:35681
2025-09-03 16:27:11,153 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,153 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,153 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,153 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,153 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-op3bn0yu
2025-09-03 16:27:11,153 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,159 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,160 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,160 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,162 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,182 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,183 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,183 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,184 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,244 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:33567
2025-09-03 16:27:11,244 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:33567
2025-09-03 16:27:11,245 - distributed.worker - INFO -          dashboard at:          10.6.102.14:43529
2025-09-03 16:27:11,245 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,245 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,245 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,245 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,245 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-nr0s4f9s
2025-09-03 16:27:11,245 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,270 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,271 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,271 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,272 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,274 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:46837
2025-09-03 16:27:11,274 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:46837
2025-09-03 16:27:11,274 - distributed.worker - INFO -          dashboard at:          10.6.102.14:43951
2025-09-03 16:27:11,274 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,274 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,274 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,274 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,274 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-d73rplhv
2025-09-03 16:27:11,274 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,292 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:33589
2025-09-03 16:27:11,292 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:33589
2025-09-03 16:27:11,292 - distributed.worker - INFO -          dashboard at:          10.6.102.14:36019
2025-09-03 16:27:11,292 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,292 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,292 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,292 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,292 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-0zbw782s
2025-09-03 16:27:11,292 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,299 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,299 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,300 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,301 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,316 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:39263
2025-09-03 16:27:11,316 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:39263
2025-09-03 16:27:11,316 - distributed.worker - INFO -          dashboard at:          10.6.102.14:42185
2025-09-03 16:27:11,316 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,316 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,316 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,316 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,316 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-8h4_e8u9
2025-09-03 16:27:11,316 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,321 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:33751
2025-09-03 16:27:11,322 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:33751
2025-09-03 16:27:11,322 - distributed.worker - INFO -          dashboard at:          10.6.102.14:33319
2025-09-03 16:27:11,322 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,322 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,322 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,322 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,322 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-6pcrqrr4
2025-09-03 16:27:11,322 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,322 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,323 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,323 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,324 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,346 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,347 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,347 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,349 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,352 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,352 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:33523
2025-09-03 16:27:11,353 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:33523
2025-09-03 16:27:11,353 - distributed.worker - INFO -          dashboard at:          10.6.102.14:34205
2025-09-03 16:27:11,353 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,353 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,353 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,353 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,353 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-tr_eb4aa
2025-09-03 16:27:11,353 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,353 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,353 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,355 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:43905
2025-09-03 16:27:11,355 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:43905
2025-09-03 16:27:11,355 - distributed.worker - INFO -          dashboard at:          10.6.102.14:44063
2025-09-03 16:27:11,355 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,355 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,355 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,355 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,355 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,355 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-mztsd936
2025-09-03 16:27:11,355 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,374 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:39565
2025-09-03 16:27:11,374 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:39565
2025-09-03 16:27:11,374 - distributed.worker - INFO -          dashboard at:          10.6.102.14:45033
2025-09-03 16:27:11,374 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,375 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,375 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,375 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,375 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-rozfmw3d
2025-09-03 16:27:11,375 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,375 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:39405
2025-09-03 16:27:11,375 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:39405
2025-09-03 16:27:11,375 - distributed.worker - INFO -          dashboard at:          10.6.102.14:39773
2025-09-03 16:27:11,375 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,375 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,375 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,375 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,375 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-zv0exx26
2025-09-03 16:27:11,375 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,378 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,378 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,379 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,380 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,381 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:45063
2025-09-03 16:27:11,381 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:45063
2025-09-03 16:27:11,381 - distributed.worker - INFO -          dashboard at:          10.6.102.14:43603
2025-09-03 16:27:11,381 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,381 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,381 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,381 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,381 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-xgyn5vnn
2025-09-03 16:27:11,381 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,383 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,384 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:43399
2025-09-03 16:27:11,384 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:43399
2025-09-03 16:27:11,384 - distributed.worker - INFO -          dashboard at:          10.6.102.14:44381
2025-09-03 16:27:11,384 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,384 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,384 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,384 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,384 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,384 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,384 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-qxz75y9g
2025-09-03 16:27:11,384 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,386 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,397 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,397 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,397 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,398 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,405 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,406 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,407 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,408 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,410 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,411 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,411 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,413 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,414 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,415 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,416 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,417 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,520 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:37415
2025-09-03 16:27:11,520 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:37415
2025-09-03 16:27:11,520 - distributed.worker - INFO -          dashboard at:          10.6.102.14:46483
2025-09-03 16:27:11,520 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,520 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,520 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,520 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,520 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-hkm45_2t
2025-09-03 16:27:11,520 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,526 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:45139
2025-09-03 16:27:11,526 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:45139
2025-09-03 16:27:11,526 - distributed.worker - INFO -          dashboard at:          10.6.102.14:46423
2025-09-03 16:27:11,526 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,526 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,526 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,526 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,526 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-1tv2fu9y
2025-09-03 16:27:11,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,527 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:42691
2025-09-03 16:27:11,528 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:42691
2025-09-03 16:27:11,528 - distributed.worker - INFO -          dashboard at:          10.6.102.14:35405
2025-09-03 16:27:11,528 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,528 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,528 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,528 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,528 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-8whsdbab
2025-09-03 16:27:11,528 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,532 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:32907
2025-09-03 16:27:11,532 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:32907
2025-09-03 16:27:11,532 - distributed.worker - INFO -          dashboard at:          10.6.102.14:46843
2025-09-03 16:27:11,532 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,532 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,532 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,532 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,532 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-q_6236by
2025-09-03 16:27:11,533 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,544 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,545 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,545 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,547 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,550 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,551 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,551 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,553 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:37837
2025-09-03 16:27:11,553 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:37837
2025-09-03 16:27:11,553 - distributed.worker - INFO -          dashboard at:          10.6.102.14:39983
2025-09-03 16:27:11,553 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,553 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,553 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,553 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,553 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-4d3eursu
2025-09-03 16:27:11,553 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,553 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,554 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,555 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:41789
2025-09-03 16:27:11,555 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:41789
2025-09-03 16:27:11,555 - distributed.worker - INFO -          dashboard at:          10.6.102.14:40291
2025-09-03 16:27:11,555 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,555 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,555 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,555 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,555 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,555 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-3y6h709f
2025-09-03 16:27:11,555 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,555 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,556 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,557 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:33099
2025-09-03 16:27:11,557 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:33099
2025-09-03 16:27:11,557 - distributed.worker - INFO -          dashboard at:          10.6.102.14:43319
2025-09-03 16:27:11,557 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,557 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,557 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,557 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,557 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-6iegtvam
2025-09-03 16:27:11,557 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,558 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:42405
2025-09-03 16:27:11,558 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:41491
2025-09-03 16:27:11,558 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:42405
2025-09-03 16:27:11,558 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:41491
2025-09-03 16:27:11,558 - distributed.worker - INFO -          dashboard at:          10.6.102.14:36561
2025-09-03 16:27:11,558 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:37115
2025-09-03 16:27:11,558 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,558 - distributed.worker - INFO -          dashboard at:          10.6.102.14:41025
2025-09-03 16:27:11,558 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,558 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:37115
2025-09-03 16:27:11,558 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,558 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,558 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,558 - distributed.worker - INFO -          dashboard at:          10.6.102.14:46821
2025-09-03 16:27:11,558 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,558 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,558 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,558 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,558 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-512ai6b_
2025-09-03 16:27:11,558 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,558 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,558 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-gqj3jzrc
2025-09-03 16:27:11,558 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,558 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,558 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,558 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-zx40k3eb
2025-09-03 16:27:11,558 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,559 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,560 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:39871
2025-09-03 16:27:11,560 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:39871
2025-09-03 16:27:11,560 - distributed.worker - INFO -          dashboard at:          10.6.102.14:45441
2025-09-03 16:27:11,560 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,560 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,560 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,560 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,560 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-lcjecalc
2025-09-03 16:27:11,560 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,560 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,560 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,562 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:41133
2025-09-03 16:27:11,562 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:41133
2025-09-03 16:27:11,562 - distributed.worker - INFO -          dashboard at:          10.6.102.14:44043
2025-09-03 16:27:11,562 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,562 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,562 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,562 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,562 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,562 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-7pxffzd3
2025-09-03 16:27:11,562 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,563 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:36361
2025-09-03 16:27:11,563 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:36361
2025-09-03 16:27:11,563 - distributed.worker - INFO -          dashboard at:          10.6.102.14:37691
2025-09-03 16:27:11,563 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,563 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,563 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,563 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,563 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-ek6_lfr4
2025-09-03 16:27:11,563 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,563 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:37879
2025-09-03 16:27:11,564 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:37879
2025-09-03 16:27:11,564 - distributed.worker - INFO -          dashboard at:          10.6.102.14:43209
2025-09-03 16:27:11,564 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,564 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,564 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,564 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,564 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-af8txa8t
2025-09-03 16:27:11,564 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,567 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:36771
2025-09-03 16:27:11,567 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:36771
2025-09-03 16:27:11,567 - distributed.worker - INFO -          dashboard at:          10.6.102.14:43631
2025-09-03 16:27:11,567 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,567 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,567 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,567 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,567 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-c9yccl8r
2025-09-03 16:27:11,567 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,567 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:34781
2025-09-03 16:27:11,567 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:34781
2025-09-03 16:27:11,567 - distributed.worker - INFO -          dashboard at:          10.6.102.14:42265
2025-09-03 16:27:11,567 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,568 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,568 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,568 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,568 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-7bi2lhvw
2025-09-03 16:27:11,568 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,570 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:45665
2025-09-03 16:27:11,570 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:45665
2025-09-03 16:27:11,570 - distributed.worker - INFO -          dashboard at:          10.6.102.14:43057
2025-09-03 16:27:11,570 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,570 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,570 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,570 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,570 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-awtn3o3h
2025-09-03 16:27:11,570 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,570 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:35961
2025-09-03 16:27:11,570 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:35961
2025-09-03 16:27:11,571 - distributed.worker - INFO -          dashboard at:          10.6.102.14:44221
2025-09-03 16:27:11,571 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,571 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,571 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,571 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,571 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-uflqxo8n
2025-09-03 16:27:11,571 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,572 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:38215
2025-09-03 16:27:11,572 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:38215
2025-09-03 16:27:11,572 - distributed.worker - INFO -          dashboard at:          10.6.102.14:35789
2025-09-03 16:27:11,573 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,573 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,573 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,573 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,573 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-8wvquo40
2025-09-03 16:27:11,573 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,574 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,574 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,574 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,574 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:44361
2025-09-03 16:27:11,575 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:44361
2025-09-03 16:27:11,575 - distributed.worker - INFO -          dashboard at:          10.6.102.14:38213
2025-09-03 16:27:11,575 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,575 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,575 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,575 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,575 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,575 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-pbwj97fl
2025-09-03 16:27:11,575 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,576 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.14:34765
2025-09-03 16:27:11,576 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.14:34765
2025-09-03 16:27:11,576 - distributed.worker - INFO -          dashboard at:          10.6.102.14:41073
2025-09-03 16:27:11,576 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,576 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,576 - distributed.worker - INFO -               Threads:                          2
2025-09-03 16:27:11,577 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-03 16:27:11,577 - distributed.worker - INFO -       Local Directory: /jobfs/148644899.gadi-pbs/dask-scratch-space/worker-lgotb93b
2025-09-03 16:27:11,577 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,581 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,582 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,582 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,583 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,584 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,585 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,585 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,586 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,587 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,588 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,588 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,589 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,590 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,591 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,591 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,592 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,592 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,593 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,594 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,594 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,595 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,595 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,595 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,596 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,597 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,597 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,597 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,598 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,599 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,599 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,599 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,600 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,600 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,601 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,601 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,602 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,603 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,603 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,603 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,604 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,605 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,605 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,605 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,606 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,607 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,607 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,607 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,608 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,609 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,609 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,609 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,609 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,610 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,610 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,610 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,611 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,611 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,612 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,612 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,613 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 16:27:11,613 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:11,614 - distributed.worker - INFO -         Registered to:      tcp://10.6.102.1:8786
2025-09-03 16:27:11,614 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 16:27:11,615 - distributed.core - INFO - Starting established connection to tcp://10.6.102.1:8786
2025-09-03 16:27:35,581 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,581 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,581 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,581 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,582 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,582 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,582 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,582 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,582 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,582 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,582 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,582 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,583 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,583 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,583 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,583 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,583 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,583 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,583 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,583 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,583 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,584 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,584 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,584 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,584 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,584 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,584 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,584 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,584 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,585 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,584 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,585 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,585 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,585 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,585 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,585 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,585 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,585 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,585 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,585 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,585 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,585 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,585 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,585 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,585 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,584 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,585 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,586 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,586 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,586 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,584 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,586 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,586 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,586 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,586 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,586 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,587 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,586 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,586 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,586 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,586 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,587 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,586 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,586 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,586 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,586 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,586 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,587 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,587 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,587 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,587 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,587 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,588 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,588 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,588 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,586 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,586 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,589 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,589 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,589 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,587 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,587 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,589 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,589 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,590 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,590 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,590 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,590 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,588 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,591 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,589 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,590 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,594 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,595 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,596 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,596 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,597 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,598 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,598 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,596 - distributed.worker - INFO - Starting Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:27:35,599 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,601 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,602 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:35,605 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 16:27:38,256 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,257 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,257 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,257 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,257 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,257 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,257 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,257 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,257 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,257 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,258 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,258 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,258 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,258 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,258 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,258 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,258 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,258 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,258 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,258 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,259 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,259 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,259 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,259 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,259 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,259 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,260 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,260 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,260 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,260 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,260 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,260 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,260 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,260 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,260 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,261 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,261 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,260 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,261 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,260 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,261 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,260 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,260 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,261 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,261 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,261 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,260 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,260 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,261 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,261 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,261 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,261 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,261 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,261 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,261 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,261 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,261 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,261 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,261 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,261 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,262 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,262 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,261 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,261 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,261 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,262 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,261 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,262 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,262 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,262 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,262 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,262 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,262 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,262 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,263 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,260 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,262 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,263 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,263 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,263 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,263 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,262 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,264 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,264 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,264 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,264 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,264 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,261 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,264 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,265 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,265 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,265 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,265 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,262 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,266 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,266 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,266 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,266 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,266 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,263 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:27:38,267 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,267 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,268 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,269 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 16:27:38,651 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,651 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,651 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,651 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,651 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,652 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,652 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,652 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,652 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,652 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,652 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,652 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,652 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,652 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,652 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,653 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,653 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,653 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,653 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,653 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,653 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,653 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,653 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,653 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,654 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,654 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,654 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,654 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,654 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,654 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,654 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,655 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,655 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,655 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,655 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,655 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,655 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,655 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,655 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,655 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,655 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,655 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,655 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,656 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,656 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,656 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,656 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,656 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,656 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,656 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,655 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,656 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,656 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,656 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,656 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,656 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,656 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,657 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,656 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,656 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,657 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,657 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,657 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,657 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,657 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,657 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,657 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,657 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,657 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,658 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,658 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,658 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,658 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,658 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,658 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,658 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,658 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,659 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,658 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,659 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,659 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,659 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,659 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,659 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,658 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:27:38,659 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,659 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,660 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,660 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,660 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,661 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,661 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,661 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,661 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,661 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,661 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,661 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,661 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,662 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,662 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,662 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,666 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,669 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:38,677 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-03 16:27:39,095 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,095 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,095 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,095 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,095 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,095 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,096 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,096 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,096 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,096 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,096 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,096 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,096 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,096 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,097 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,097 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,097 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,097 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,097 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,097 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,097 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,097 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,097 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,097 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,097 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,097 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,098 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,098 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,098 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,098 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,098 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,098 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,098 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,098 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,098 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,098 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,098 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,098 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,098 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,098 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,098 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,098 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,099 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,099 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,099 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,099 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,099 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,099 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,099 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,099 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,099 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,099 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,099 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,099 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,099 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,099 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,099 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,099 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,100 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,100 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,100 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,100 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,099 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,100 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,100 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,100 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,100 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,100 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,100 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,100 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,100 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,100 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,100 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,100 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,100 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,101 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,101 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,101 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,101 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,101 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:27:39,101 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,101 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,101 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,101 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,101 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,102 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,102 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,102 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,102 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,102 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,103 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,103 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,103 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,103 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,103 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,103 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,104 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,104 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,104 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,104 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,104 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,104 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,105 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:27:39,106 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-03 16:30:59,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,505 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,507 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,511 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,512 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,524 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:30:59,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:44,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:47,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:49,304 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:49,853 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:50,165 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:50,166 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:50,998 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:51,076 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:51,077 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:51,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:51,551 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:51,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:51,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:51,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:51,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:51,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:51,586 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:51,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:52,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:53,210 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:53,307 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:53,555 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:53,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:54,284 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:54,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:54,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:55,575 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:55,580 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:56,225 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:56,268 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:56,786 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:56,788 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:56,908 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:56,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:57,329 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:58,090 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:31:59,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:00,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:00,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:02,309 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:08,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:08,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:09,561 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:09,975 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:09,978 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:10,370 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:11,206 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:11,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:11,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:11,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:11,840 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:12,173 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:12,384 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:12,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:12,420 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:12,421 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:12,849 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:13,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:13,307 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:13,329 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:14,385 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:14,386 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:14,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:14,451 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:14,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:16,776 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:16,783 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:17,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:18,928 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:19,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:19,633 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:20,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:20,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:20,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:20,454 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:20,459 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:26,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:27,007 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:33,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:33,883 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:34,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:34,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:36,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:37,994 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:38,662 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:40,400 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:40,401 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:40,651 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:40,652 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:41,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:41,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:41,963 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:42,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:42,126 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:42,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:42,903 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:42,905 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:43,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:43,522 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:43,525 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:43,527 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:43,527 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:43,603 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:43,845 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:44,650 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:44,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:44,660 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:44,662 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:44,995 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:46,092 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:46,108 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:46,119 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:46,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:53,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:32:55,221 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:02,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:02,283 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:02,786 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:03,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:04,099 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:04,543 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:04,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:04,838 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:05,473 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:05,924 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:05,926 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:05,926 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:06,470 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:06,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:06,475 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:06,890 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:07,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:07,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:07,166 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:07,166 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:07,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:07,460 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:07,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:08,205 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:09,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:09,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:09,303 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:09,304 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:09,887 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:10,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:11,556 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:15,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:16,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:16,934 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:16,935 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:17,212 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:17,663 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:18,356 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:18,356 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:18,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:18,420 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:18,508 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:18,610 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:18,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:19,817 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:20,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:20,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:20,682 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:20,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:21,618 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:21,900 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,292 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,639 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,745 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,952 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,995 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:22,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:23,273 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:23,606 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:23,800 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:23,802 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:26,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:26,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:39,779 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:55,459 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:55,459 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:55,467 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:33:56,077 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:04,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:04,555 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:05,131 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:07,454 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:07,463 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:07,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:07,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:07,969 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:08,037 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:08,513 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:08,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:08,665 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:09,551 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:10,128 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:10,300 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:11,814 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:12,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:12,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:12,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:13,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:13,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:15,361 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:15,904 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:16,674 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:17,085 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:17,304 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:20,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:21,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:24,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:24,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:25,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:25,028 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:25,031 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:25,034 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:25,035 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:25,052 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:27,889 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:31,028 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:31,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:31,480 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:31,957 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:32,812 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:32,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:34,072 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:34,072 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:34,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:34,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:34,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:34,797 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:34,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:35,407 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:35,672 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:35,883 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:36,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:36,601 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:36,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:36,963 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:37,859 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:38,709 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:38,714 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:39,367 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:39,562 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:40,656 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:40,806 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:41,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:41,619 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:42,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:44,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:44,825 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:44,849 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:46,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:48,755 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:49,925 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:49,928 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:52,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:52,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:52,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:54,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:54,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:54,737 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:55,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:55,164 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:55,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:55,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:55,237 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:55,801 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:55,806 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:55,910 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:55,913 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:55,919 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:56,211 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:56,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:59,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:59,477 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:59,480 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:59,484 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:34:59,484 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:01,682 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:02,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:02,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:02,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:03,532 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:05,268 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:05,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:08,386 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:08,580 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:08,609 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:08,708 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:11,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:12,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:12,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:12,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:12,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:12,835 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:14,399 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:14,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:14,953 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:15,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:15,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:15,518 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:15,524 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:16,407 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:17,716 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:17,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:17,952 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:17,957 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:18,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:18,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:18,716 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:19,479 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:19,481 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:19,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:19,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:19,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:19,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:19,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:20,188 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:20,222 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:20,265 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:20,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:20,871 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:21,709 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:23,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:23,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:23,605 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:24,754 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:24,813 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:30,625 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:31,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:31,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:31,997 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:32,587 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:33,881 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:33,888 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:34,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:34,510 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:35,475 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:35,479 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:35,618 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:35,762 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:36,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:36,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:36,586 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:36,591 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:36,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:36,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:37,044 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:37,056 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:37,361 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:38,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:39,178 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:39,180 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:39,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:39,958 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:39,960 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:40,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:40,910 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:41,037 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:41,040 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:41,047 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:41,051 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:43,495 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:43,712 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:43,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:44,198 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:45,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:51,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:51,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,005 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,279 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,312 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,398 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,639 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,639 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,639 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,639 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,668 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,678 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,845 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:52,845 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:53,054 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:53,447 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:53,450 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:54,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:54,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:54,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:55,439 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:55,443 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:55,494 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:55,682 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:55,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:55,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:55,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:55,814 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:55,890 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:56,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:56,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:56,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:56,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:56,908 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:56,913 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:56,918 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:57,067 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:57,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:57,881 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:57,884 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:57,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:58,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:58,318 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:35:58,748 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:00,714 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:02,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:02,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:02,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:10,887 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:12,023 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:12,470 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:12,501 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:13,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:13,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:13,428 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:13,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:15,208 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:15,385 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:15,389 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:16,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:16,158 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:16,815 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:16,821 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:16,921 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:16,921 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:16,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:17,063 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:19,352 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:19,360 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:19,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:20,596 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:20,835 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:20,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:21,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:21,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:21,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:22,378 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:24,360 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:25,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:25,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:26,250 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:26,252 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:26,253 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:26,777 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:32,487 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:32,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:32,890 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:34,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:34,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:35,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:35,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:35,534 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:35,697 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:36,396 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:36,402 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:36,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:36,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:36,407 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:36,426 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:36,525 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:36,535 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:36,544 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:36,589 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:36,869 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:37,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:37,801 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:38,176 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:38,179 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:38,182 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:38,256 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:38,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:39,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:39,783 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:43,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:43,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:43,428 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:45,599 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:45,599 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:45,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:47,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:49,004 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:49,012 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:49,401 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:50,119 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:51,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:54,774 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:57,323 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:36:58,661 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:01,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:01,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:02,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:02,122 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:02,189 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:02,733 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:03,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:03,233 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:03,239 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:04,137 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:04,137 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:04,139 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:04,420 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:04,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:04,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:04,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:04,855 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:04,858 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:05,211 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:05,558 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:05,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:05,690 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:05,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:05,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:05,888 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:06,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:06,690 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:06,691 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:07,907 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:07,917 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:09,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:09,575 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:10,287 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:11,925 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:12,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:14,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:14,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:17,356 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:23,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:28,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:28,805 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:28,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:28,812 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:29,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:30,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:35,760 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:35,765 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:36,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:37,641 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:37,660 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:37,666 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:37,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:38,065 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:38,070 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:38,072 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:38,285 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:38,285 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:38,400 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:39,158 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:41,131 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:41,137 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:41,143 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:41,148 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:41,195 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:41,203 - distributed.comm.tcp - INFO - Connection from tcp://10.6.102.6:58216 closed before handshake completed
2025-09-03 16:37:41,206 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:41,210 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:41,256 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:41,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:41,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:41,505 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:41,516 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:41,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:42,208 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:42,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:43,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:43,750 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:47,074 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:47,481 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:47,910 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:48,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:48,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:48,449 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:48,635 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:48,892 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:48,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:49,097 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:49,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:49,210 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:49,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:49,776 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:49,776 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:49,784 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:49,840 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:50,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:51,249 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:51,252 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:52,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:52,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:52,217 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:52,223 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:52,608 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:53,627 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:53,629 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:54,131 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:54,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:54,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:54,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:55,021 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:55,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:55,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:55,950 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:55,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:55,969 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:56,551 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:57,892 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:57,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:37:58,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:02,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:02,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:04,131 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:04,138 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:04,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:05,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:05,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:07,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:07,045 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:07,045 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:07,055 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:07,135 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:07,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:07,888 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:08,217 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:08,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:08,618 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:08,621 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:09,328 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:09,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:10,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:10,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:10,181 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:11,076 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:11,149 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:11,516 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:11,542 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:14,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:15,445 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:15,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:15,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:19,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:19,344 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:19,345 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:19,370 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:19,403 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:21,430 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:21,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:24,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:24,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:38,591 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:38,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:38,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:38,600 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:39,663 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:39,666 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:39,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:40,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:40,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:41,311 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:53,910 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:54,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:54,303 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:54,306 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:55,243 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:55,359 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:55,362 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:55,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:55,567 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:55,939 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:56,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:56,011 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:56,233 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:56,352 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:56,352 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:56,591 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:56,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:56,805 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:56,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:57,067 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:57,396 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:57,574 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,402 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:58,837 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:59,138 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:59,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:38:59,528 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:00,077 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:00,080 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:00,080 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:00,360 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:00,360 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:00,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:01,833 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:01,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:01,837 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:01,842 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:01,908 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:05,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:05,761 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:05,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:10,910 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:14,740 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:28,811 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:28,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:29,064 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:29,695 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:29,695 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:29,698 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:29,699 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:30,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:31,004 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:31,202 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:32,347 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:33,058 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:33,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:33,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:33,892 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:33,920 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:33,927 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:34,044 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:34,224 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:34,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:35,002 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:35,586 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:36,077 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:36,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:36,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:36,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:36,910 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:36,916 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:37,025 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:37,396 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:37,397 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:37,663 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:38,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:38,053 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:38,953 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:39,511 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:40,410 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:41,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:49,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:49,367 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:49,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:53,040 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:53,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:53,281 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:53,289 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:53,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:54,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:54,323 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:54,595 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:55,127 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:55,307 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:55,771 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:55,779 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:55,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:56,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:56,365 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:56,889 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,319 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,348 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,540 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,541 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,589 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,645 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:57,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:59,105 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:59,106 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:59,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:59,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:59,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:39:59,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:00,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:00,297 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:05,454 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:06,105 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:07,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:07,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:07,652 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:10,514 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:11,201 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:12,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:12,284 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:14,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:14,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:14,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:14,600 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:14,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:14,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:15,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:15,036 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:15,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:15,127 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:15,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:15,693 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:15,969 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:15,971 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:15,974 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:15,998 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:15,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:16,000 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:16,007 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:16,139 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:16,140 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:16,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,788 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:17,796 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:18,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:18,034 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:18,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:19,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:19,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:19,386 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:20,666 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:20,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:20,673 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:21,132 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:22,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:24,560 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:25,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:26,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:26,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:26,660 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:27,567 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:27,600 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:27,815 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:28,183 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:28,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:28,478 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:28,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:29,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:29,472 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:29,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:29,541 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:29,958 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:30,361 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:30,365 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:30,366 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:30,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:30,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:30,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:31,568 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:31,571 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:31,571 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:31,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:31,645 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:31,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:31,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:31,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:31,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:32,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:33,882 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:33,883 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:34,037 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:34,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:40,166 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:46,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:46,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:47,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:47,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:47,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:47,997 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:48,349 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:48,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:48,611 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:48,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:48,972 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:49,162 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:50,190 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:50,194 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:50,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:50,598 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:50,600 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:50,708 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:50,740 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:50,758 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:51,010 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:54,745 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:57,071 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:57,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:40:57,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:00,017 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:00,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:00,271 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:00,604 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:00,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:01,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:02,738 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:04,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:04,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:04,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:04,938 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:04,938 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:04,997 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:04,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:05,657 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:05,753 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:05,802 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:06,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:06,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:08,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:08,127 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:08,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:08,776 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:09,627 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:22,744 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:22,753 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:23,035 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:23,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:24,889 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:25,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:26,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:29,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:29,308 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:32,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:33,166 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:33,465 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:35,677 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:36,443 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:36,575 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:36,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:36,697 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:36,830 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:37,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:37,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:37,345 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:37,380 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:37,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:37,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:37,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:38,357 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:38,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:38,779 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:38,913 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:39,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:39,189 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:39,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:39,552 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:39,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:39,790 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:40,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:40,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:40,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:42,342 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:42,343 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:42,366 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:45,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:45,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:49,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:50,774 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:52,619 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:52,626 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:41:53,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:00,356 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:00,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:01,046 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:02,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:02,285 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:02,309 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:02,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:03,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:03,120 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:03,555 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:03,559 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:03,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:04,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:05,719 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:05,734 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:08,056 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:08,058 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:08,059 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:08,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:08,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:08,067 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:08,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:08,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:08,237 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:08,237 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:08,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:08,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:08,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:09,401 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:09,496 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:09,574 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:10,619 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:12,415 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:12,790 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:13,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:13,192 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:13,534 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,000 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,288 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,473 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,881 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,892 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:14,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:15,108 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:15,250 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:15,840 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:15,841 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:15,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:16,695 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:16,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:16,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:16,741 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:17,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:17,586 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:17,587 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:18,522 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:18,535 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:18,654 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:19,270 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:19,475 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:20,342 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:20,342 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:20,345 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:21,751 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:21,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:21,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:22,384 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:22,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:23,610 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:28,453 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:28,598 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:30,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:30,585 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:30,598 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:30,731 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:30,731 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:30,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:30,737 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:30,874 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:31,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:31,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:31,673 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:34,806 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.102.1:37265
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2876, in get_data_from_worker
    comm = await rpc.connect(worker)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.14:50108 remote=tcp://10.6.102.1:37265>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-03 16:42:34,822 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,822 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:39871. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,822 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,823 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,823 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,822 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,823 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,823 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,823 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,823 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:38215. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,824 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:34831. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,824 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:33421. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,824 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:40815. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,824 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:33139. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,824 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:45689. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,824 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:35775. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,822 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,822 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,822 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,824 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,824 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,822 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,825 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:36361. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,822 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,822 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,825 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:41133. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,823 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,825 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:33523. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,823 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,825 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:44361. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,825 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:33751. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,823 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,822 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,825 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:34781. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,824 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,825 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:37115. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,824 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,824 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,826 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:41491. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,826 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:41875. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,826 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:41245. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,825 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,824 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,826 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:32789. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,824 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,824 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,826 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:32907. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,826 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:45665. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,824 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,826 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:37415. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,826 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:45139. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,827 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:37837. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,825 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,827 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:42691. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,827 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:39405. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,827 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:46837. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,827 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:45063. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,827 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:33163. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,823 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:34,828 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:42405. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,834 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:38393'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,836 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.16:38355, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,836 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3823, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,836 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1516, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,837 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,837 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,837 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,837 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,837 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,839 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:37511'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,840 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:36647'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,840 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1117, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,840 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:34721'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,841 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 5769, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,841 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:39211'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,841 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,841 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,841 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3012, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,841 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:35157'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,841 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,841 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 5178, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,842 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,842 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 8722, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,842 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,842 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3900, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,842 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 8662, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,842 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,842 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,842 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.14:35961, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,842 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,843 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,843 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1735, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,843 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1848, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,843 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,843 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,843 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,843 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1850, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,843 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,843 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,843 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,843 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,843 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,843 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,843 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,843 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,843 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,844 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,844 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,844 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,844 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,851 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:40483'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,852 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:34981'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,852 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:33455'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,853 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:43287'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,853 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:42545'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,853 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 8367, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,853 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:37367'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,853 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2901, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,854 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:37351'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,854 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2301, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,854 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:45701'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,854 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2224, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,854 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.14:33589, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,854 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2312, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,854 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 5258, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,854 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:44851'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,854 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,855 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.16:36957, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,855 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,855 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:38921'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,855 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3014, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,855 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,855 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,855 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:42825'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,855 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,855 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1798, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,855 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,855 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,855 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,855 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,855 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,855 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.21:44151, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,855 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:34191'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,855 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,856 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,856 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 5193, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,856 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:36119'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,856 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1593, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,856 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,856 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2711, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,856 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.35:43085, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,856 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,856 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:34929'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,856 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2714, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,856 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,856 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 8651, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,856 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:34329'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,856 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 8366, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,856 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,856 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1752, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,856 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:44953'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,857 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3791, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,857 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,857 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3344, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,857 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:33747'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,857 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,857 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 8568, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,857 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1866, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,857 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2780, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,857 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,857 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,857 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,857 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:46027'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,857 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1731, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,857 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1600, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,857 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,857 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,857 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:34263'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,857 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,857 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,857 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,858 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1800, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,858 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:46365'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,858 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1940, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,858 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 8210, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,858 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2657, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,858 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:43565'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,858 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2968, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,858 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3691, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,858 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.35:39481, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,858 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:46415'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,858 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,859 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,858 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1873, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,859 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,859 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,859 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:37455'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,859 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2316, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,859 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,859 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1338, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,859 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,859 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,859 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,859 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3862, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,859 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:39383'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:34,859 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2317, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,859 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1517, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,859 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,859 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,859 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,859 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,859 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,859 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,859 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,859 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.8:38701, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,859 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,859 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,859 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3740, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,860 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,860 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,860 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,860 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,860 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,860 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,860 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2770, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,860 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,860 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1867, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,860 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,860 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,860 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,860 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,860 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,860 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2067, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,860 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,860 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1374, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,860 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,860 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,860 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1732, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,861 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,861 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,861 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,861 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,861 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.2:39947, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,861 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,861 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,861 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1734, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,861 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,861 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,861 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,861 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.20:34249, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,861 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,861 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3368, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,861 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,861 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,861 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1520, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,861 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,861 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,861 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1733, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,861 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,862 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,861 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 1797, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,862 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,862 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,862 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,862 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,862 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.8:41823, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,862 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,862 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.6:38971, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,862 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,862 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,862 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,863 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2022, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,863 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2021, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:34,863 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,864 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,865 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,865 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:34,871 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:34,872 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:34,872 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:34,872 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:34,872 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:35,373 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:35,376 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:33099. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,394 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:35,400 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.14:48832 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:35,405 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:46499'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,406 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:35,406 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:35,407 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:35,407 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:35,407 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:35,415 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x152de8567990>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:35,424 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:35,444 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:35,447 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:36771. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,463 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:35,468 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.14:48908 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:35,473 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:44883'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:35,474 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:35,474 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:35,474 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:35,474 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:35,474 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:35,481 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14556981b810>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:35,488 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:35,543 - distributed.worker - ERROR - failed during get data with tcp://10.6.102.14:42405 -> tcp://10.6.102.11:34465
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.14:42405 remote=tcp://10.6.102.11:47994>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-03 16:42:35,729 - distributed.worker - ERROR - failed during get data with tcp://10.6.102.14:39871 -> tcp://10.6.102.18:35227
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.14:39871 remote=tcp://10.6.102.18:46066>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-03 16:42:36,020 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:36,023 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:40083. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,024 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:36,026 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:35961. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,038 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:36,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:36,044 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.14:48516 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:36,046 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.14:48942 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:36,048 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:43245'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,050 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:36,050 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:36,050 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:36,050 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:36,050 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:36,053 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:41097'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,054 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:36,054 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:36,054 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:36,054 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:36,054 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:36,058 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x150aa0870b10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:36,062 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x155367f4cb50>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:36,066 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,070 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,149 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,221 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,606 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:36,607 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:44281. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,606 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:36,607 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:39263. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,623 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:36,623 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:36,624 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:33483. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,625 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:36,629 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.14:48586 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:36,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:36,631 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.14:48686 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:36,632 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:36,634 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:43905. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,635 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.14:48492 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.102.14:48492 remote=tcp://10.6.102.1:8786>: Stream is closed
2025-09-03 16:42:36,642 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:33165'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,642 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:46335'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,642 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:38413'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,642 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:36,642 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:36,642 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:36,642 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:36,643 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:36,643 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:36,643 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.20:34249, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:36,644 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:36,644 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:36,644 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:36,644 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:36,644 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:36,645 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.25:40747, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:36,646 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:36,646 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:36,646 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:36,646 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:36,646 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:36,647 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.14:48724 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.102.14:48724 remote=tcp://10.6.102.1:8786>: Stream is closed
2025-09-03 16:42:36,652 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:45233'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,652 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:36,653 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:36,653 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:36,653 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:36,653 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:36,652 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x148cc2c44250>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:36,654 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14f71ca35710>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:36,657 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,659 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,660 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14f179ba8050>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:36,664 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,668 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,792 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:36,792 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:46509. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,798 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:36,800 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.14:48642 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:36,803 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:45021'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,806 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 8473, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:36,806 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 8356, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:36,807 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.18:35761, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:36,807 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.2:35915, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:36,807 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:36,807 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:36,807 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:36,807 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:36,807 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:36,811 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x154c2932ffd0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:36,885 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:36,886 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:33567. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,903 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:36,908 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.14:48658 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:36,912 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:43697'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:36,917 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.24:43297, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:36,917 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:36,917 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:36,918 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:36,918 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:36,918 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:36,923 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x154f398d24d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:36,929 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:36,987 - distributed.worker - ERROR - failed during get data with tcp://10.6.102.14:37115 -> tcp://10.6.102.16:32819
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.14:37115 remote=tcp://10.6.102.16:45348>: Stream is closed
2025-09-03 16:42:37,002 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:37,005 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:34765. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,021 - distributed.worker - ERROR - failed during get data with tcp://10.6.102.14:34765 -> tcp://10.6.102.21:46725
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.14:34765 remote=tcp://10.6.102.21:49448>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-03 16:42:37,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:37,031 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,033 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.14:48966 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:37,035 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:41769'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,036 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:37,036 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:37,036 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:37,037 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:37,037 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:37,041 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14fdede7d010>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:37,047 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,057 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,149 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:37,152 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:41789. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,167 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,172 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:37,177 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.14:48820 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:37,181 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:38667'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:37,181 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:37,182 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:37,182 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:37,182 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:37,182 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:37,187 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x145c61c60910>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:37,194 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,238 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,427 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:37,492 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:37,568 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,787 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,823 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:37,922 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:46499'. Reason: nanny-close-gracefully
2025-09-03 16:42:37,927 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:46499' closed.
2025-09-03 16:42:37,939 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:44883'. Reason: nanny-close-gracefully
2025-09-03 16:42:37,940 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:44883' closed.
2025-09-03 16:42:38,061 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:38,064 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:37879. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:38,069 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,073 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,085 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:38,089 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.14:48900 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:38,097 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:46165'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:38,098 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:38,099 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:38,099 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:38,099 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:38,099 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:38,103 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1526886048d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:38,109 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,119 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:38,120 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:36709. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:38,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:38,145 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.14:48588 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:38,149 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:38055'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:38,149 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:38,149 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:38,149 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:38,149 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:38,149 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:38,153 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,157 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,224 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,233 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,249 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:38,249 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:40221. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:38,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:38,260 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.14:48506 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:38,263 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:33025'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:38,263 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 3667, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:38,264 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:38,265 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:38,265 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:38,265 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:38,265 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:38,267 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1530743fa750>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:38,272 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,492 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:43245'. Reason: nanny-close-gracefully
2025-09-03 16:42:38,494 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:43245' closed.
2025-09-03 16:42:38,523 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:41097'. Reason: nanny-close-gracefully
2025-09-03 16:42:38,524 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:41097' closed.
2025-09-03 16:42:38,529 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,568 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:38,571 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:42541. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:38,593 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:38,598 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:33745. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:38,597 - distributed.worker - ERROR - failed during get data with tcp://10.6.102.14:42541 -> tcp://10.6.102.22:34653
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.14:42541 remote=tcp://10.6.102.22:51682>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-03 16:42:38,600 - distributed.worker - ERROR - failed during get data with tcp://10.6.102.14:33745 -> tcp://10.6.102.17:46019
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.14:33745 remote=tcp://10.6.102.17:59984>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-03 16:42:38,606 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:38,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:38,609 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.14:48634 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.102.14:48634 remote=tcp://10.6.102.1:8786>: Stream is closed
2025-09-03 16:42:38,611 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.14:48522 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:38,618 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:45701'. Reason: nanny-close-gracefully
2025-09-03 16:42:38,621 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:40253'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:38,621 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:41385'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:38,622 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.22:46847, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:38,622 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.22:36537, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:38,622 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:38,622 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.102.21:33739, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:38,622 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-664014352b01c2eea80f3f701e468b9a', 2966, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-03 16:42:38,622 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:38,622 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:38,622 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:38,622 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:38,622 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:38,622 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:38,622 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:38,622 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:38,622 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:38,622 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:45701' closed.
2025-09-03 16:42:38,626 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x154c205baa10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:38,626 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14f079586950>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:38,633 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,636 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:35157'. Reason: nanny-close-gracefully
2025-09-03 16:42:38,637 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:35157' closed.
2025-09-03 16:42:38,662 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,661 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:38,663 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:39565. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:38,666 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,672 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,683 - distributed.worker - ERROR - failed during get data with tcp://10.6.102.14:39565 -> tcp://10.6.102.15:44621
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.102.14:39565 remote=tcp://10.6.102.15:37252>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-03 16:42:38,691 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:38,694 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.14:48730 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:38,697 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:39817'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:38,698 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:38,698 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:38,698 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:38,698 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:38,698 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:38,702 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:38,707 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,824 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,836 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,912 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:38,931 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:38,967 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,007 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,037 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,050 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,061 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,083 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:46335'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,084 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:46335' closed.
2025-09-03 16:42:39,173 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,181 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:33165'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,183 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:33165' closed.
2025-09-03 16:42:39,197 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,197 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,226 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:38413'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,227 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:38413' closed.
2025-09-03 16:42:39,243 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,270 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,270 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,270 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,270 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,284 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,306 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:39,307 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:37235. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:39,328 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:39,333 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.14:48528 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:39,336 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:44103'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:39,336 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:39,336 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:39,337 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:39,337 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:39,337 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:39,339 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x154823149ed0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:39,344 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,354 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:45233'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,356 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:45233' closed.
2025-09-03 16:42:39,392 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,393 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,448 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,465 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,469 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:43697'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,469 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,470 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:43697' closed.
2025-09-03 16:42:39,482 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:39,483 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:42845. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:39,487 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.102.18:33903
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.102.14:40172 remote=tcp://10.6.102.18:33903>: Stream is closed
2025-09-03 16:42:39,491 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.102.16:43901
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 228, in read
    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 367, in read_bytes_rw
    actual = await stream.read_into(chunk)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.102.14:55230 remote=tcp://10.6.102.16:43901>: Stream is closed
2025-09-03 16:42:39,494 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:39,498 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,497 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.14:48508 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.102.14:48508 remote=tcp://10.6.102.1:8786>: Stream is closed
2025-09-03 16:42:39,500 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:43547'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:39,501 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:39,501 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:39,501 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:39,501 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:39,501 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:39,503 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x146325418290>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:39,508 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,528 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:39,529 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:43399. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:39,547 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.102.16:43155
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.102.14:52932 remote=tcp://10.6.102.16:43155>: Stream is closed
2025-09-03 16:42:39,550 - distributed.core - INFO - Connection to tcp://10.6.102.1:8786 has been closed.
2025-09-03 16:42:39,550 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.14:33589. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:39,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:39,557 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.14:48754 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:39,560 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-03 16:42:39,560 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:43617'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:39,561 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:39,561 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:39,561 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:39,561 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:39,561 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:39,563 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.102.14:48672 remote=tcp://10.6.102.1:8786>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-03 16:42:39,564 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:39,565 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.14:41667'. Reason: worker-handle-scheduler-connection-broken
2025-09-03 16:42:39,566 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-03 16:42:39,566 - distributed.worker - INFO - Removing Worker plugin qme_utils.py1d5c8fe7-a714-4c02-b47d-37cc6d7f0848
2025-09-03 16:42:39,566 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye7f0b07d-9c67-4b5f-926b-4dff9694ebc5
2025-09-03 16:42:39,566 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb56dbd10-95ab-4a3b-9925-4131dd74b838
2025-09-03 16:42:39,566 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyd1c7ce75-6e80-44d8-9cff-2674c5c3b113
2025-09-03 16:42:39,568 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,568 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,568 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14dc7813dd50>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-03 16:42:39,570 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,571 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,586 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:45021'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,587 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,587 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:45021' closed.
2025-09-03 16:42:39,624 - distributed.nanny - INFO - Worker closed
2025-09-03 16:42:39,631 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:40483'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,632 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:40483' closed.
2025-09-03 16:42:39,648 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:34981'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,648 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:34981' closed.
2025-09-03 16:42:39,655 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:41769'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,656 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:41769' closed.
2025-09-03 16:42:39,695 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:38921'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,696 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:38921' closed.
2025-09-03 16:42:39,734 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:38667'. Reason: nanny-close-gracefully
2025-09-03 16:42:39,736 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:38667' closed.
2025-09-03 16:42:39,791 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:39,827 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,007 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:34191'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,008 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:34191' closed.
2025-09-03 16:42:40,112 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,159 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,237 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,241 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:37455'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,242 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:37455' closed.
2025-09-03 16:42:40,271 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:34263'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,272 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:34263' closed.
2025-09-03 16:42:40,274 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,533 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,551 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:46165'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,553 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:46165' closed.
2025-09-03 16:42:40,609 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:38055'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,610 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:38055' closed.
2025-09-03 16:42:40,636 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,689 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:38393'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,690 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:38393' closed.
2025-09-03 16:42:40,696 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:33025'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,698 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:33025' closed.
2025-09-03 16:42:40,710 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,828 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,840 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,918 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,971 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:40,981 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:46365'. Reason: nanny-close-gracefully
2025-09-03 16:42:40,982 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:46365' closed.
2025-09-03 16:42:41,010 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,106 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:41385'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,108 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:41385' closed.
2025-09-03 16:42:41,156 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:39817'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,157 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:39817' closed.
2025-09-03 16:42:41,274 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,274 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,274 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,274 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,287 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,347 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,390 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:42545'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,391 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:42545' closed.
2025-09-03 16:42:41,396 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,397 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,399 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:46415'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,400 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:46415' closed.
2025-09-03 16:42:41,412 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:34721'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,413 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:34721' closed.
2025-09-03 16:42:41,416 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:37351'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,417 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:37351' closed.
2025-09-03 16:42:41,422 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:43287'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,423 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:43287' closed.
2025-09-03 16:42:41,451 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,468 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,473 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,511 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,570 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,572 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,573 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,574 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,590 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,629 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-03 16:42:41,698 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:42825'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,699 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:42825' closed.
2025-09-03 16:42:41,772 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:44953'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,773 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:44953' closed.
2025-09-03 16:42:41,786 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:33455'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,787 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:33455' closed.
2025-09-03 16:42:41,810 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:44103'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,811 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:44103' closed.
2025-09-03 16:42:41,817 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:44851'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,818 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:44851' closed.
2025-09-03 16:42:41,825 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:33747'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,826 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:33747' closed.
2025-09-03 16:42:41,890 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:39383'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,892 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:36647'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,892 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:39383' closed.
2025-09-03 16:42:41,893 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:36647' closed.
2025-09-03 16:42:41,943 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:37367'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,945 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:37367' closed.
2025-09-03 16:42:41,984 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:39211'. Reason: nanny-close-gracefully
2025-09-03 16:42:41,985 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:39211' closed.
2025-09-03 16:42:42,011 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:43547'. Reason: nanny-close-gracefully
2025-09-03 16:42:42,012 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:43547' closed.
2025-09-03 16:42:42,017 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:46027'. Reason: nanny-close-gracefully
2025-09-03 16:42:42,018 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:46027' closed.
2025-09-03 16:42:42,039 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:40253'. Reason: nanny-close-gracefully
2025-09-03 16:42:42,039 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:40253' closed.
2025-09-03 16:42:42,058 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:34929'. Reason: nanny-close-gracefully
2025-09-03 16:42:42,060 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:37511'. Reason: nanny-close-gracefully
2025-09-03 16:42:42,060 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:34929' closed.
2025-09-03 16:42:42,061 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:37511' closed.
2025-09-03 16:42:42,129 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:43617'. Reason: nanny-close-gracefully
2025-09-03 16:42:42,130 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:43617' closed.
2025-09-03 16:42:42,132 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:41667'. Reason: nanny-close-gracefully
2025-09-03 16:42:42,132 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:41667' closed.
2025-09-03 16:42:42,141 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:34329'. Reason: nanny-close-gracefully
2025-09-03 16:42:42,142 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:34329' closed.
2025-09-03 16:42:42,190 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:36119'. Reason: nanny-close-gracefully
2025-09-03 16:42:42,191 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:36119' closed.
2025-09-03 16:42:42,282 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.14:43565'. Reason: nanny-close-gracefully
2025-09-03 16:42:42,283 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.14:43565' closed.
2025-09-03 16:42:42,286 - distributed.dask_worker - INFO - End worker
