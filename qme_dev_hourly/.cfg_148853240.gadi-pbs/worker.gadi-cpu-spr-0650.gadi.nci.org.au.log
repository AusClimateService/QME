Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-05 09:25:11,712 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:34745'
2025-09-05 09:25:11,721 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:45821'
2025-09-05 09:25:11,725 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:43289'
2025-09-05 09:25:11,728 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:39823'
2025-09-05 09:25:11,733 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:42309'
2025-09-05 09:25:11,736 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:46183'
2025-09-05 09:25:11,741 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:39677'
2025-09-05 09:25:11,744 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:41881'
2025-09-05 09:25:11,749 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:40857'
2025-09-05 09:25:11,754 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:38693'
2025-09-05 09:25:11,768 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:43727'
2025-09-05 09:25:11,771 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:38349'
2025-09-05 09:25:11,775 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:37059'
2025-09-05 09:25:11,779 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:40947'
2025-09-05 09:25:11,783 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:45031'
2025-09-05 09:25:11,788 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:36279'
2025-09-05 09:25:11,792 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:39577'
2025-09-05 09:25:11,796 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:38941'
2025-09-05 09:25:11,801 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:35307'
2025-09-05 09:25:11,805 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:45955'
2025-09-05 09:25:11,891 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:45893'
2025-09-05 09:25:11,895 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:46693'
2025-09-05 09:25:11,900 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:46727'
2025-09-05 09:25:11,905 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:38715'
2025-09-05 09:25:11,909 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:44373'
2025-09-05 09:25:11,913 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:36391'
2025-09-05 09:25:11,918 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:43081'
2025-09-05 09:25:11,923 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:33249'
2025-09-05 09:25:11,928 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:45871'
2025-09-05 09:25:11,932 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:42101'
2025-09-05 09:25:11,936 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:45217'
2025-09-05 09:25:11,940 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:38389'
2025-09-05 09:25:11,945 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:39607'
2025-09-05 09:25:11,949 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:34449'
2025-09-05 09:25:11,953 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:43561'
2025-09-05 09:25:11,958 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:43771'
2025-09-05 09:25:11,962 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:36449'
2025-09-05 09:25:11,966 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:35699'
2025-09-05 09:25:11,971 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:45795'
2025-09-05 09:25:11,975 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:45347'
2025-09-05 09:25:11,979 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:34531'
2025-09-05 09:25:11,984 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:38033'
2025-09-05 09:25:11,988 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:35203'
2025-09-05 09:25:11,993 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:39039'
2025-09-05 09:25:11,998 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:46685'
2025-09-05 09:25:12,002 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:40223'
2025-09-05 09:25:12,007 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:42665'
2025-09-05 09:25:12,011 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:39317'
2025-09-05 09:25:12,015 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:43627'
2025-09-05 09:25:12,019 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:42495'
2025-09-05 09:25:12,025 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:37985'
2025-09-05 09:25:12,031 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.2:40205'
2025-09-05 09:25:13,194 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:37565
2025-09-05 09:25:13,194 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:39623
2025-09-05 09:25:13,194 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:45285
2025-09-05 09:25:13,194 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:46601
2025-09-05 09:25:13,194 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:46815
2025-09-05 09:25:13,194 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:40225
2025-09-05 09:25:13,194 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:43961
2025-09-05 09:25:13,194 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:37565
2025-09-05 09:25:13,194 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:44723
2025-09-05 09:25:13,194 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:38023
2025-09-05 09:25:13,194 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:37523
2025-09-05 09:25:13,194 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:39623
2025-09-05 09:25:13,194 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:36513
2025-09-05 09:25:13,194 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:45285
2025-09-05 09:25:13,194 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:34471
2025-09-05 09:25:13,194 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:46601
2025-09-05 09:25:13,194 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:42627
2025-09-05 09:25:13,194 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:46815
2025-09-05 09:25:13,194 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:40225
2025-09-05 09:25:13,194 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:43961
2025-09-05 09:25:13,194 - distributed.worker - INFO -          dashboard at:           10.6.105.2:45515
2025-09-05 09:25:13,194 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:44723
2025-09-05 09:25:13,195 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:38023
2025-09-05 09:25:13,195 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:37523
2025-09-05 09:25:13,195 - distributed.worker - INFO -          dashboard at:           10.6.105.2:42565
2025-09-05 09:25:13,195 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:36513
2025-09-05 09:25:13,195 - distributed.worker - INFO -          dashboard at:           10.6.105.2:41669
2025-09-05 09:25:13,195 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:34471
2025-09-05 09:25:13,195 - distributed.worker - INFO -          dashboard at:           10.6.105.2:42643
2025-09-05 09:25:13,195 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:42627
2025-09-05 09:25:13,195 - distributed.worker - INFO -          dashboard at:           10.6.105.2:43425
2025-09-05 09:25:13,195 - distributed.worker - INFO -          dashboard at:           10.6.105.2:41361
2025-09-05 09:25:13,195 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,195 - distributed.worker - INFO -          dashboard at:           10.6.105.2:43127
2025-09-05 09:25:13,195 - distributed.worker - INFO -          dashboard at:           10.6.105.2:43165
2025-09-05 09:25:13,195 - distributed.worker - INFO -          dashboard at:           10.6.105.2:34603
2025-09-05 09:25:13,195 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,195 - distributed.worker - INFO -          dashboard at:           10.6.105.2:40497
2025-09-05 09:25:13,195 - distributed.worker - INFO -          dashboard at:           10.6.105.2:42353
2025-09-05 09:25:13,195 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,195 - distributed.worker - INFO -          dashboard at:           10.6.105.2:34309
2025-09-05 09:25:13,195 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,195 - distributed.worker - INFO -          dashboard at:           10.6.105.2:44451
2025-09-05 09:25:13,195 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,195 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,195 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,195 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,195 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,195 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,195 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,195 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,195 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,195 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,195 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,195 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,195 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,195 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,195 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,195 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,195 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,195 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,195 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,195 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,195 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,195 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,195 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,195 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,195 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,195 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,195 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,195 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,195 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-ofal6gvb
2025-09-05 09:25:13,195 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,195 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,195 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,195 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-kq1r0_7l
2025-09-05 09:25:13,195 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,195 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,195 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-hv0fnlo3
2025-09-05 09:25:13,195 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,195 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-0368mpau
2025-09-05 09:25:13,195 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,195 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-2yk1jdpn
2025-09-05 09:25:13,195 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-yo58zn63
2025-09-05 09:25:13,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,195 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-4469_dp8
2025-09-05 09:25:13,195 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-8k3e95pp
2025-09-05 09:25:13,195 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-qp79vvnh
2025-09-05 09:25:13,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,195 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-5bl40kqp
2025-09-05 09:25:13,195 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-jzg9pwk4
2025-09-05 09:25:13,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,195 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-ohqq4ffg
2025-09-05 09:25:13,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,195 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-rikr7h97
2025-09-05 09:25:13,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,202 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:41451
2025-09-05 09:25:13,202 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:41451
2025-09-05 09:25:13,202 - distributed.worker - INFO -          dashboard at:           10.6.105.2:46597
2025-09-05 09:25:13,202 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,202 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,202 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,202 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,202 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:38729
2025-09-05 09:25:13,202 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-5gsfckrz
2025-09-05 09:25:13,202 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:38729
2025-09-05 09:25:13,202 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,202 - distributed.worker - INFO -          dashboard at:           10.6.105.2:37373
2025-09-05 09:25:13,202 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,202 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,202 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,202 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,202 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-po1oun3c
2025-09-05 09:25:13,202 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,203 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:43129
2025-09-05 09:25:13,203 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:43129
2025-09-05 09:25:13,203 - distributed.worker - INFO -          dashboard at:           10.6.105.2:36869
2025-09-05 09:25:13,203 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,203 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,203 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,203 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,203 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-2_68rtvj
2025-09-05 09:25:13,203 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,204 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:45155
2025-09-05 09:25:13,204 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:45155
2025-09-05 09:25:13,204 - distributed.worker - INFO -          dashboard at:           10.6.105.2:35199
2025-09-05 09:25:13,204 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,204 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,204 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,204 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,204 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-_xyq9tsy
2025-09-05 09:25:13,204 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,205 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:40949
2025-09-05 09:25:13,205 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:40949
2025-09-05 09:25:13,205 - distributed.worker - INFO -          dashboard at:           10.6.105.2:39879
2025-09-05 09:25:13,205 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,205 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,206 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,206 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,206 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-70zy930t
2025-09-05 09:25:13,206 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,206 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:33973
2025-09-05 09:25:13,206 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:33973
2025-09-05 09:25:13,206 - distributed.worker - INFO -          dashboard at:           10.6.105.2:43401
2025-09-05 09:25:13,207 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,207 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,207 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,207 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,207 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-08d2_0pe
2025-09-05 09:25:13,207 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,216 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,217 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,217 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,217 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,218 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,218 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,218 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,219 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,220 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,220 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,220 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,221 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,221 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,222 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,222 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,223 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,224 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,224 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,224 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,225 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,226 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,226 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,226 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,227 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,228 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,228 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,229 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,229 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,229 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,229 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,230 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,230 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,230 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,231 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,231 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,231 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,232 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,232 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,233 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,233 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,233 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,233 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,233 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,234 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,234 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,234 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,234 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,235 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,235 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,235 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,235 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,236 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,236 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,236 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,236 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,237 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,237 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,237 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,237 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,238 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,237 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,238 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,238 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,238 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,239 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,239 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,239 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,240 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,240 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,240 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,240 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,241 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,241 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:45949
2025-09-05 09:25:13,241 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:45949
2025-09-05 09:25:13,241 - distributed.worker - INFO -          dashboard at:           10.6.105.2:45435
2025-09-05 09:25:13,241 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,241 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,241 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,241 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,241 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-mynf16c2
2025-09-05 09:25:13,241 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,241 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,242 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,242 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,243 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,243 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:38025
2025-09-05 09:25:13,244 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:38025
2025-09-05 09:25:13,244 - distributed.worker - INFO -          dashboard at:           10.6.105.2:37135
2025-09-05 09:25:13,244 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,244 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,244 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,244 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,244 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-58ibwx9_
2025-09-05 09:25:13,244 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,250 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:42261
2025-09-05 09:25:13,250 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:42261
2025-09-05 09:25:13,250 - distributed.worker - INFO -          dashboard at:           10.6.105.2:34057
2025-09-05 09:25:13,250 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,250 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,250 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,250 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,250 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-9v1z_6a5
2025-09-05 09:25:13,250 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,254 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:44449
2025-09-05 09:25:13,254 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:44449
2025-09-05 09:25:13,254 - distributed.worker - INFO -          dashboard at:           10.6.105.2:46669
2025-09-05 09:25:13,254 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,254 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,254 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,254 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,254 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-8lgenmsi
2025-09-05 09:25:13,255 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,261 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,261 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:45459
2025-09-05 09:25:13,261 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:45459
2025-09-05 09:25:13,261 - distributed.worker - INFO -          dashboard at:           10.6.105.2:43795
2025-09-05 09:25:13,261 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,261 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,261 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,261 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,261 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-9mq9zbmh
2025-09-05 09:25:13,261 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,262 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,262 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,262 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,263 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,263 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,263 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:39693
2025-09-05 09:25:13,263 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,264 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:39693
2025-09-05 09:25:13,264 - distributed.worker - INFO -          dashboard at:           10.6.105.2:45813
2025-09-05 09:25:13,264 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,264 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,264 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,264 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,264 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-9w4qi6oq
2025-09-05 09:25:13,264 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,265 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,265 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,265 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,265 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,267 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,269 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,270 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,270 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,270 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,273 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,274 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,274 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,276 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,282 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:43893
2025-09-05 09:25:13,283 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:43893
2025-09-05 09:25:13,283 - distributed.worker - INFO -          dashboard at:           10.6.105.2:45223
2025-09-05 09:25:13,283 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,283 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,283 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,283 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,283 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-_wqhj0u7
2025-09-05 09:25:13,283 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,284 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,285 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,285 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,287 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,294 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:34533
2025-09-05 09:25:13,294 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:34533
2025-09-05 09:25:13,294 - distributed.worker - INFO -          dashboard at:           10.6.105.2:39641
2025-09-05 09:25:13,294 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,294 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,294 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,294 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,294 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-qm5jvyjy
2025-09-05 09:25:13,294 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,298 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:42855
2025-09-05 09:25:13,298 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:42855
2025-09-05 09:25:13,298 - distributed.worker - INFO -          dashboard at:           10.6.105.2:36641
2025-09-05 09:25:13,298 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,299 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,299 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,299 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,299 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-ipr50d2u
2025-09-05 09:25:13,299 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,300 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:45335
2025-09-05 09:25:13,300 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:45335
2025-09-05 09:25:13,300 - distributed.worker - INFO -          dashboard at:           10.6.105.2:41857
2025-09-05 09:25:13,300 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,300 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,300 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,300 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,300 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-470jp5bz
2025-09-05 09:25:13,300 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,304 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,305 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,305 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,306 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,307 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:42357
2025-09-05 09:25:13,307 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:42357
2025-09-05 09:25:13,307 - distributed.worker - INFO -          dashboard at:           10.6.105.2:38119
2025-09-05 09:25:13,307 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,307 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,307 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,307 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,307 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-fj551lxl
2025-09-05 09:25:13,307 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,308 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:45991
2025-09-05 09:25:13,308 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:45991
2025-09-05 09:25:13,308 - distributed.worker - INFO -          dashboard at:           10.6.105.2:43803
2025-09-05 09:25:13,308 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,308 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,308 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,308 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-cbawyr99
2025-09-05 09:25:13,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,311 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,311 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,311 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,311 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,314 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,315 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,316 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,317 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,317 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,317 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,318 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,320 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,320 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,320 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,321 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,324 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,325 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,325 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,326 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,327 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:38695
2025-09-05 09:25:13,327 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:38695
2025-09-05 09:25:13,327 - distributed.worker - INFO -          dashboard at:           10.6.105.2:33125
2025-09-05 09:25:13,327 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,327 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,327 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,327 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,327 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-px9bih82
2025-09-05 09:25:13,327 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,347 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,347 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,348 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,349 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,353 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:45147
2025-09-05 09:25:13,354 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:45147
2025-09-05 09:25:13,354 - distributed.worker - INFO -          dashboard at:           10.6.105.2:36993
2025-09-05 09:25:13,354 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,354 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,354 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,354 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,354 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-m288nli6
2025-09-05 09:25:13,354 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,357 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:43827
2025-09-05 09:25:13,357 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:43827
2025-09-05 09:25:13,357 - distributed.worker - INFO -          dashboard at:           10.6.105.2:38593
2025-09-05 09:25:13,357 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,357 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,357 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,357 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,357 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-yqu4gbti
2025-09-05 09:25:13,357 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,375 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,376 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:43809
2025-09-05 09:25:13,376 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:43809
2025-09-05 09:25:13,376 - distributed.worker - INFO -          dashboard at:           10.6.105.2:44367
2025-09-05 09:25:13,376 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,376 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,376 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,376 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,376 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-7jp8k22d
2025-09-05 09:25:13,376 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,376 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,376 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,377 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,377 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,378 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,378 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,379 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,394 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,395 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,395 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,396 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,435 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:40823
2025-09-05 09:25:13,435 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:40823
2025-09-05 09:25:13,435 - distributed.worker - INFO -          dashboard at:           10.6.105.2:36539
2025-09-05 09:25:13,435 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,435 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,435 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,435 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,435 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-iz5n_j0p
2025-09-05 09:25:13,435 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,438 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:42757
2025-09-05 09:25:13,438 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:42757
2025-09-05 09:25:13,438 - distributed.worker - INFO -          dashboard at:           10.6.105.2:43395
2025-09-05 09:25:13,438 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,438 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,438 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,438 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,438 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-eq4pq84x
2025-09-05 09:25:13,438 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,457 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,458 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,458 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,459 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,459 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,460 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,460 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,461 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,585 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:39309
2025-09-05 09:25:13,585 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:39309
2025-09-05 09:25:13,585 - distributed.worker - INFO -          dashboard at:           10.6.105.2:35403
2025-09-05 09:25:13,585 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,585 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,585 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,585 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,585 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-93qgsn7f
2025-09-05 09:25:13,585 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,595 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:36291
2025-09-05 09:25:13,595 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:36291
2025-09-05 09:25:13,595 - distributed.worker - INFO -          dashboard at:           10.6.105.2:35205
2025-09-05 09:25:13,595 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,595 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,595 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,595 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,595 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-muql7g7p
2025-09-05 09:25:13,595 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,596 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:43805
2025-09-05 09:25:13,596 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:43805
2025-09-05 09:25:13,596 - distributed.worker - INFO -          dashboard at:           10.6.105.2:37437
2025-09-05 09:25:13,596 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,596 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,596 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,596 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,596 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-j_6_iutc
2025-09-05 09:25:13,596 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,600 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:35877
2025-09-05 09:25:13,600 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:35877
2025-09-05 09:25:13,600 - distributed.worker - INFO -          dashboard at:           10.6.105.2:46699
2025-09-05 09:25:13,600 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,600 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,600 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,600 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,600 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-n0qbwi7a
2025-09-05 09:25:13,600 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,605 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:37291
2025-09-05 09:25:13,605 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:37291
2025-09-05 09:25:13,605 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:33977
2025-09-05 09:25:13,605 - distributed.worker - INFO -          dashboard at:           10.6.105.2:44445
2025-09-05 09:25:13,605 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:33977
2025-09-05 09:25:13,605 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,605 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,605 - distributed.worker - INFO -          dashboard at:           10.6.105.2:41557
2025-09-05 09:25:13,605 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,605 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,605 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,605 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,605 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,605 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-k8xwqis6
2025-09-05 09:25:13,605 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,605 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,605 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-bp312d23
2025-09-05 09:25:13,605 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,605 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:37261
2025-09-05 09:25:13,605 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:37261
2025-09-05 09:25:13,605 - distributed.worker - INFO -          dashboard at:           10.6.105.2:35923
2025-09-05 09:25:13,605 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,605 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,605 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,605 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,605 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-zpv_ky24
2025-09-05 09:25:13,605 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,606 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,607 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,607 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,609 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,610 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:40281
2025-09-05 09:25:13,610 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:40281
2025-09-05 09:25:13,610 - distributed.worker - INFO -          dashboard at:           10.6.105.2:35957
2025-09-05 09:25:13,610 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,610 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,610 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,611 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,611 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-tzalu8_e
2025-09-05 09:25:13,611 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,613 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:33931
2025-09-05 09:25:13,613 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:33931
2025-09-05 09:25:13,613 - distributed.worker - INFO -          dashboard at:           10.6.105.2:36613
2025-09-05 09:25:13,613 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,613 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,613 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,613 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,613 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-i36b3jc6
2025-09-05 09:25:13,613 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,616 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,617 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,617 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,617 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:42685
2025-09-05 09:25:13,617 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:42685
2025-09-05 09:25:13,618 - distributed.worker - INFO -          dashboard at:           10.6.105.2:33995
2025-09-05 09:25:13,618 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,618 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,618 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,618 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,618 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-1g1dtd0s
2025-09-05 09:25:13,618 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,618 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,618 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,619 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,619 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,620 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,620 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,620 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,620 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,621 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,623 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,623 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,623 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,624 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:39139
2025-09-05 09:25:13,624 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:39139
2025-09-05 09:25:13,624 - distributed.worker - INFO -          dashboard at:           10.6.105.2:37365
2025-09-05 09:25:13,624 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,625 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,625 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,625 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,625 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-rg6fu8z_
2025-09-05 09:25:13,625 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,625 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,628 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,628 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:38719
2025-09-05 09:25:13,628 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:38719
2025-09-05 09:25:13,628 - distributed.worker - INFO -          dashboard at:           10.6.105.2:41469
2025-09-05 09:25:13,628 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,628 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,628 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,628 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,628 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-f11klpl3
2025-09-05 09:25:13,628 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,629 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,629 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,630 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,630 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,631 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,631 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,632 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:45731
2025-09-05 09:25:13,632 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,632 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:45731
2025-09-05 09:25:13,632 - distributed.worker - INFO -          dashboard at:           10.6.105.2:43577
2025-09-05 09:25:13,633 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,633 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,633 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,633 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,633 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:41879
2025-09-05 09:25:13,633 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-tu6kv1lw
2025-09-05 09:25:13,633 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:41879
2025-09-05 09:25:13,633 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,633 - distributed.worker - INFO -          dashboard at:           10.6.105.2:40569
2025-09-05 09:25:13,633 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,633 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,633 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,633 - distributed.worker - INFO -       Start worker at:     tcp://10.6.105.2:42331
2025-09-05 09:25:13,633 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,633 - distributed.worker - INFO -          Listening to:     tcp://10.6.105.2:42331
2025-09-05 09:25:13,633 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-gg4mzc55
2025-09-05 09:25:13,633 - distributed.worker - INFO -          dashboard at:           10.6.105.2:37529
2025-09-05 09:25:13,633 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,633 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,633 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,633 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:13,633 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:13,633 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-etz906xt
2025-09-05 09:25:13,633 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,633 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,634 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,634 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,635 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,635 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,636 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,636 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,636 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,636 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,637 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,637 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,638 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,644 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,645 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,645 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,645 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,649 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,649 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,650 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,650 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,651 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,651 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,651 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,652 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,653 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,654 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,655 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,655 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:13,656 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:13,656 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:13,656 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:13,657 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:50,926 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,926 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,927 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,927 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,927 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,927 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,927 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,928 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,927 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,927 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,928 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,928 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,928 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,928 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,928 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,928 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,928 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,929 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,929 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,929 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,929 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,929 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,929 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,929 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,930 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,930 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,930 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,930 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,930 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,930 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,930 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,930 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,930 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,930 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,930 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,930 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,930 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,930 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,931 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,931 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,931 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,931 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,930 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,931 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,931 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,931 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,931 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,931 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,931 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,931 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,932 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,932 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,932 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,932 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,932 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,932 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,932 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,930 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,932 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,932 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,932 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,932 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,933 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,931 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,931 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,933 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,933 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,934 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,934 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,934 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,932 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,934 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,934 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,932 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,934 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,934 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,935 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,935 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,933 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,934 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,934 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,936 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,934 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,935 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,935 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,935 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,936 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,936 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,938 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,941 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,941 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,941 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,942 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,943 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,943 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,944 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,944 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,945 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,945 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,945 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,945 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,945 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,946 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,947 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:53,675 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,675 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,675 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,675 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,675 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,675 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,675 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,676 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,676 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,676 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,676 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,676 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,676 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,676 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,677 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,677 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,676 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,677 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,677 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,677 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,678 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,678 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,678 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,678 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,678 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,677 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,678 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,677 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,678 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,677 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,678 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,678 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,677 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,678 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,678 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,678 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,678 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,678 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,678 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,677 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,678 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,677 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,678 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,679 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,678 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,678 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,679 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,678 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,679 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,679 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,679 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,679 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,679 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,679 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,679 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,679 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,679 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,679 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,680 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,679 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,679 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,680 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,679 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,680 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,680 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,680 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,680 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,680 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,680 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,679 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,680 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,680 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,680 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,680 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,680 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,680 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,681 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,681 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,681 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,681 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,680 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,679 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,681 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,682 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,682 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,682 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,682 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,682 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,682 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,682 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,682 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,682 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,683 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,683 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,683 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,683 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,683 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,681 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,683 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,683 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,684 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,684 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,684 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,685 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:54,061 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,061 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,061 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,061 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,061 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,061 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,061 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,062 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,062 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,062 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,062 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,062 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,062 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,063 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,063 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,063 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,063 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,063 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,063 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,063 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,063 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,063 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,063 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,063 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,064 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,064 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,064 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,064 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,064 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,064 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,064 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,064 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,064 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,064 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,065 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,065 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,065 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,065 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,065 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,065 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,065 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,065 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,065 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,065 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,065 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,066 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,066 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,066 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,066 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,066 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,066 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,066 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,066 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,066 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,066 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,066 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,066 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,066 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,067 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,067 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,067 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,067 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,067 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,067 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,067 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:54,067 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,067 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,067 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,067 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,068 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,068 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,068 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,068 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,068 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,068 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,069 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,069 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,069 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,069 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,069 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,069 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,069 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,069 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,069 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,069 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,069 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,069 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,070 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,070 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,070 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,070 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,070 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,070 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,070 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,070 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,070 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,071 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,071 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,071 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,071 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,071 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,072 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,072 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,079 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,435 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,435 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,435 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,435 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,435 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,436 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,436 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,436 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,436 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,436 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,436 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,436 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,436 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,436 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,436 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,437 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,437 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,437 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,437 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,437 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,437 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,437 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,437 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,437 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,437 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,437 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,437 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,437 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,438 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,438 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,438 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,438 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,438 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,438 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,438 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,438 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,438 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,438 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,438 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,438 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,438 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,438 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,439 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,439 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,439 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,439 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,439 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,439 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,439 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,439 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,439 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,439 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,439 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,439 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,439 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,439 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,439 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,439 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,439 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,439 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,439 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,440 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,440 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,440 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,440 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,440 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,440 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,440 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,440 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,440 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,440 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,440 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,440 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,440 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,440 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,440 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,440 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,441 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,441 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,441 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,441 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,441 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,441 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,441 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,441 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,441 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,442 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,442 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,442 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,442 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,442 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,442 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,442 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,443 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,443 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,443 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,443 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,443 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,443 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,444 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,444 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,444 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,444 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,444 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:33:01,926 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,926 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,926 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,929 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,007 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,011 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,011 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,011 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,012 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,012 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,014 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,017 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,021 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,022 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,023 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,033 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,033 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,037 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,039 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,049 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,055 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,072 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,073 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:02,073 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:47,289 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:48,926 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:49,039 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:52,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:52,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:52,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:52,749 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:53,234 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:56,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:56,585 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:57,021 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:57,025 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:57,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:57,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:57,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:57,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:57,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:57,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:57,785 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:57,786 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:57,788 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:57,853 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:58,256 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:58,575 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:58,953 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:58,954 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:58,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:58,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:58,954 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:58,956 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:59,194 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:59,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:59,787 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:59,787 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:59,788 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:00,047 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:00,585 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:01,287 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:01,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:01,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:05,643 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:05,701 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:07,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:07,560 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:07,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:11,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:12,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:12,475 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:55,585 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:01,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:01,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:01,345 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:01,561 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:01,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:01,718 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:01,920 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:01,921 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:01,925 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:02,148 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:02,151 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:02,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:02,671 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:02,797 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:02,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:03,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:03,073 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:03,073 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:03,080 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:03,244 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:03,318 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:03,443 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:03,445 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:03,450 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:03,639 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:05,051 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:05,055 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:05,059 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:06,574 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:06,742 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:07,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:07,049 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:07,198 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:07,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:08,163 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:08,461 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:08,463 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:08,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:08,580 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:09,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:09,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:09,231 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:09,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:09,312 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:09,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:11,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:11,864 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:12,384 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:13,370 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:13,371 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:14,849 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:14,855 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:15,674 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:15,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:17,021 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:17,207 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:17,965 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:18,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:18,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:19,105 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:19,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:19,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:19,446 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:19,454 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:19,939 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:21,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:22,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:22,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:22,727 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:22,729 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:22,731 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:23,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:23,299 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:23,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:23,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:23,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:24,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:24,190 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:25,134 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:26,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:31,162 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:31,859 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:32,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:33,328 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:35,798 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:35,802 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:35,870 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:36,085 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:36,085 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:37,138 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:37,255 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:37,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,097 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,456 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,457 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,522 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,806 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,937 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,939 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,961 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:39,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:39,289 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:39,317 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:39,434 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:39,451 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:39,533 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:39,790 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:40,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:40,196 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:40,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:43,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:43,461 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:43,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:43,639 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:44,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:44,843 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:44,844 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:44,846 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:44,922 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:45,120 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:45,208 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:45,402 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:45,874 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:45,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:45,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:45,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:46,207 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:46,207 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:46,420 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:46,598 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:46,598 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:46,861 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:46,862 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:46,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:47,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:48,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:48,369 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:49,816 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:49,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:49,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:53,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:53,728 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:56,102 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:00,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:00,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:07,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:07,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:07,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:07,248 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:08,033 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:08,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:08,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:08,494 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:08,498 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:08,756 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:08,768 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:08,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:08,916 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:08,997 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:11,059 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:12,441 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:12,716 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:13,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:13,550 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:14,952 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:15,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:16,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:16,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:17,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:18,299 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:18,301 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:18,303 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:18,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:18,907 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:18,939 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:19,187 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:19,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:20,508 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:20,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:20,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:20,981 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:21,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:22,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:25,175 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:25,329 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:25,795 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:25,800 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:26,081 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:26,082 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:26,098 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:26,542 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:26,824 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:27,172 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:27,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:27,439 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:27,440 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:27,963 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:27,963 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:27,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:27,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:27,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:29,085 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:29,750 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:29,788 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:29,788 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:29,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:31,709 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:31,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:34,010 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:34,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:34,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:35,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:40,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:40,785 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:40,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:40,919 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:41,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:42,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:42,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:42,386 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:42,387 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:42,634 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:45,815 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:48,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:48,685 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:48,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:48,851 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:49,004 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:49,138 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:49,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:49,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:49,209 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:49,441 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:49,494 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:49,748 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:49,752 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:50,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:50,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:50,672 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:50,748 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:50,903 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:50,907 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:51,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:51,484 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:51,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:51,587 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:51,589 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:51,934 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:51,937 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:51,939 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:51,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:52,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:52,036 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:52,159 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:52,161 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:14,378 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:14,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:14,380 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:14,380 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:14,380 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:14,383 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:14,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:14,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:15,004 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:15,106 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:15,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:15,389 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:15,415 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:15,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:15,600 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:15,602 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:15,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:15,636 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:15,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:15,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:15,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:15,733 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:15,736 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:15,770 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:15,995 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:16,628 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:16,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:18,573 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:20,098 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:20,356 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:20,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:21,630 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:21,636 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:22,950 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:23,421 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:24,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:27,212 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:27,363 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:30,426 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:32,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:34,225 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:34,756 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:35,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:35,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 25.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:37,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:40,052 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:40,243 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:45,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:47,794 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:47,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:48,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:49,308 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:49,612 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:50,352 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:50,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:50,357 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:51,046 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:51,624 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:52,190 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:52,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:52,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:53,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:53,156 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:53,158 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:53,946 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:54,421 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:56,887 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:56,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:57,073 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:57,247 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:57,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:58,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:58,128 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:58,513 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:58,513 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:58,604 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:58,606 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:01,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:01,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:01,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:02,981 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:04,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:04,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:04,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:04,319 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:04,356 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:04,428 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:04,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:05,182 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:05,267 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:05,296 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:05,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:05,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:09,884 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:10,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:10,621 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:10,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:10,899 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:10,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:11,753 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:12,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:14,359 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:14,513 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:15,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:15,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:15,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:15,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:17,323 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:17,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:18,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:18,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:18,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:18,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:18,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:20,950 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:20,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:21,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:21,391 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:21,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:21,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:21,757 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:21,761 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:21,762 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:21,813 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:21,903 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:22,010 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:22,591 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:22,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:22,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:22,608 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:23,166 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:27,251 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:27,782 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:27,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:27,921 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:27,959 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:28,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:28,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:28,309 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:28,336 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:28,336 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:28,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:28,385 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:28,443 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:28,445 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:28,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:28,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:29,188 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:29,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:29,636 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:30,161 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:31,612 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:32,656 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:32,657 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:32,661 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:33,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:33,765 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:33,798 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:34,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:34,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:34,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:34,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:36,464 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:36,465 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:38,307 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:38,310 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:39,096 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:39,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:39,298 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:40,085 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:40,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:49,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:51,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:54,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:54,889 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:55,749 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,411 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,682 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:57,053 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:57,055 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:57,073 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:57,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:57,312 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:57,384 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:57,387 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:57,410 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:57,469 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:57,472 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:57,473 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:58,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:58,149 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:58,463 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:58,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:59,288 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:59,289 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:59,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:59,297 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:00,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:01,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:01,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:01,445 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:01,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:01,450 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:01,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:02,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:05,871 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:07,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:07,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:07,668 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:07,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:08,255 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:08,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:10,480 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:10,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:10,483 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:10,805 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:10,813 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:10,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:11,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:11,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:11,655 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:12,895 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:12,910 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:12,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:14,265 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:14,265 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:15,017 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:15,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:15,023 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:17,602 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:17,604 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:17,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:18,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:18,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:18,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:18,276 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:19,801 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:21,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:22,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:30,868 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:30,871 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:31,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:32,223 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:32,224 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:32,224 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:32,225 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:32,225 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:32,319 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:32,322 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:32,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:33,720 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:33,721 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:34,337 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:34,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:34,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:34,343 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:34,512 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:34,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:34,768 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:35,098 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:35,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:35,317 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:35,318 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:35,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:35,768 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:36,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:36,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:36,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:36,066 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:36,755 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:37,313 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:37,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:38,163 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:38,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:38,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:38,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:39,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:39,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:40,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:40,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:42,739 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:43,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:43,755 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:49,723 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:50,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:50,328 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:51,268 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:51,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:51,712 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:52,211 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:52,386 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:52,387 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:52,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:52,389 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:52,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:53,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:54,362 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:54,365 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:54,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:54,832 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:54,833 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:57,036 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:57,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:57,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:58,309 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:02,477 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:02,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:02,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:02,881 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:03,766 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:03,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:04,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:04,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:04,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:05,135 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:05,140 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:05,449 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:07,621 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:07,622 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:09,460 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:15,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:16,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:18,085 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:18,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:18,088 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:18,792 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:18,801 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:19,293 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:19,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:20,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:20,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:20,440 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:20,658 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:20,921 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:20,922 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:21,460 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:21,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:21,658 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:21,973 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:21,973 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:21,977 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:22,273 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:22,361 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:22,651 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:23,156 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:23,395 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:23,588 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:24,038 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:24,038 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:24,120 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:24,129 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:24,450 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:25,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:25,738 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:25,739 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:25,874 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:26,156 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:26,157 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:26,159 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:26,369 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:29,148 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:29,149 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:29,157 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:29,217 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:29,218 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:31,063 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:31,550 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:31,551 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:31,558 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:32,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:32,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:32,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:32,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:32,231 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:32,231 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:32,234 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:32,252 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:32,453 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:32,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:33,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:33,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:33,495 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:33,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:34,084 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:34,256 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:36,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:36,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:37,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:37,233 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:38,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:38,506 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:38,526 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:38,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:38,745 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:38,900 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:39,690 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:40,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:40,149 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:40,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:40,208 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:40,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:41,329 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:41,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:41,967 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:42,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:42,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:42,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:42,446 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:42,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:42,505 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:42,624 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:42,842 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:42,846 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:43,064 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:43,139 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:45,888 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:46,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:47,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:47,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:47,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:47,926 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:48,179 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:48,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:48,190 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:48,729 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:49,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:49,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:49,571 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:49,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:49,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:50,487 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:51,253 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:51,739 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:51,807 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:52,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:52,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:52,420 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:52,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:53,120 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:53,122 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:53,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:53,348 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:00,510 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:00,513 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:00,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:00,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:01,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:02,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:03,021 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:03,023 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:03,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:03,562 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:09,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:09,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:09,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:09,439 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:09,468 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:09,469 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:09,792 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:10,633 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:11,035 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:11,037 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:11,038 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:11,040 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:11,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:11,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:12,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:12,071 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:12,073 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:12,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:13,022 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:13,028 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:13,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:13,518 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:13,524 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:13,526 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:13,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:13,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:14,096 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:14,097 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:14,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:17,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:17,481 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:22,588 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:23,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:27,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:27,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:28,855 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:28,861 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:28,861 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:28,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:29,101 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:29,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:29,158 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:29,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:29,365 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:29,589 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:29,744 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:29,744 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:29,745 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:29,747 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:30,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:30,468 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:30,470 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:30,472 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:30,674 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:30,830 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:31,398 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:32,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:32,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:34,859 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:34,937 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:34,940 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:34,943 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:35,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:35,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:36,518 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:40,307 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:40,308 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:42,983 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:42,986 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:42,987 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:42,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:47,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:47,837 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:47,905 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:48,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:48,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:48,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:48,764 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:49,057 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:49,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:49,672 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:50,345 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:50,661 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:50,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:50,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:53,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:56,373 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:56,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:56,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:56,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:56,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:59,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:59,835 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:01,088 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:01,108 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:01,371 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:02,686 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:02,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:03,349 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:07,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:08,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:08,898 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:08,902 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:09,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:10,156 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:10,158 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:10,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:10,271 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:10,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:10,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:11,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:11,360 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:11,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:11,905 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:11,908 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:11,908 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:12,972 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:12,975 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:12,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:14,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:14,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:14,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:14,627 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:14,629 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:14,954 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:15,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:15,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:15,345 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:15,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:15,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:16,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:16,758 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:16,759 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:16,761 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:16,762 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:16,762 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:17,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:19,550 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:20,305 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:24,381 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:24,383 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:24,913 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:24,969 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:25,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:25,540 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:26,205 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:26,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:26,575 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:26,935 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:26,940 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:27,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:27,171 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:27,173 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:27,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:27,762 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:27,764 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:27,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:27,865 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:27,870 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:28,864 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:28,869 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:29,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:29,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:29,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:30,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:31,036 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:31,758 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:33,586 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:33,657 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:36,942 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:37,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:37,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:38,033 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:38,586 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:45,306 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:45,434 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:45,656 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:45,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:45,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:46,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:46,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:47,735 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:48,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:48,636 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:48,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:48,641 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:48,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:48,678 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:48,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:50,485 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:50,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:50,558 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:50,559 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:51,165 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:51,178 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:51,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:52,187 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:52,188 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:52,279 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:52,280 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:52,642 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:53,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:53,247 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:53,948 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:54,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:54,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:54,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:54,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:54,309 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:54,367 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:54,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:54,397 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:54,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:55,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:55,354 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:55,501 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:55,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:56,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:56,995 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:59,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:59,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:59,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:59,714 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:00,025 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:00,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:00,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:00,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,475 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,478 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,622 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,687 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,695 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,807 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:02,014 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:02,163 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:02,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:02,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:02,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:02,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:02,788 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:03,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:03,127 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:04,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:05,801 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:05,805 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:06,695 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:07,129 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:09,686 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:09,687 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:09,690 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:11,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:11,040 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:11,779 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:12,481 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:12,481 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:12,619 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:12,619 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:12,628 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:12,628 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:12,925 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:12,926 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:13,211 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:13,247 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:13,247 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:13,973 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:13,980 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:14,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:14,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:15,091 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:15,091 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:15,098 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:15,765 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:16,911 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:17,131 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:18,954 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:21,550 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:22,606 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:22,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:22,608 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:22,608 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:22,609 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:22,611 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:59,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:59,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:59,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:03,826 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:03,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:03,833 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:03,834 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:05,310 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:05,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:05,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:05,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:05,662 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:06,481 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:06,937 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:07,047 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:07,319 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:07,410 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:07,596 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:07,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:08,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:09,279 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:09,295 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:09,296 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:09,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:09,641 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:09,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:10,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:10,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:10,192 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:10,198 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:10,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:10,794 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:10,814 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:11,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:11,811 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:15,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:16,475 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:16,943 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:16,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:17,090 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:17,369 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:19,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:22,175 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:22,370 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:22,372 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:22,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:22,523 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:22,766 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:23,102 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:23,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:25,439 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:26,516 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:26,885 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:27,573 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:27,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:27,661 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:27,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:27,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:28,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:28,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:28,467 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:28,467 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:28,469 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:28,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:28,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:28,958 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:29,296 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:29,349 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:29,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:29,685 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:30,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:30,651 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:34,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:36,349 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:36,787 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:38,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:38,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:41,847 - distributed.core - INFO - Event loop was unresponsive in Worker for 24.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:43,937 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:44,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:44,366 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:46,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:47,285 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:47,842 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:48,630 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:48,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:49,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:49,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:49,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:49,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:49,665 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:49,668 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:49,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:50,813 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:50,865 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:50,871 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:51,046 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:51,048 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:52,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:52,070 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:52,846 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:53,063 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:53,063 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:53,065 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:57,047 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:57,130 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:57,451 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:57,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:58,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:59,363 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:03,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:03,180 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:03,180 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:04,209 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:06,087 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:06,095 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:06,244 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:06,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:06,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:07,699 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:07,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:07,703 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:07,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:08,066 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:08,068 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:09,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:09,657 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:09,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:09,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:10,627 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:10,840 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:10,890 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:10,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:10,905 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:11,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:11,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:12,858 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:13,230 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:13,349 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:13,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:14,134 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:16,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:19,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:19,969 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:20,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:20,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:27,428 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:27,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:29,101 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:30,288 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:30,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:30,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:30,449 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:31,824 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:31,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:31,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:32,157 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:32,738 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:32,738 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:32,887 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:33,601 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:34,634 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:35,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:35,281 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:35,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:35,797 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:35,800 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:35,823 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:35,824 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:36,138 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:36,691 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:37,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:37,087 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:37,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:37,159 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:37,494 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:37,494 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:37,498 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:37,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:37,981 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:38,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:38,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:38,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:38,395 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:38,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:39,130 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:39,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:39,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:39,284 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:39,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:39,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:19,599 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:19,600 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:19,927 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:21,689 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:21,800 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:22,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:22,378 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:23,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:25,779 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:25,779 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:25,784 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:26,037 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:29,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:29,480 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:30,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:31,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:32,838 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:34,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:34,317 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:34,317 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:34,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:34,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:34,774 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:35,271 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:35,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:35,971 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:36,000 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:36,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:36,001 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:36,004 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:36,208 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:36,591 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:36,595 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:36,595 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:37,443 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:37,868 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:38,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:38,530 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:38,532 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:38,532 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:38,533 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:39,156 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:39,164 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:39,859 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:39,859 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:44,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:44,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:44,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:52,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:52,604 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:53,759 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:53,766 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:54,558 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:54,559 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:55,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:55,214 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:55,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:55,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:55,421 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:55,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:56,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:56,036 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:56,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:56,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:56,556 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:56,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:56,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:57,005 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:57,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:57,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:57,055 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:57,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:57,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:57,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:57,994 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:58,129 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:58,131 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:58,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:58,137 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:58,785 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:59,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:59,535 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:59,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:59,977 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:59,979 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:59,981 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:00,037 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:00,201 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:00,202 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:00,204 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:01,238 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:01,271 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:01,273 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:01,275 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:01,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:03,541 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:04,478 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:07,973 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:09,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:10,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:10,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:11,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:11,580 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:11,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:12,228 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:12,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:12,276 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:12,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:12,386 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:13,073 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:13,074 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:13,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:13,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:13,977 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:14,128 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:14,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:14,256 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:14,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:14,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:14,369 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:14,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:15,238 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:15,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:15,411 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:15,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:15,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:15,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:15,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:15,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:15,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:15,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:16,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:16,288 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:16,306 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:16,311 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:17,051 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:17,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:17,590 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:17,590 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:17,900 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:17,903 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:17,903 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:18,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:18,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:18,148 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:18,149 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:18,394 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:18,733 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:19,207 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:26,979 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:27,137 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:27,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:27,847 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:30,310 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:30,555 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:30,939 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:30,939 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:30,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:31,622 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:32,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:32,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:32,438 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:32,441 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:32,818 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:33,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:33,618 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:33,701 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:33,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:33,708 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:33,732 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:33,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:34,101 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:34,494 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:34,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:35,189 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:35,201 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:35,204 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:35,218 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:35,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:35,361 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:35,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:35,694 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:35,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:35,731 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:36,132 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:36,387 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:36,727 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:36,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:37,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:37,444 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:38,936 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:39,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:40,651 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:40,790 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:40,792 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:47,036 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:51,977 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:52,654 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:53,181 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:53,793 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:53,796 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:53,841 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:53,843 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:54,661 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:55,456 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:55,621 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:55,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:55,986 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:02,671 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:19,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:19,513 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:19,518 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:19,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:19,958 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:20,606 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:20,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:20,924 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:21,651 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:21,657 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:21,660 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:21,662 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:22,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:22,522 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:22,524 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:22,615 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:24,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:24,100 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:24,479 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:26,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:26,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:26,065 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:26,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:26,207 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:26,213 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:26,870 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:26,956 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:27,671 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:29,195 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:29,198 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:29,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:29,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:30,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:30,243 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:30,356 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:31,052 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:31,932 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:32,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:32,777 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:33,548 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:33,551 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:34,088 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:34,098 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:40,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:44,470 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:45,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:45,505 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:45,628 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:45,761 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:45,868 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:45,980 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:46,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:46,464 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:47,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:48,728 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:48,935 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:48,938 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:49,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:49,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:49,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:49,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:49,748 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:49,750 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:49,818 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:49,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:49,936 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:50,399 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:50,462 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:52,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:52,357 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:52,611 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:53,464 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:53,466 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:53,484 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:53,636 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:54,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:55,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:55,634 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:57,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:57,183 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:58,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:58,550 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:58,552 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:58,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:58,554 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:00,138 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:00,360 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:04,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:08,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:14,288 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:14,849 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:14,853 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:15,551 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:16,165 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:16,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:16,427 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:18,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:18,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:19,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:19,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:19,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:20,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:20,817 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:20,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:21,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:21,091 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:21,092 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:21,093 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:21,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:21,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:21,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:21,510 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:21,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:21,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:21,538 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:21,542 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:21,663 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:21,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:21,958 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:21,971 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:22,045 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:22,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:22,518 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:22,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:22,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:22,532 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:22,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:22,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:22,658 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:22,744 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:26,059 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:26,510 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:27,253 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:27,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:28,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:28,273 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:29,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:29,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:29,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:29,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:29,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:29,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:29,942 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:30,805 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:31,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:31,487 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:31,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:31,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:32,595 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:32,784 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:32,900 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:34,000 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:34,004 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:34,005 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:34,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:34,520 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:34,520 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:35,459 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:35,460 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:35,462 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:35,701 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:35,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:35,822 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:35,973 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:35,975 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:35,975 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:35,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:35,978 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:36,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:36,591 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:36,595 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:37,196 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:41,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:42,135 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:46,166 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:47,181 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:47,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:47,821 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:47,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:47,919 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:47,921 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:48,671 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:50,023 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:50,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:50,032 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:50,396 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:50,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:50,449 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:51,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:51,641 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:52,776 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,777 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:41879. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,776 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,776 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,776 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,776 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,776 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,776 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,776 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,776 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,778 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:40949. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,776 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,776 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,776 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,778 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:46815. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,776 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,778 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:37565. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,778 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:38023. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,778 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:40281. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,776 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,779 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:33977. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,777 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,779 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:37261. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,779 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:40823. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,777 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,779 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:39309. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,779 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:42757. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,779 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:45731. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,779 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:35877. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,780 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:40225. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,780 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:43809. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,780 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:42331. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,775 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.2:49888 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,781 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,782 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:42855. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,775 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.2:49742 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,776 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.2:49724 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,775 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.2:49788 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,776 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.2:49726 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,775 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.2:49942 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,775 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.2:49774 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,776 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.2:49694 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,775 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.2:49816 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,775 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.2:49910 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,775 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.2:49800 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,775 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.2:49814 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,775 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.2:49894 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,775 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.2:49890 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,776 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.2:49678 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,777 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.2:49832 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,776 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.2:49880 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,778 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.2:49854 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,784 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,784 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,786 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:43961. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,784 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,784 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,785 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,785 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,785 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,786 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:42627. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,785 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,786 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:36513. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,786 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:44723. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,785 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,787 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:43827. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,787 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:46601. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,787 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:45949. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,787 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:38695. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,785 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,787 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:45155. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,786 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,785 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,787 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:41451. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,785 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,786 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,787 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:44449. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,788 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:45991. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,785 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,788 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:42357. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,788 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:42261. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,785 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,786 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,788 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:37523. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,788 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:38025. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,789 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:34533. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,792 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:36391'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,793 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:40857'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,794 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:39823'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,794 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:39577'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,794 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763175, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,794 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:46183'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,794 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 766649, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,794 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:39607'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,794 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 766308, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,795 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:33249'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,795 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,795 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 766307, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,795 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,795 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:42495'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,795 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765513, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,795 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765066, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,795 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,795 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,795 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:35699'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,795 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765511, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,795 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,795 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763177, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,795 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,795 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:42665'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,795 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,795 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:43561'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,796 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764455, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,796 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:46693'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,796 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:34449'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,796 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764460, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,796 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764030, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,796 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.31:38479, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,796 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:38693'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,796 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763365, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,796 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763427, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,796 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:38389'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,796 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763610, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,796 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,796 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.2:43893, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,796 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764359, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,797 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:45347'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,796 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763423, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,797 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,797 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,797 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763607, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,797 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,797 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.11:38001, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,797 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764166, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,797 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764847, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,797 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763620, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,797 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.26:42957, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,797 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763173, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,797 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.1:37089, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,797 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,797 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.7:32995, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,797 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764848, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,797 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.2:39139, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,797 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 766674, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,797 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764171, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,797 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,797 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764358, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,797 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,798 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,798 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,798 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 768893, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,798 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,798 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,798 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.2:36291, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,798 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.31:38493, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,798 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,798 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,798 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:43081'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,798 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763361, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,798 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,798 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,798 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763602, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,798 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,798 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,798 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.8:44443, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,798 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,798 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,798 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,798 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763615, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,798 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.8:46657, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,798 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,798 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,799 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 767680, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,799 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764355, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,799 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.6:39411, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,799 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.2:38729, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,799 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 767692, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,799 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764361, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,800 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,800 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,800 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,800 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,800 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,800 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,800 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,800 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,800 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,800 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,800 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,800 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,800 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,800 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,801 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,801 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,801 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,801 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,801 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,801 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,801 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:43289'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,801 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,801 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:42309'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,801 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,801 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:37059'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,801 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,801 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:34745'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,802 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:38033'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,802 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:43771'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,802 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:38349'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,802 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:37985'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,802 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:41881'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,802 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:36279'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,802 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:40205'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,803 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:40223'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,803 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763428, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,803 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765238, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,803 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:35203'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,803 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 767629, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,803 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763429, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,803 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764398, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,803 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765732, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,803 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:45955'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,803 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764264, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,803 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 767623, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,803 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763363, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,803 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.22:34383, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,803 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764346, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,803 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764389, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,803 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765657, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,803 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:45893'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,803 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763226, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,804 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.2:45459, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,804 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,804 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.24:46413, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,804 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763435, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,804 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.4:35031, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,804 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,804 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765707, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,804 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:38715'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,804 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764357, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,804 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.2:43893, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,804 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.2:42685, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,804 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764266, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,804 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763432, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,804 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,804 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.17:37205, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,804 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763364, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,804 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:43627'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,804 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,804 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,804 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 768498, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,804 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764083, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,804 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765234, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,804 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.15:43021, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,805 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.8:40835, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,805 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,805 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 768275, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,805 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,805 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.13:42835, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,805 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764084, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,805 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,805 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.28:33943, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,805 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,805 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.31:42215, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,805 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,805 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764061, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,805 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,805 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,805 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,805 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765004, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,805 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,805 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764440, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,805 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763573, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,805 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,805 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.26:39059, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,806 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763500, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,806 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.25:45661, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,806 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765353, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,807 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764107, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,807 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764311, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,809 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,809 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,809 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,809 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,809 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,809 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,809 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,809 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,809 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,809 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,809 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,809 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,810 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,810 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,811 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,811 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,812 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,869 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:54,550 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:54,552 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:38729. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:54,566 - distributed.worker - ERROR - failed during get data with tcp://10.6.105.2:38729 -> tcp://10.6.105.2:45731
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.105.2:38729 remote=tcp://10.6.105.2:60820>: Stream is closed
2025-09-05 09:49:54,573 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:54,578 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.2:49266 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:54,582 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:45031'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:54,583 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:54,583 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:54,583 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:54,583 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:54,584 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:54,592 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14c3acea0610>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:54,600 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:54,707 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:54,708 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:45147. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:54,714 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:54,717 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.2:49406 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:54,719 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:39317'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:54,719 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:54,719 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:54,719 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:54,719 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:54,719 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:54,725 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14ebd53d3e10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:54,730 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:54,872 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:54,919 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:54,922 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:45459. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:54,925 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:54,927 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:37291. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:54,939 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:54,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:54,944 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.2:49340 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:54,948 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:46727'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:54,949 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:54,949 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:54,949 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:54,949 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:54,949 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:54,949 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.2:49494 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:54,952 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:45795'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:54,953 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 766417, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:54,953 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:54,953 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:54,953 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:54,953 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:54,953 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:54,957 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x149fe5825b90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:54,960 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14f8661d6850>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:54,964 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:55,137 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:55,140 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:45335. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:55,155 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:55,160 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.2:49370 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:55,170 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:42101'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:55,171 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:55,171 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:55,171 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:55,171 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:55,171 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:55,180 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14d2037c3650>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:55,187 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:55,258 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:55,260 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:43129. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:55,279 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:55,283 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.2:49278 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:55,287 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:38941'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:55,288 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 767045, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:55,288 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:55,288 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:55,288 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:55,288 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:55,288 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:55,295 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x147b4cc22310>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:55,477 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:42309'. Reason: nanny-close-gracefully
2025-09-05 09:49:55,480 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:42309' closed.
2025-09-05 09:49:55,909 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:55,912 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:33973. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:55,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:55,936 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.2:49288 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:55,940 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:39677'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:55,941 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:55,942 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:55,942 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:55,942 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:55,942 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:55,949 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x15263c1bdc90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:55,957 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:56,604 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:56,654 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:56,655 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:56,732 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:56,967 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:57,111 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:45031'. Reason: nanny-close-gracefully
2025-09-05 09:49:57,113 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:45031' closed.
2025-09-05 09:49:57,156 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:39317'. Reason: nanny-close-gracefully
2025-09-05 09:49:57,156 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:57,157 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:39317' closed.
2025-09-05 09:49:57,190 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:57,333 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:57,356 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:57,403 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:57,404 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:57,437 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:46727'. Reason: nanny-close-gracefully
2025-09-05 09:49:57,438 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:46727' closed.
2025-09-05 09:49:57,542 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:57,659 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:42101'. Reason: nanny-close-gracefully
2025-09-05 09:49:57,660 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:42101' closed.
2025-09-05 09:49:57,885 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:57,887 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:33931. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:57,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:57,914 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.2:49536 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:57,917 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:45871'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:57,918 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:57,918 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:57,919 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:57,919 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:57,919 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:57,924 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x15544f956310>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:57,930 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:57,960 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:57,980 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:57,982 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:57,985 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:43893. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:58,003 - distributed.worker - ERROR - failed during get data with tcp://10.6.105.2:43893 -> tcp://10.6.105.2:40281
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.105.2:43893 remote=tcp://10.6.105.2:50466>: Stream is closed
2025-09-05 09:49:58,014 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.2:49350 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:58,018 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:45217'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:58,018 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:58,019 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:58,019 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:58,019 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:58,019 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:58,024 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14f95f10b590>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:58,029 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:58,156 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:58,232 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:58,234 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:45285. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:58,253 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:58,256 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.2:49162 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:58,272 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:43727'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:58,273 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:58,273 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:58,273 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:58,273 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:58,273 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:58,284 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:58,422 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:39677'. Reason: nanny-close-gracefully
2025-09-05 09:49:58,424 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:39677' closed.
2025-09-05 09:49:58,658 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:58,659 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:58,876 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:58,970 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:59,160 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:59,239 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:43081'. Reason: nanny-close-gracefully
2025-09-05 09:49:59,240 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:39823'. Reason: nanny-close-gracefully
2025-09-05 09:49:59,241 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:43081' closed.
2025-09-05 09:49:59,241 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:39823' closed.
2025-09-05 09:49:59,337 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:59,360 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:59,384 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:59,387 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:43805. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:59,407 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:59,408 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:59,405 - distributed.worker - ERROR - failed during get data with tcp://10.6.105.2:43805 -> tcp://10.6.105.7:42333
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.105.2:43805 remote=tcp://10.6.105.7:48126>: Stream is closed
2025-09-05 09:49:59,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:59,416 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.2:49474 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:59,419 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:34531'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:59,420 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:59,420 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:59,420 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:59,420 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:59,420 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:59,425 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14bf1a188b90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:59,430 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:59,547 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:59,632 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:40857'. Reason: nanny-close-gracefully
2025-09-05 09:49:59,633 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:40857' closed.
2025-09-05 09:49:59,638 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:59,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:59,642 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:59,645 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:39139. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:59,644 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:59,645 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:38719. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:59,661 - distributed.worker - ERROR - failed during get data with tcp://10.6.105.2:39139 -> tcp://10.6.105.2:40823
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.105.2:39139 remote=tcp://10.6.105.2:59596>: Stream is closed
2025-09-05 09:49:59,662 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.105.15:33727
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.105.2:55476 remote=tcp://10.6.105.15:33727>: Stream is closed
2025-09-05 09:49:59,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:59,672 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.2:49544 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:59,673 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.2:49546 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:59,680 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:35307'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:59,680 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:59,680 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:39039'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:59,681 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:59,681 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:59,681 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:59,681 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:59,681 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:59,681 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:59,681 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:59,681 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:59,681 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:59,685 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x15136392d050>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:59,685 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14f1cfa030d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:59,691 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:59,691 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:59,775 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:59,778 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:34471. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:59,798 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:59,802 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.2:49240 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:59,805 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:40947'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:59,806 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:59,806 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:59,806 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:59,806 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:59,806 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:59,810 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x152593d38d90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:59,815 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:59,815 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:34449'. Reason: nanny-close-gracefully
2025-09-05 09:49:59,816 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:34449' closed.
2025-09-05 09:49:59,877 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:33249'. Reason: nanny-close-gracefully
2025-09-05 09:49:59,878 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:33249' closed.
2025-09-05 09:49:59,896 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:37059'. Reason: nanny-close-gracefully
2025-09-05 09:49:59,897 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:37059' closed.
2025-09-05 09:49:59,933 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:59,959 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:46183'. Reason: nanny-close-gracefully
2025-09-05 09:49:59,960 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:46183' closed.
2025-09-05 09:49:59,963 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,032 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:00,072 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:38715'. Reason: nanny-close-gracefully
2025-09-05 09:50:00,073 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:38715' closed.
2025-09-05 09:50:00,160 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:00,181 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,182 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,183 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,185 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,189 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,287 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:00,325 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,387 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:45871'. Reason: nanny-close-gracefully
2025-09-05 09:50:00,389 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:45871' closed.
2025-09-05 09:50:00,476 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,510 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:45217'. Reason: nanny-close-gracefully
2025-09-05 09:50:00,511 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:45217' closed.
2025-09-05 09:50:00,557 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,630 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:40205'. Reason: nanny-close-gracefully
2025-09-05 09:50:00,631 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:40205' closed.
2025-09-05 09:50:00,703 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:50:00,706 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:36291. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:50:00,726 - distributed.worker - ERROR - failed during get data with tcp://10.6.105.2:36291 -> tcp://10.6.105.2:45731
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.105.2:36291 remote=tcp://10.6.105.2:38096>: Stream is closed
2025-09-05 09:50:00,733 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:50:00,736 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:43727'. Reason: nanny-close-gracefully
2025-09-05 09:50:00,737 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.2:49468 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:50:00,737 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:43727' closed.
2025-09-05 09:50:00,739 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:46685'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:50:00,740 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:50:00,740 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:50:00,740 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:50:00,740 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:50:00,740 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:50:00,743 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14a65b4bf710>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:50:00,748 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,880 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:00,916 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:50:00,917 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:50:00,919 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:42685. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:50:00,920 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:39623. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:50:00,939 - distributed.worker - ERROR - failed during get data with tcp://10.6.105.2:42685 -> tcp://10.6.105.2:43961
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.105.2:42685 remote=tcp://10.6.105.2:44100>: Stream is closed
2025-09-05 09:50:00,946 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:50:00,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:50:00,950 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.2:49540 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:50:00,951 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.2:49150 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:50:00,952 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:44373'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:50:00,953 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:50:00,953 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:50:00,953 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:50:00,953 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:50:00,953 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:50:00,954 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:45821'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:50:00,955 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:50:00,955 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:50:00,955 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:50:00,955 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:50:00,955 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:50:00,956 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x149b0d277890>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:50:00,958 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1549801bc4d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:50:00,961 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,964 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,965 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,966 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,974 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:01,033 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,043 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,301 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,302 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,302 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,303 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,307 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:50:01,308 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.2:39693. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:50:01,309 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:50:01,333 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,335 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.2:49342 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:50:01,337 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.2:36449'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:50:01,338 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:50:01,338 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:50:01,338 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:50:01,339 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:50:01,339 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:50:01,342 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:42665'. Reason: nanny-close-gracefully
2025-09-05 09:50:01,341 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14ccfbe3ae90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:50:01,343 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:42665' closed.
2025-09-05 09:50:01,346 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,415 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,415 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,417 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,428 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:34745'. Reason: nanny-close-gracefully
2025-09-05 09:50:01,429 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:34745' closed.
2025-09-05 09:50:01,434 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:01,539 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,642 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:01,693 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:01,694 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:01,818 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:01,878 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:34531'. Reason: nanny-close-gracefully
2025-09-05 09:50:01,879 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:34531' closed.
2025-09-05 09:50:01,967 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,112 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:45795'. Reason: nanny-close-gracefully
2025-09-05 09:50:02,113 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:45795' closed.
2025-09-05 09:50:02,156 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:39039'. Reason: nanny-close-gracefully
2025-09-05 09:50:02,157 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:39039' closed.
2025-09-05 09:50:02,167 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:35307'. Reason: nanny-close-gracefully
2025-09-05 09:50:02,173 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:35307' closed.
2025-09-05 09:50:02,185 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,186 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,187 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,189 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,193 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,298 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:40947'. Reason: nanny-close-gracefully
2025-09-05 09:50:02,299 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:40947' closed.
2025-09-05 09:50:02,329 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,481 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,487 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:41881'. Reason: nanny-close-gracefully
2025-09-05 09:50:02,488 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:41881' closed.
2025-09-05 09:50:02,561 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,656 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:43627'. Reason: nanny-close-gracefully
2025-09-05 09:50:02,657 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:43627' closed.
2025-09-05 09:50:02,669 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:39577'. Reason: nanny-close-gracefully
2025-09-05 09:50:02,673 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:39577' closed.
2025-09-05 09:50:02,674 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:45347'. Reason: nanny-close-gracefully
2025-09-05 09:50:02,676 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:45347' closed.
2025-09-05 09:50:02,726 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:45893'. Reason: nanny-close-gracefully
2025-09-05 09:50:02,727 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:45893' closed.
2025-09-05 09:50:02,770 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:40223'. Reason: nanny-close-gracefully
2025-09-05 09:50:02,771 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:40223' closed.
2025-09-05 09:50:02,811 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:39607'. Reason: nanny-close-gracefully
2025-09-05 09:50:02,812 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:39607' closed.
2025-09-05 09:50:02,964 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,967 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,968 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,971 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,989 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:42495'. Reason: nanny-close-gracefully
2025-09-05 09:50:02,990 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:42495' closed.
2025-09-05 09:50:03,039 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:03,048 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:03,053 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:43771'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,054 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:43771' closed.
2025-09-05 09:50:03,228 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:46685'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,229 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:46685' closed.
2025-09-05 09:50:03,305 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:03,305 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:03,306 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:03,307 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:03,312 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:03,337 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:03,348 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:03,419 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:03,419 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:03,421 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:03,444 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:45821'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,445 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:45821' closed.
2025-09-05 09:50:03,512 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:38033'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,515 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:38033' closed.
2025-09-05 09:50:03,530 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:37985'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,531 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:37985' closed.
2025-09-05 09:50:03,543 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:03,554 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:44373'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,555 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:44373' closed.
2025-09-05 09:50:03,602 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:46693'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,603 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:46693' closed.
2025-09-05 09:50:03,789 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:38941'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,790 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:38941' closed.
2025-09-05 09:50:03,793 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:38693'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,794 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:38693' closed.
2025-09-05 09:50:03,860 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:36391'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,861 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:36391' closed.
2025-09-05 09:50:03,900 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:43289'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,901 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:43289' closed.
2025-09-05 09:50:03,903 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:45955'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,903 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:45955' closed.
2025-09-05 09:50:03,914 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:35203'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,914 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:35203' closed.
2025-09-05 09:50:03,923 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:43561'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,924 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:43561' closed.
2025-09-05 09:50:03,948 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:36449'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,948 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:36449' closed.
2025-09-05 09:50:03,952 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:38349'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,953 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:38349' closed.
2025-09-05 09:50:04,017 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:35699'. Reason: nanny-close-gracefully
2025-09-05 09:50:04,017 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:35699' closed.
2025-09-05 09:50:04,061 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:38389'. Reason: nanny-close-gracefully
2025-09-05 09:50:04,062 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:38389' closed.
2025-09-05 09:50:04,222 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.2:36279'. Reason: nanny-close-gracefully
2025-09-05 09:50:04,223 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.2:36279' closed.
2025-09-05 09:50:04,225 - distributed.dask_worker - INFO - End worker
