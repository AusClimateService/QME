Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-10-02 16:13:06,814 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:43061'
2025-10-02 16:13:06,823 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:35111'
2025-10-02 16:13:06,828 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:40343'
2025-10-02 16:13:06,832 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:38995'
2025-10-02 16:13:06,836 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:43285'
2025-10-02 16:13:06,841 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:37799'
2025-10-02 16:13:06,845 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:33451'
2025-10-02 16:13:06,849 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:43213'
2025-10-02 16:13:06,852 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:40039'
2025-10-02 16:13:06,857 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:38739'
2025-10-02 16:13:06,861 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:34871'
2025-10-02 16:13:06,865 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:36407'
2025-10-02 16:13:06,870 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:37813'
2025-10-02 16:13:06,874 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:46877'
2025-10-02 16:13:06,877 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:34229'
2025-10-02 16:13:06,881 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:38803'
2025-10-02 16:13:06,885 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:39169'
2025-10-02 16:13:06,889 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:38833'
2025-10-02 16:13:06,894 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:37253'
2025-10-02 16:13:06,898 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:40875'
2025-10-02 16:13:06,990 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:43649'
2025-10-02 16:13:06,995 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:36031'
2025-10-02 16:13:06,999 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:38177'
2025-10-02 16:13:07,005 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:36723'
2025-10-02 16:13:07,010 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:41827'
2025-10-02 16:13:07,014 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:40553'
2025-10-02 16:13:07,019 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:45915'
2025-10-02 16:13:07,024 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:42907'
2025-10-02 16:13:07,028 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:45403'
2025-10-02 16:13:07,032 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:36395'
2025-10-02 16:13:07,037 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:45211'
2025-10-02 16:13:07,042 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:36237'
2025-10-02 16:13:07,047 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:42645'
2025-10-02 16:13:07,051 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:45027'
2025-10-02 16:13:07,056 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:46551'
2025-10-02 16:13:07,061 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:40569'
2025-10-02 16:13:07,065 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:41017'
2025-10-02 16:13:07,069 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:45851'
2025-10-02 16:13:07,074 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:38501'
2025-10-02 16:13:07,079 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:37117'
2025-10-02 16:13:07,083 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:36409'
2025-10-02 16:13:07,087 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:38691'
2025-10-02 16:13:07,092 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:36385'
2025-10-02 16:13:07,097 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:38749'
2025-10-02 16:13:07,102 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:33417'
2025-10-02 16:13:07,109 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:35679'
2025-10-02 16:13:07,113 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:35107'
2025-10-02 16:13:07,118 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:34403'
2025-10-02 16:13:07,122 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:42161'
2025-10-02 16:13:07,126 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:43333'
2025-10-02 16:13:07,130 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:35407'
2025-10-02 16:13:07,135 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.82.19:40647'
2025-10-02 16:13:08,296 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:46273
2025-10-02 16:13:08,296 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:43955
2025-10-02 16:13:08,296 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:37335
2025-10-02 16:13:08,296 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:37823
2025-10-02 16:13:08,296 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:45227
2025-10-02 16:13:08,296 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:40823
2025-10-02 16:13:08,296 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:38569
2025-10-02 16:13:08,296 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:39193
2025-10-02 16:13:08,296 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:39477
2025-10-02 16:13:08,296 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:32965
2025-10-02 16:13:08,296 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:43263
2025-10-02 16:13:08,296 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:43627
2025-10-02 16:13:08,296 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:35235
2025-10-02 16:13:08,296 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:44161
2025-10-02 16:13:08,296 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:46273
2025-10-02 16:13:08,296 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:35327
2025-10-02 16:13:08,296 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:36911
2025-10-02 16:13:08,296 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:37197
2025-10-02 16:13:08,296 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:42821
2025-10-02 16:13:08,296 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:43955
2025-10-02 16:13:08,296 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:37335
2025-10-02 16:13:08,296 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:37823
2025-10-02 16:13:08,296 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:45227
2025-10-02 16:13:08,296 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:40823
2025-10-02 16:13:08,296 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:38569
2025-10-02 16:13:08,296 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:39193
2025-10-02 16:13:08,296 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:39477
2025-10-02 16:13:08,296 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:32965
2025-10-02 16:13:08,296 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:43263
2025-10-02 16:13:08,296 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:43627
2025-10-02 16:13:08,296 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:35235
2025-10-02 16:13:08,296 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:44161
2025-10-02 16:13:08,296 - distributed.worker - INFO -          dashboard at:           10.6.82.19:36671
2025-10-02 16:13:08,296 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:35327
2025-10-02 16:13:08,296 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:36911
2025-10-02 16:13:08,296 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:37197
2025-10-02 16:13:08,296 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:42821
2025-10-02 16:13:08,296 - distributed.worker - INFO -          dashboard at:           10.6.82.19:46463
2025-10-02 16:13:08,296 - distributed.worker - INFO -          dashboard at:           10.6.82.19:37645
2025-10-02 16:13:08,297 - distributed.worker - INFO -          dashboard at:           10.6.82.19:39763
2025-10-02 16:13:08,297 - distributed.worker - INFO -          dashboard at:           10.6.82.19:37631
2025-10-02 16:13:08,297 - distributed.worker - INFO -          dashboard at:           10.6.82.19:32853
2025-10-02 16:13:08,297 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,297 - distributed.worker - INFO -          dashboard at:           10.6.82.19:39161
2025-10-02 16:13:08,297 - distributed.worker - INFO -          dashboard at:           10.6.82.19:44437
2025-10-02 16:13:08,297 - distributed.worker - INFO -          dashboard at:           10.6.82.19:40927
2025-10-02 16:13:08,297 - distributed.worker - INFO -          dashboard at:           10.6.82.19:35129
2025-10-02 16:13:08,297 - distributed.worker - INFO -          dashboard at:           10.6.82.19:35355
2025-10-02 16:13:08,297 - distributed.worker - INFO -          dashboard at:           10.6.82.19:43741
2025-10-02 16:13:08,297 - distributed.worker - INFO -          dashboard at:           10.6.82.19:33603
2025-10-02 16:13:08,297 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,297 - distributed.worker - INFO -          dashboard at:           10.6.82.19:33489
2025-10-02 16:13:08,297 - distributed.worker - INFO -          dashboard at:           10.6.82.19:39745
2025-10-02 16:13:08,297 - distributed.worker - INFO -          dashboard at:           10.6.82.19:43199
2025-10-02 16:13:08,297 - distributed.worker - INFO -          dashboard at:           10.6.82.19:34409
2025-10-02 16:13:08,297 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,297 - distributed.worker - INFO -          dashboard at:           10.6.82.19:45481
2025-10-02 16:13:08,297 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,297 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,297 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,297 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,297 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,297 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,297 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,297 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,297 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,297 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,297 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,297 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,297 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,297 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,297 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,297 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,297 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,297 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,297 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,297 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,297 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,297 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,297 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,297 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,297 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,297 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,297 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,297 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,297 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,297 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,297 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,297 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,297 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-99ycit0e
2025-10-02 16:13:08,297 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,297 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,297 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,297 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,297 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,297 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,297 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,297 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-pjap_geg
2025-10-02 16:13:08,297 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,297 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,297 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,297 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,297 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-2mv5yx0g
2025-10-02 16:13:08,297 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,297 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-rmyp9mpv
2025-10-02 16:13:08,297 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-uti49xkf
2025-10-02 16:13:08,297 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-9zew_2hg
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-2mgznd9x
2025-10-02 16:13:08,297 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-7sudhcma
2025-10-02 16:13:08,297 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-h462z01v
2025-10-02 16:13:08,297 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-sqx5zk84
2025-10-02 16:13:08,297 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-85v1p1wa
2025-10-02 16:13:08,297 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-_hj3fvak
2025-10-02 16:13:08,297 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-yjis_6e9
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-aoc_vw54
2025-10-02 16:13:08,297 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-gwvgu88f
2025-10-02 16:13:08,297 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-ks21p5n2
2025-10-02 16:13:08,297 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-quurx9i6
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-3jeqd8u4
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,297 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,298 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,298 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,298 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,303 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:43465
2025-10-02 16:13:08,303 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:43465
2025-10-02 16:13:08,303 - distributed.worker - INFO -          dashboard at:           10.6.82.19:40417
2025-10-02 16:13:08,303 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,303 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,303 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,303 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,303 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-l8qr6bzw
2025-10-02 16:13:08,303 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:43721
2025-10-02 16:13:08,303 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,303 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:43721
2025-10-02 16:13:08,303 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:46675
2025-10-02 16:13:08,304 - distributed.worker - INFO -          dashboard at:           10.6.82.19:38479
2025-10-02 16:13:08,304 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,304 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:46675
2025-10-02 16:13:08,304 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,304 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:36829
2025-10-02 16:13:08,304 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,304 - distributed.worker - INFO -          dashboard at:           10.6.82.19:39669
2025-10-02 16:13:08,304 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,304 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:36829
2025-10-02 16:13:08,304 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,304 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,304 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-1mr4n0sr
2025-10-02 16:13:08,304 - distributed.worker - INFO -          dashboard at:           10.6.82.19:36677
2025-10-02 16:13:08,304 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,304 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,304 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,304 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,304 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,304 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,304 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-p6cplzy3
2025-10-02 16:13:08,304 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,304 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,304 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-ects8h4_
2025-10-02 16:13:08,304 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,309 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:38211
2025-10-02 16:13:08,310 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:38211
2025-10-02 16:13:08,310 - distributed.worker - INFO -          dashboard at:           10.6.82.19:40353
2025-10-02 16:13:08,310 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,310 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,310 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,310 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,310 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-lfwog9m7
2025-10-02 16:13:08,310 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,341 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:42547
2025-10-02 16:13:08,341 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:42547
2025-10-02 16:13:08,341 - distributed.worker - INFO -          dashboard at:           10.6.82.19:37019
2025-10-02 16:13:08,341 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,341 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,341 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,341 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,341 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-hl06rikg
2025-10-02 16:13:08,341 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,342 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,343 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,344 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,345 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,361 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,362 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,362 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,364 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,376 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,377 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,377 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,379 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,390 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,391 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,391 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,393 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,403 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,404 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,405 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,405 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:42225
2025-10-02 16:13:08,405 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:42225
2025-10-02 16:13:08,405 - distributed.worker - INFO -          dashboard at:           10.6.82.19:44461
2025-10-02 16:13:08,405 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,405 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,405 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,406 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,406 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-_x2wtwt8
2025-10-02 16:13:08,406 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,406 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,416 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,417 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,417 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,418 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,428 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,429 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,430 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,432 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,435 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:36085
2025-10-02 16:13:08,435 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:36085
2025-10-02 16:13:08,435 - distributed.worker - INFO -          dashboard at:           10.6.82.19:41501
2025-10-02 16:13:08,435 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,435 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,435 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,435 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,435 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-5opobpyp
2025-10-02 16:13:08,435 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,439 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,440 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,440 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,442 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,449 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,449 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,449 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,450 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,456 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:46177
2025-10-02 16:13:08,457 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:46177
2025-10-02 16:13:08,457 - distributed.worker - INFO -          dashboard at:           10.6.82.19:38887
2025-10-02 16:13:08,457 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,457 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,457 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,457 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,457 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-w0lqix5j
2025-10-02 16:13:08,457 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,458 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:38955
2025-10-02 16:13:08,458 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:38955
2025-10-02 16:13:08,458 - distributed.worker - INFO -          dashboard at:           10.6.82.19:40579
2025-10-02 16:13:08,458 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,458 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,458 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,459 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,459 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-m98doga9
2025-10-02 16:13:08,459 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,460 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,462 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,462 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,463 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,470 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,471 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,471 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,473 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,474 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:35699
2025-10-02 16:13:08,474 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:35699
2025-10-02 16:13:08,474 - distributed.worker - INFO -          dashboard at:           10.6.82.19:46847
2025-10-02 16:13:08,474 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,474 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,474 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,474 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,474 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-4873xhgi
2025-10-02 16:13:08,474 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,480 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,482 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,482 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:43953
2025-10-02 16:13:08,482 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,482 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:43953
2025-10-02 16:13:08,482 - distributed.worker - INFO -          dashboard at:           10.6.82.19:36015
2025-10-02 16:13:08,482 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,482 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,482 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,482 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,482 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-u9sr2tt9
2025-10-02 16:13:08,482 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,483 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,490 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,491 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,491 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,493 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,500 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,501 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,501 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,503 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,509 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,510 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:37147
2025-10-02 16:13:08,510 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:37147
2025-10-02 16:13:08,510 - distributed.worker - INFO -          dashboard at:           10.6.82.19:36393
2025-10-02 16:13:08,510 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,510 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,510 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,510 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,510 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,510 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-54mycuq1
2025-10-02 16:13:08,510 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,510 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,512 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,518 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,519 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,520 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,521 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,527 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,529 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,529 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,529 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:37345
2025-10-02 16:13:08,530 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:37345
2025-10-02 16:13:08,530 - distributed.worker - INFO -          dashboard at:           10.6.82.19:43331
2025-10-02 16:13:08,530 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,530 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,530 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,530 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,530 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-4s3dv721
2025-10-02 16:13:08,530 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,530 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,537 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,538 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,538 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,540 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,546 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,547 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,547 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,549 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,554 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:39197
2025-10-02 16:13:08,554 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:39197
2025-10-02 16:13:08,554 - distributed.worker - INFO -          dashboard at:           10.6.82.19:36695
2025-10-02 16:13:08,554 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,554 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,554 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,554 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,554 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:35755
2025-10-02 16:13:08,554 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-rqmns_fb
2025-10-02 16:13:08,554 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:35755
2025-10-02 16:13:08,554 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,554 - distributed.worker - INFO -          dashboard at:           10.6.82.19:46673
2025-10-02 16:13:08,554 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,554 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,554 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,554 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,554 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-vcm126hw
2025-10-02 16:13:08,554 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,555 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,555 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:46341
2025-10-02 16:13:08,555 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:46341
2025-10-02 16:13:08,555 - distributed.worker - INFO -          dashboard at:           10.6.82.19:42199
2025-10-02 16:13:08,555 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,555 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,555 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,555 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,555 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-lrduri4r
2025-10-02 16:13:08,555 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,556 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,556 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,557 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,563 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,564 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,565 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,566 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,567 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:34727
2025-10-02 16:13:08,567 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:34727
2025-10-02 16:13:08,567 - distributed.worker - INFO -          dashboard at:           10.6.82.19:46361
2025-10-02 16:13:08,567 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,567 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,567 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,567 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,567 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-nm2y321z
2025-10-02 16:13:08,567 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,569 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:34289
2025-10-02 16:13:08,569 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:34289
2025-10-02 16:13:08,570 - distributed.worker - INFO -          dashboard at:           10.6.82.19:40997
2025-10-02 16:13:08,570 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,570 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,570 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,570 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,570 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-0pvudyqs
2025-10-02 16:13:08,570 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,573 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,574 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,574 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,576 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,581 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,582 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:46025
2025-10-02 16:13:08,582 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:46025
2025-10-02 16:13:08,582 - distributed.worker - INFO -          dashboard at:           10.6.82.19:46265
2025-10-02 16:13:08,582 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,582 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,583 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,583 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,583 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-06gmblts
2025-10-02 16:13:08,583 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,583 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,583 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,585 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,612 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:33995
2025-10-02 16:13:08,612 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:33995
2025-10-02 16:13:08,612 - distributed.worker - INFO -          dashboard at:           10.6.82.19:43073
2025-10-02 16:13:08,612 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,612 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,612 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,612 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,612 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-b3uz82s8
2025-10-02 16:13:08,612 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,613 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,614 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,614 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,616 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,621 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,622 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,622 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,622 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,631 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,632 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,633 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,634 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,640 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,641 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,641 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,643 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,649 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,650 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,650 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,652 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,658 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,659 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,659 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:42013
2025-10-02 16:13:08,659 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,659 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:42013
2025-10-02 16:13:08,659 - distributed.worker - INFO -          dashboard at:           10.6.82.19:34037
2025-10-02 16:13:08,659 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,659 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,659 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,659 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,659 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-ugqwov4n
2025-10-02 16:13:08,659 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,660 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,667 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,668 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,668 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,670 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,676 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,677 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,677 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,678 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,685 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,686 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,686 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,687 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:40903
2025-10-02 16:13:08,687 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:40903
2025-10-02 16:13:08,687 - distributed.worker - INFO -          dashboard at:           10.6.82.19:40225
2025-10-02 16:13:08,687 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,687 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,687 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:46061
2025-10-02 16:13:08,687 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,687 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,687 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:46061
2025-10-02 16:13:08,687 - distributed.worker - INFO -          dashboard at:           10.6.82.19:41341
2025-10-02 16:13:08,687 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-3v2ggh8l
2025-10-02 16:13:08,687 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:43241
2025-10-02 16:13:08,687 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,687 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,687 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,687 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:43241
2025-10-02 16:13:08,687 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,687 - distributed.worker - INFO -          dashboard at:           10.6.82.19:44315
2025-10-02 16:13:08,687 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,687 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,687 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-n8q35gmc
2025-10-02 16:13:08,687 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,687 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,687 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,687 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,687 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-wj4g_svf
2025-10-02 16:13:08,687 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,687 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,688 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:43773
2025-10-02 16:13:08,688 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:43773
2025-10-02 16:13:08,688 - distributed.worker - INFO -          dashboard at:           10.6.82.19:38423
2025-10-02 16:13:08,688 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,688 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,688 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,688 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,688 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-qq8ecnzl
2025-10-02 16:13:08,688 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,689 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:46271
2025-10-02 16:13:08,689 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:46271
2025-10-02 16:13:08,689 - distributed.worker - INFO -          dashboard at:           10.6.82.19:34149
2025-10-02 16:13:08,689 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,689 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,689 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,689 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,689 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-gxpcaqjm
2025-10-02 16:13:08,689 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,690 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:33805
2025-10-02 16:13:08,690 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:33805
2025-10-02 16:13:08,690 - distributed.worker - INFO -          dashboard at:           10.6.82.19:40819
2025-10-02 16:13:08,690 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,690 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,690 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,690 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,690 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-gr5u29fe
2025-10-02 16:13:08,690 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,692 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:42897
2025-10-02 16:13:08,692 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:42897
2025-10-02 16:13:08,692 - distributed.worker - INFO -          dashboard at:           10.6.82.19:39317
2025-10-02 16:13:08,692 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,692 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,692 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,692 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,692 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-_zixesct
2025-10-02 16:13:08,692 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,692 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:32827
2025-10-02 16:13:08,692 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:32827
2025-10-02 16:13:08,692 - distributed.worker - INFO -          dashboard at:           10.6.82.19:36797
2025-10-02 16:13:08,692 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,693 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,693 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,693 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,693 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-fj6vjslq
2025-10-02 16:13:08,693 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,694 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,695 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,695 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,697 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,698 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:44691
2025-10-02 16:13:08,698 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:44691
2025-10-02 16:13:08,698 - distributed.worker - INFO -          dashboard at:           10.6.82.19:45965
2025-10-02 16:13:08,698 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,698 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,698 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,698 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,698 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-yqy9x9pl
2025-10-02 16:13:08,698 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:34897
2025-10-02 16:13:08,698 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,698 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:34897
2025-10-02 16:13:08,698 - distributed.worker - INFO -          dashboard at:           10.6.82.19:40167
2025-10-02 16:13:08,698 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,698 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,698 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,698 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,698 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-13pidk5_
2025-10-02 16:13:08,698 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,702 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,704 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,704 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,705 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,712 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,713 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,713 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,714 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:37913
2025-10-02 16:13:08,714 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:37913
2025-10-02 16:13:08,714 - distributed.worker - INFO -          dashboard at:           10.6.82.19:33711
2025-10-02 16:13:08,714 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,714 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,714 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,714 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,714 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-c3gwb7_g
2025-10-02 16:13:08,714 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,715 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,719 - distributed.worker - INFO -       Start worker at:     tcp://10.6.82.19:44247
2025-10-02 16:13:08,719 - distributed.worker - INFO -          Listening to:     tcp://10.6.82.19:44247
2025-10-02 16:13:08,719 - distributed.worker - INFO -          dashboard at:           10.6.82.19:46449
2025-10-02 16:13:08,719 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,719 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,719 - distributed.worker - INFO -               Threads:                          2
2025-10-02 16:13:08,719 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 16:13:08,719 - distributed.worker - INFO -       Local Directory: /jobfs/151466801.gadi-pbs/dask-scratch-space/worker-xfgmeajw
2025-10-02 16:13:08,719 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,720 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,721 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,721 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,723 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,729 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,731 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,731 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,732 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,747 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,748 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,748 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,750 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,768 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,769 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,769 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,771 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,777 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,778 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,779 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,780 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,786 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,787 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,787 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,789 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,795 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,796 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,796 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,798 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,804 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,805 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,805 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,807 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,813 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,814 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,814 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,816 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,822 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,823 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,823 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,825 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,831 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,832 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,832 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,834 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,840 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,841 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,841 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,843 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,849 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,850 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,850 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,851 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,857 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,859 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,859 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,860 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,866 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,867 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,867 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,869 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,875 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,876 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,877 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,878 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:08,884 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 16:13:08,885 - distributed.worker - INFO -         Registered to:       tcp://10.6.81.1:8752
2025-10-02 16:13:08,885 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 16:13:08,887 - distributed.core - INFO - Starting established connection to tcp://10.6.81.1:8752
2025-10-02 16:13:22,762 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,762 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,762 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,763 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,763 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,763 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,763 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,763 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,763 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,763 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,763 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,764 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,763 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,764 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,764 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,764 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,764 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,765 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,765 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,765 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,765 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,765 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,765 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,765 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,765 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,766 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,766 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,766 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,766 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,766 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,766 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,766 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,766 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,766 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,766 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,766 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,766 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,766 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,766 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,766 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,766 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,766 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,766 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,766 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,767 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,767 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,767 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,767 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,767 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,767 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,767 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,766 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,767 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,767 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,767 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,767 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,767 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,767 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,767 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,767 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,767 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,767 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,767 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,768 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,768 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,768 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,768 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,768 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,768 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,768 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,768 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,768 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,768 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,768 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,768 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,769 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,769 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,768 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,769 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,769 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,770 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,770 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,768 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,770 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,768 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,768 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 16:13:22,770 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,771 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,771 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,771 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,771 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,771 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,771 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,772 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,772 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,772 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,772 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,774 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,777 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,777 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,777 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,778 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,778 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:22,781 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 16:13:27,199 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,199 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,199 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,199 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,199 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,199 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,200 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,200 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,200 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,200 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,200 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,200 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,201 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,201 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,201 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,201 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,201 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,201 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,201 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,201 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,201 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,201 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,201 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,202 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,202 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,202 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,202 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,202 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,202 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,202 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,202 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,203 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,202 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,203 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,203 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,203 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,203 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,203 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,203 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,203 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,203 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,203 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,203 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,203 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,203 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,203 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,204 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,204 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,204 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,203 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,203 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,203 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,204 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,203 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,202 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,204 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,203 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,204 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,204 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,204 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,204 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,204 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,204 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,205 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,204 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,205 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,204 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,204 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,205 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,204 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,204 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,204 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,204 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,205 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,205 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,204 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,205 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,204 - distributed.worker - INFO - Starting Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 16:13:27,205 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,205 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,205 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,205 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,206 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,206 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,206 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,206 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,206 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,206 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,207 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,207 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,207 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,208 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,208 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,208 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,208 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,208 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,208 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,208 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,208 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,208 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,208 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,208 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,209 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:27,209 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 16:13:28,660 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,660 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,660 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,660 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,660 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,660 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,660 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,660 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,661 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,661 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,661 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,661 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,661 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,661 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,661 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,662 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,662 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,662 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,662 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,662 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,662 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,662 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,662 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,662 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,662 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,663 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,663 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,663 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,663 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,663 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,663 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,663 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,664 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,664 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,664 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,664 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,664 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,664 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,664 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,664 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,664 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,665 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,665 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,665 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,665 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,665 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,665 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,665 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,665 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,665 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,665 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,665 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,665 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,665 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,665 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,665 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,665 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,665 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,665 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,665 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,665 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,665 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,665 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,665 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,666 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,666 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,666 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,666 - distributed.worker - INFO - Starting Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 16:13:28,666 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,666 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,666 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,667 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,667 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,667 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,667 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,667 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,667 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,667 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,667 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,667 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,667 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,668 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,668 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,668 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,668 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,668 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,668 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,668 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,669 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,669 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,669 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,669 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,669 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,669 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,669 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,670 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,670 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,670 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,671 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,671 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,671 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,671 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,671 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:28,673 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 16:13:30,080 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,081 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,081 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,081 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,081 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,081 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,081 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,081 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,081 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,081 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,082 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,082 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,082 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,082 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,082 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,082 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,082 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,082 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,083 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,083 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,083 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,083 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,083 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,083 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,083 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,083 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,083 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,083 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,083 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,083 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,084 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,084 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,084 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,084 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,084 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,084 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,084 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,084 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,084 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,084 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,084 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,084 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,084 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,084 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,084 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,084 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,084 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,084 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,085 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,085 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,085 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,085 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,085 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,085 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,085 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,085 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,085 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,085 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,085 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,085 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,085 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,085 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,085 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,085 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,085 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,085 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,085 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,086 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,086 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,086 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,086 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,086 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,086 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,086 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,086 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,086 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,086 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,086 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,086 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,086 - distributed.worker - INFO - Starting Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 16:13:30,086 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,087 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,087 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,087 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,087 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,087 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,088 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,088 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,088 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,088 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,088 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,089 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,087 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,087 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,088 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,089 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,088 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,089 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,089 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,089 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,090 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,090 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,090 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:30,094 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 16:13:50,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:13:50,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:13:50,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:13:50,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:13:50,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:13:50,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:13:50,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:13:50,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:13:50,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:13:50,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:13:50,148 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:13:50,148 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:13:50,151 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:13:50,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:13:50,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:13:50,164 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 21:12:57,929 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,929 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,929 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,929 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,929 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,929 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,929 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,929 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,929 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,929 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,929 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,929 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,929 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,929 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,929 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,930 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,929 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,930 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,929 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,930 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,930 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,930 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,930 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,930 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,930 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,930 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,930 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,930 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,930 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,930 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,930 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,930 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,930 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,930 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,930 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:43773. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,931 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:37823. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,931 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:40903. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,931 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:39197. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,931 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:46271. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,931 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:39477. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,931 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:33805. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,931 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:40823. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,931 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:42013. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,931 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:37345. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,931 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:42897. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,932 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:43953. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,929 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,930 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,932 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:37197. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,932 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:44247. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,930 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,930 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,934 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:35235. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,932 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:38211. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,930 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,930 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,932 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:37147. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,932 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:46025. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,932 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:42547. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,930 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,932 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:32965. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,932 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:44691. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,933 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:43955. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,933 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:43465. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,932 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,933 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:46177. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,933 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:36829. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,933 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:46675. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,933 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:33995. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,933 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:35327. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,933 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:34727. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,933 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:44161. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,933 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:38569. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,934 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:43263. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,934 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:46341. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,934 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:43627. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,934 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:45227. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,934 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:46273. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,934 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:42821. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,935 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:36911. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,935 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:38955. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,935 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:35755. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,935 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:39193. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,935 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:36085. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,937 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.82.19:57442 remote=tcp://10.6.81.1:8752>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 21:12:57,937 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.82.19:57488 remote=tcp://10.6.81.1:8752>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 21:12:57,937 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.82.19:57424 remote=tcp://10.6.81.1:8752>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 21:12:57,938 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.82.19:57428 remote=tcp://10.6.81.1:8752>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 21:12:57,937 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.82.19:57494 remote=tcp://10.6.81.1:8752>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 21:12:57,937 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.82.19:57480 remote=tcp://10.6.81.1:8752>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 21:12:57,937 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.82.19:57426 remote=tcp://10.6.81.1:8752>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 21:12:57,938 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.82.19:57472 remote=tcp://10.6.81.1:8752>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 21:12:57,938 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.82.19:57496 remote=tcp://10.6.81.1:8752>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 21:12:57,938 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.82.19:57416 remote=tcp://10.6.81.1:8752>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 21:12:57,950 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:37117'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,952 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:33451'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,952 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,952 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:40647'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,952 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:35107'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,952 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,952 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,953 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:43213'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,953 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,953 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,953 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,953 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,953 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:33417'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,953 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,953 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,953 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,953 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,953 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,953 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,953 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:37253'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,953 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,953 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,953 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,953 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,953 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:43649'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,953 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,953 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,954 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,954 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,954 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:36237'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,954 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,954 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,954 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,954 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:37813'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,954 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,954 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,954 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,954 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,954 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,954 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:36385'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,954 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,954 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,954 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,954 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,954 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:38749'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,954 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,954 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,955 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,954 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,955 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,955 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:38691'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,955 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,955 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,955 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,955 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,955 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,955 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,955 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,955 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,955 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,955 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,955 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,955 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,955 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,955 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,955 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,955 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,956 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,956 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,956 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,956 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,956 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,956 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,956 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,956 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,956 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,956 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,956 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,956 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,959 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,959 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,959 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,960 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,960 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,960 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,960 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,961 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,962 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,962 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,962 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:38833'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,963 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:43061'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,963 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:36395'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,963 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:42161'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,963 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,963 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:36723'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,963 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,964 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:36409'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,959 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 21:12:57,964 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,964 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,964 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:34871'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,964 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,964 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,964 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,964 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:40039'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,964 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,964 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,964 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,964 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:40343'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,964 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,964 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,964 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,964 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,964 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,964 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:35111'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,964 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,964 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,964 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,964 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,964 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,965 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:43285'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,964 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,965 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,965 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,965 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,965 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,965 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:41017'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,965 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,965 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,965 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,965 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,965 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,965 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:38739'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,965 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,965 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,965 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,965 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,965 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,965 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,965 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:45915'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,965 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,965 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,965 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,965 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,965 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,965 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,965 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,965 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:38803'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,965 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,965 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,965 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,965 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,966 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,966 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:38177'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,966 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,966 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,966 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,966 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:39169'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,966 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,966 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,966 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,966 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,966 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,966 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,966 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:40569'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,966 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,966 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,966 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,966 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,966 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,966 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,966 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:38995'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,966 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,964 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,966 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,966 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,966 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,966 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,967 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:42645'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,967 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,967 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,967 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,967 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,967 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,967 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:34229'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,967 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,967 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,967 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,967 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,967 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:41827'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,967 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,967 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,967 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,967 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,967 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,967 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,967 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:35407'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,967 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,967 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,967 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,967 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,967 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,967 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,967 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,967 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:36031'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,967 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,967 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,968 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,968 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,968 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,968 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:37799'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,968 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,968 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,968 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,968 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,968 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:45403'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,968 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,968 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,968 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,968 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,968 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:38501'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,968 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,968 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,968 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,968 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:36407'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,968 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,968 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,968 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,968 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,968 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,969 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:45211'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,969 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,969 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,969 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,969 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,969 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,969 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,969 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,969 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,969 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,969 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,969 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,969 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,969 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,969 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,969 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,969 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,969 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,969 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,969 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,969 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,969 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:57,969 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,970 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,970 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,970 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,970 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,970 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,970 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,970 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,970 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,970 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,970 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,970 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:57,970 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,967 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,970 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,970 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,970 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:57,970 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,970 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:57,971 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,971 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:57,971 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,971 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,971 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,972 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:43721. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,972 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,973 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,973 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,973 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,973 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,967 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 21:12:57,974 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,974 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,968 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 21:12:57,974 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,975 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,976 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:34289. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,976 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,976 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,976 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,976 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,976 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,976 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,976 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,977 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,977 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,978 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,978 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,965 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,983 - distributed.nanny - INFO - Worker closed
2025-10-02 21:12:57,986 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:35699. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:57,973 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:57,994 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:42225. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,016 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:58,038 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:34897. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,149 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:40875'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,150 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:58,158 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:43241. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,177 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:43333'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,184 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:40553'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,185 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:58,193 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:37913. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,194 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:46551'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,200 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:58,207 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:46061. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,208 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:58,217 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:32827. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,227 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:45851'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,256 - distributed.core - INFO - Connection to tcp://10.6.81.1:8752 has been closed.
2025-10-02 21:12:58,265 - distributed.worker - INFO - Stopping worker at tcp://10.6.82.19:37335. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,267 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 14, 9, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 21:12:58,269 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 0, 6, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 21:12:58,272 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:35679'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,288 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 13, 2, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 21:12:58,299 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:42907'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,308 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:45027'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,314 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:34403'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:58,316 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,321 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,324 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 3, 10, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 21:12:58,327 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,330 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 12, 22, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 21:12:58,332 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,335 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,337 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,340 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 17, 18, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 21:12:58,340 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,346 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,351 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,356 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,371 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,377 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,377 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,381 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 5, 13, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 21:12:58,382 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,383 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,383 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,383 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,384 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,384 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,384 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,387 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,388 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,388 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,392 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,393 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,393 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,398 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,399 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,403 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,409 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,434 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 8, 6, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 21:12:58,442 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 2, 16, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 21:12:58,447 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,453 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,458 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,463 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,468 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,504 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,509 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,512 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:58,515 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,517 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:58,520 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,523 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:58,525 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,528 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:58,533 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
2025-10-02 21:12:58,296 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.82.19:57456 remote=tcp://10.6.81.1:8752>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 21:12:59,051 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.82.19:46877'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 21:12:59,139 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 8, 3, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 21:12:59,145 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 21:12:59,150 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyb9a4401a-22a8-4719-946c-a918ac29c622
2025-10-02 21:12:59,155 - distributed.worker - INFO - Removing Worker plugin qme_vars.py57f70674-16ba-4ff8-8380-2d5627a056c7
2025-10-02 21:12:59,161 - distributed.worker - INFO - Removing Worker plugin qme_train.pyc88ab8ea-a3ba-4692-9f00-208380d5cab6
2025-10-02 21:12:59,166 - distributed.worker - INFO - Removing Worker plugin qme_apply.pyc4f9a426-ea51-4e34-b98e-ee14324ea4c4
