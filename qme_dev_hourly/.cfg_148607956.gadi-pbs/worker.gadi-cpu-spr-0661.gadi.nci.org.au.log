Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-03 10:52:50,430 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:38831'
2025-09-03 10:52:50,441 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:45749'
2025-09-03 10:52:50,447 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:39279'
2025-09-03 10:52:50,452 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:41281'
2025-09-03 10:52:50,456 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:34647'
2025-09-03 10:52:50,532 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:44057'
2025-09-03 10:52:50,535 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:40765'
2025-09-03 10:52:50,541 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:41147'
2025-09-03 10:52:50,545 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:33069'
2025-09-03 10:52:50,552 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:46591'
2025-09-03 10:52:50,556 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:46161'
2025-09-03 10:52:50,561 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:46683'
2025-09-03 10:52:50,565 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:34985'
2025-09-03 10:52:50,570 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:35775'
2025-09-03 10:52:50,574 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:35203'
2025-09-03 10:52:50,579 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:45651'
2025-09-03 10:52:50,583 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:43083'
2025-09-03 10:52:50,588 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:46739'
2025-09-03 10:52:50,597 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:41057'
2025-09-03 10:52:50,601 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:44341'
2025-09-03 10:52:50,605 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:46349'
2025-09-03 10:52:50,610 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:46237'
2025-09-03 10:52:50,615 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:44253'
2025-09-03 10:52:50,619 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:35515'
2025-09-03 10:52:50,624 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:46197'
2025-09-03 10:52:50,628 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:35403'
2025-09-03 10:52:50,632 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:39213'
2025-09-03 10:52:50,637 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:36765'
2025-09-03 10:52:50,642 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:36917'
2025-09-03 10:52:50,646 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:42969'
2025-09-03 10:52:50,651 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:44125'
2025-09-03 10:52:50,656 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:42827'
2025-09-03 10:52:50,660 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:32991'
2025-09-03 10:52:50,665 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:35345'
2025-09-03 10:52:50,669 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:42591'
2025-09-03 10:52:50,674 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:33151'
2025-09-03 10:52:50,679 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:36759'
2025-09-03 10:52:50,683 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:34549'
2025-09-03 10:52:50,688 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:36213'
2025-09-03 10:52:50,693 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:44343'
2025-09-03 10:52:50,697 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:45673'
2025-09-03 10:52:50,701 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:44321'
2025-09-03 10:52:50,754 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:40955'
2025-09-03 10:52:50,760 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:44689'
2025-09-03 10:52:50,764 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:39795'
2025-09-03 10:52:50,768 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:40211'
2025-09-03 10:52:50,773 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:37739'
2025-09-03 10:52:50,778 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:38555'
2025-09-03 10:52:50,784 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:41799'
2025-09-03 10:52:50,788 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:44417'
2025-09-03 10:52:50,792 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:34345'
2025-09-03 10:52:50,797 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:41199'
2025-09-03 10:52:50,802 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:42617'
2025-09-03 10:52:50,806 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:33263'
2025-09-03 10:52:50,811 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:36923'
2025-09-03 10:52:50,816 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:37817'
2025-09-03 10:52:50,819 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:41591'
2025-09-03 10:52:50,826 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:39153'
2025-09-03 10:52:50,830 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:35725'
2025-09-03 10:52:50,834 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:40723'
2025-09-03 10:52:50,840 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:45147'
2025-09-03 10:52:50,844 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:45429'
2025-09-03 10:52:50,849 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:36889'
2025-09-03 10:52:50,853 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:34169'
2025-09-03 10:52:50,857 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:39503'
2025-09-03 10:52:50,861 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:33285'
2025-09-03 10:52:50,865 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:45509'
2025-09-03 10:52:50,869 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:37411'
2025-09-03 10:52:50,874 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:35731'
2025-09-03 10:52:50,877 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:45277'
2025-09-03 10:52:50,882 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:43499'
2025-09-03 10:52:50,885 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:33483'
2025-09-03 10:52:50,890 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:36429'
2025-09-03 10:52:50,893 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:40359'
2025-09-03 10:52:50,897 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:39299'
2025-09-03 10:52:50,902 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:45589'
2025-09-03 10:52:50,906 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:45363'
2025-09-03 10:52:50,910 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:40355'
2025-09-03 10:52:50,915 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:43161'
2025-09-03 10:52:50,920 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:46425'
2025-09-03 10:52:50,923 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:41041'
2025-09-03 10:52:50,928 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:38857'
2025-09-03 10:52:50,932 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:33609'
2025-09-03 10:52:50,937 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:33975'
2025-09-03 10:52:50,943 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:36635'
2025-09-03 10:52:50,961 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:35605'
2025-09-03 10:52:50,966 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:45587'
2025-09-03 10:52:50,971 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:43357'
2025-09-03 10:52:50,975 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:45675'
2025-09-03 10:52:50,981 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:44947'
2025-09-03 10:52:50,984 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:44937'
2025-09-03 10:52:50,990 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:33001'
2025-09-03 10:52:50,994 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:41447'
2025-09-03 10:52:51,000 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:40935'
2025-09-03 10:52:51,005 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:38183'
2025-09-03 10:52:52,016 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:43947'
2025-09-03 10:52:52,020 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:38105'
2025-09-03 10:52:52,025 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:42517'
2025-09-03 10:52:52,027 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:46241'
2025-09-03 10:52:52,032 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:37799'
2025-09-03 10:52:52,036 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:46693'
2025-09-03 10:52:52,039 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:36675'
2025-09-03 10:52:52,044 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:44903'
2025-09-03 10:52:52,048 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.13:46157'
2025-09-03 10:52:52,163 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:45289
2025-09-03 10:52:52,163 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:45289
2025-09-03 10:52:52,163 - distributed.worker - INFO -          dashboard at:          10.6.105.13:38991
2025-09-03 10:52:52,163 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:45503
2025-09-03 10:52:52,163 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,163 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:42337
2025-09-03 10:52:52,163 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:40265
2025-09-03 10:52:52,163 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:42733
2025-09-03 10:52:52,163 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:38141
2025-09-03 10:52:52,163 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,163 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:45503
2025-09-03 10:52:52,163 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:42337
2025-09-03 10:52:52,163 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:46071
2025-09-03 10:52:52,163 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:40265
2025-09-03 10:52:52,163 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:42733
2025-09-03 10:52:52,163 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,163 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:38141
2025-09-03 10:52:52,163 - distributed.worker - INFO -          dashboard at:          10.6.105.13:39313
2025-09-03 10:52:52,164 - distributed.worker - INFO -          dashboard at:          10.6.105.13:33625
2025-09-03 10:52:52,164 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:46071
2025-09-03 10:52:52,164 - distributed.worker - INFO -          dashboard at:          10.6.105.13:44923
2025-09-03 10:52:52,164 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,164 - distributed.worker - INFO -          dashboard at:          10.6.105.13:35155
2025-09-03 10:52:52,164 - distributed.worker - INFO -          dashboard at:          10.6.105.13:39979
2025-09-03 10:52:52,164 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,164 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,164 - distributed.worker - INFO -          dashboard at:          10.6.105.13:34871
2025-09-03 10:52:52,164 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,164 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-tu0yshoa
2025-09-03 10:52:52,164 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,164 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,164 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,164 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,164 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,164 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,164 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,164 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,164 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,164 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,164 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,164 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,164 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,164 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,164 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,164 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,164 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,164 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,164 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,164 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,164 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-z0uaqyvf
2025-09-03 10:52:52,164 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,164 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-w1aksfqw
2025-09-03 10:52:52,164 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,164 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-664tqiuv
2025-09-03 10:52:52,164 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-4dhht1nc
2025-09-03 10:52:52,164 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,164 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-tiwnrj_s
2025-09-03 10:52:52,164 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,164 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-xo42hnw2
2025-09-03 10:52:52,164 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,164 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,164 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,164 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,165 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:44345
2025-09-03 10:52:52,165 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:44345
2025-09-03 10:52:52,166 - distributed.worker - INFO -          dashboard at:          10.6.105.13:44785
2025-09-03 10:52:52,166 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,166 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,166 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,166 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,166 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ymk9w3fl
2025-09-03 10:52:52,166 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,220 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:34147
2025-09-03 10:52:52,220 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:34147
2025-09-03 10:52:52,220 - distributed.worker - INFO -          dashboard at:          10.6.105.13:34695
2025-09-03 10:52:52,220 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,220 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,220 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,220 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,220 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-d5e7esxh
2025-09-03 10:52:52,221 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,224 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:46667
2025-09-03 10:52:52,224 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:46667
2025-09-03 10:52:52,224 - distributed.worker - INFO -          dashboard at:          10.6.105.13:34819
2025-09-03 10:52:52,224 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,224 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,224 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,224 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,224 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-g0ksbb0n
2025-09-03 10:52:52,224 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,301 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:43259
2025-09-03 10:52:52,302 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:43259
2025-09-03 10:52:52,302 - distributed.worker - INFO -          dashboard at:          10.6.105.13:44191
2025-09-03 10:52:52,302 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,302 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,302 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,302 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,302 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-fygtj90l
2025-09-03 10:52:52,302 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,365 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:52,366 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,367 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,368 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:52,381 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:52,382 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,382 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,384 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:52,406 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:45309
2025-09-03 10:52:52,406 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:45309
2025-09-03 10:52:52,406 - distributed.worker - INFO -          dashboard at:          10.6.105.13:40971
2025-09-03 10:52:52,406 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,406 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,406 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,406 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,406 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-8fssneyb
2025-09-03 10:52:52,406 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,418 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:34465
2025-09-03 10:52:52,418 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:34465
2025-09-03 10:52:52,418 - distributed.worker - INFO -          dashboard at:          10.6.105.13:34387
2025-09-03 10:52:52,418 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,418 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,418 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,418 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,418 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-dnzxwl8q
2025-09-03 10:52:52,418 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,558 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:40399
2025-09-03 10:52:52,558 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:40399
2025-09-03 10:52:52,558 - distributed.worker - INFO -          dashboard at:          10.6.105.13:43167
2025-09-03 10:52:52,558 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,558 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,558 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,558 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,558 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-505iy6ih
2025-09-03 10:52:52,558 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,592 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:40135
2025-09-03 10:52:52,592 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:40135
2025-09-03 10:52:52,592 - distributed.worker - INFO -          dashboard at:          10.6.105.13:33199
2025-09-03 10:52:52,592 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,592 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,592 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,592 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,592 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-tpjyx0bi
2025-09-03 10:52:52,593 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,596 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:37857
2025-09-03 10:52:52,596 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:37857
2025-09-03 10:52:52,597 - distributed.worker - INFO -          dashboard at:          10.6.105.13:40195
2025-09-03 10:52:52,597 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,597 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,597 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,597 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,597 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-e2mq6cr7
2025-09-03 10:52:52,597 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,598 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:37555
2025-09-03 10:52:52,598 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:37555
2025-09-03 10:52:52,598 - distributed.worker - INFO -          dashboard at:          10.6.105.13:35383
2025-09-03 10:52:52,598 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,598 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,598 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,598 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,598 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-8c1kme04
2025-09-03 10:52:52,598 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,602 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:46475
2025-09-03 10:52:52,602 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:46475
2025-09-03 10:52:52,602 - distributed.worker - INFO -          dashboard at:          10.6.105.13:43257
2025-09-03 10:52:52,602 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,602 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,602 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,602 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,602 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-woet7kae
2025-09-03 10:52:52,602 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,614 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:42129
2025-09-03 10:52:52,615 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:42129
2025-09-03 10:52:52,615 - distributed.worker - INFO -          dashboard at:          10.6.105.13:46337
2025-09-03 10:52:52,615 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,615 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,615 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,615 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,615 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-s753cb51
2025-09-03 10:52:52,615 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,627 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:40997
2025-09-03 10:52:52,627 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:40997
2025-09-03 10:52:52,627 - distributed.worker - INFO -          dashboard at:          10.6.105.13:45817
2025-09-03 10:52:52,627 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,627 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,627 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,628 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,628 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-34xydkzc
2025-09-03 10:52:52,628 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,641 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:38829
2025-09-03 10:52:52,641 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:38829
2025-09-03 10:52:52,641 - distributed.worker - INFO -          dashboard at:          10.6.105.13:43359
2025-09-03 10:52:52,641 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,641 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,641 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,641 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,641 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-aovu34ug
2025-09-03 10:52:52,641 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,765 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:34657
2025-09-03 10:52:52,765 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:34657
2025-09-03 10:52:52,765 - distributed.worker - INFO -          dashboard at:          10.6.105.13:33913
2025-09-03 10:52:52,765 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,765 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,765 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,765 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,765 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-k1llvajq
2025-09-03 10:52:52,765 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,796 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:52,797 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,797 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,799 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:52,988 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:44559
2025-09-03 10:52:52,988 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:44559
2025-09-03 10:52:52,988 - distributed.worker - INFO -          dashboard at:          10.6.105.13:39193
2025-09-03 10:52:52,988 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,988 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,988 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,988 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,988 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-77z5wxp7
2025-09-03 10:52:52,988 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,992 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:36531
2025-09-03 10:52:52,992 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:36531
2025-09-03 10:52:52,992 - distributed.worker - INFO -          dashboard at:          10.6.105.13:42195
2025-09-03 10:52:52,992 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:52,992 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:52,992 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:52,992 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:52,992 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-bapc0mhs
2025-09-03 10:52:52,992 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,043 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:46533
2025-09-03 10:52:53,043 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:46533
2025-09-03 10:52:53,043 - distributed.worker - INFO -          dashboard at:          10.6.105.13:41827
2025-09-03 10:52:53,043 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,043 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,043 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,043 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,043 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-u72acg1z
2025-09-03 10:52:53,043 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,061 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:33129
2025-09-03 10:52:53,061 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:33129
2025-09-03 10:52:53,061 - distributed.worker - INFO -          dashboard at:          10.6.105.13:34759
2025-09-03 10:52:53,061 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,061 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,061 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,061 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,061 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-zyeghvy4
2025-09-03 10:52:53,061 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,061 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:34401
2025-09-03 10:52:53,061 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:34401
2025-09-03 10:52:53,061 - distributed.worker - INFO -          dashboard at:          10.6.105.13:32875
2025-09-03 10:52:53,062 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,062 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,062 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,062 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,062 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-rkdssk6y
2025-09-03 10:52:53,062 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,076 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:36551
2025-09-03 10:52:53,077 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:36551
2025-09-03 10:52:53,077 - distributed.worker - INFO -          dashboard at:          10.6.105.13:34551
2025-09-03 10:52:53,077 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,077 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,077 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,077 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,077 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ukkjfblv
2025-09-03 10:52:53,077 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,099 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:39891
2025-09-03 10:52:53,099 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:39891
2025-09-03 10:52:53,099 - distributed.worker - INFO -          dashboard at:          10.6.105.13:39081
2025-09-03 10:52:53,099 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,099 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,099 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,099 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,099 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-9xe1eu2c
2025-09-03 10:52:53,099 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,130 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:39415
2025-09-03 10:52:53,130 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:39415
2025-09-03 10:52:53,130 - distributed.worker - INFO -          dashboard at:          10.6.105.13:46675
2025-09-03 10:52:53,130 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,130 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,130 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,130 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,130 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-vkxbmiev
2025-09-03 10:52:53,130 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,166 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:37073
2025-09-03 10:52:53,166 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:37073
2025-09-03 10:52:53,166 - distributed.worker - INFO -          dashboard at:          10.6.105.13:36093
2025-09-03 10:52:53,166 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,167 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,167 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,167 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,167 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-igua3x7n
2025-09-03 10:52:53,167 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,289 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:39893
2025-09-03 10:52:53,289 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:39893
2025-09-03 10:52:53,289 - distributed.worker - INFO -          dashboard at:          10.6.105.13:33941
2025-09-03 10:52:53,289 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,289 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,289 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,289 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,289 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-436qbtu_
2025-09-03 10:52:53,289 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,295 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:40875
2025-09-03 10:52:53,295 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:40875
2025-09-03 10:52:53,295 - distributed.worker - INFO -          dashboard at:          10.6.105.13:38147
2025-09-03 10:52:53,295 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,295 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,295 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,295 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,295 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-m727qr99
2025-09-03 10:52:53,295 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,328 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:39841
2025-09-03 10:52:53,328 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:39841
2025-09-03 10:52:53,328 - distributed.worker - INFO -          dashboard at:          10.6.105.13:43473
2025-09-03 10:52:53,328 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,328 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,328 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,328 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,328 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-9_gxtifc
2025-09-03 10:52:53,328 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,329 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:43733
2025-09-03 10:52:53,329 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:43733
2025-09-03 10:52:53,329 - distributed.worker - INFO -          dashboard at:          10.6.105.13:38103
2025-09-03 10:52:53,329 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,329 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,329 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,329 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,329 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-5yzv3v2o
2025-09-03 10:52:53,329 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,339 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:35015
2025-09-03 10:52:53,339 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:35015
2025-09-03 10:52:53,339 - distributed.worker - INFO -          dashboard at:          10.6.105.13:42289
2025-09-03 10:52:53,339 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,339 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,339 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,339 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,339 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-i4yo9mho
2025-09-03 10:52:53,339 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,384 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:38345
2025-09-03 10:52:53,384 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:38345
2025-09-03 10:52:53,384 - distributed.worker - INFO -          dashboard at:          10.6.105.13:33243
2025-09-03 10:52:53,384 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,384 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,384 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,384 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,384 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-pd6muqb0
2025-09-03 10:52:53,384 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,394 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,395 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,396 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,397 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,412 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,413 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,413 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,415 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,429 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,430 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,430 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,432 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,445 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,446 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,446 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,448 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,467 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:34301
2025-09-03 10:52:53,467 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:34301
2025-09-03 10:52:53,467 - distributed.worker - INFO -          dashboard at:          10.6.105.13:44837
2025-09-03 10:52:53,467 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,467 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,467 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,467 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,467 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-in53lgl_
2025-09-03 10:52:53,467 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,471 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:40013
2025-09-03 10:52:53,471 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:40013
2025-09-03 10:52:53,471 - distributed.worker - INFO -          dashboard at:          10.6.105.13:40517
2025-09-03 10:52:53,471 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,471 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,471 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,471 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,471 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-hq71gjr6
2025-09-03 10:52:53,471 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,477 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,478 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,479 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,480 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,493 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,494 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,494 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,494 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,541 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,542 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,542 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,544 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,557 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:53,559 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,559 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,560 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:53,602 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:46177
2025-09-03 10:52:53,602 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:46177
2025-09-03 10:52:53,602 - distributed.worker - INFO -          dashboard at:          10.6.105.13:35807
2025-09-03 10:52:53,602 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,603 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,603 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,603 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,603 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-k_bmqzap
2025-09-03 10:52:53,603 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,682 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:42245
2025-09-03 10:52:53,682 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:42245
2025-09-03 10:52:53,683 - distributed.worker - INFO -          dashboard at:          10.6.105.13:36129
2025-09-03 10:52:53,683 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,683 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,683 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,683 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,683 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-f12sq052
2025-09-03 10:52:53,683 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,737 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:46759
2025-09-03 10:52:53,737 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:46759
2025-09-03 10:52:53,737 - distributed.worker - INFO -          dashboard at:          10.6.105.13:33331
2025-09-03 10:52:53,737 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,738 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,738 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,738 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,738 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-oy5q8uty
2025-09-03 10:52:53,738 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,751 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:36193
2025-09-03 10:52:53,751 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:36193
2025-09-03 10:52:53,751 - distributed.worker - INFO -          dashboard at:          10.6.105.13:38461
2025-09-03 10:52:53,751 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,751 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,751 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,751 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,751 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-9_rbyxvk
2025-09-03 10:52:53,751 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,775 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:41143
2025-09-03 10:52:53,775 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:41143
2025-09-03 10:52:53,775 - distributed.worker - INFO -          dashboard at:          10.6.105.13:35929
2025-09-03 10:52:53,775 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,775 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,775 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,775 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,775 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-hrektdfv
2025-09-03 10:52:53,775 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,779 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:43853
2025-09-03 10:52:53,779 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:43853
2025-09-03 10:52:53,779 - distributed.worker - INFO -          dashboard at:          10.6.105.13:40251
2025-09-03 10:52:53,779 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,779 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,779 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,779 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,779 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-bvjc75an
2025-09-03 10:52:53,779 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,800 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:39261
2025-09-03 10:52:53,800 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:39261
2025-09-03 10:52:53,800 - distributed.worker - INFO -          dashboard at:          10.6.105.13:46299
2025-09-03 10:52:53,800 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,800 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,800 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,800 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,800 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-p8hcl7bg
2025-09-03 10:52:53,800 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,809 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:33031
2025-09-03 10:52:53,809 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:33031
2025-09-03 10:52:53,809 - distributed.worker - INFO -          dashboard at:          10.6.105.13:46175
2025-09-03 10:52:53,809 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,809 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,809 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,809 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,809 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-kcr7t2ly
2025-09-03 10:52:53,810 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,813 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:36267
2025-09-03 10:52:53,813 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:36267
2025-09-03 10:52:53,813 - distributed.worker - INFO -          dashboard at:          10.6.105.13:35047
2025-09-03 10:52:53,813 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,813 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,813 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,813 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,813 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-qpvib3er
2025-09-03 10:52:53,813 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,953 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:44059
2025-09-03 10:52:53,953 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:44059
2025-09-03 10:52:53,953 - distributed.worker - INFO -          dashboard at:          10.6.105.13:36255
2025-09-03 10:52:53,953 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,953 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,953 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,953 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,953 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-sqkhyjjd
2025-09-03 10:52:53,954 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,048 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,050 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,050 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,051 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,064 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,066 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,066 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,067 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,081 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:34153
2025-09-03 10:52:54,081 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:34153
2025-09-03 10:52:54,081 - distributed.worker - INFO -          dashboard at:          10.6.105.13:33637
2025-09-03 10:52:54,081 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,081 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,081 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,081 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,081 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ty_hw877
2025-09-03 10:52:54,082 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,176 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:44549
2025-09-03 10:52:54,176 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:44549
2025-09-03 10:52:54,176 - distributed.worker - INFO -          dashboard at:          10.6.105.13:46133
2025-09-03 10:52:54,176 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,176 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,176 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,176 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,176 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-1ro4hkx2
2025-09-03 10:52:54,176 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,214 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,216 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,216 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,217 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,221 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:45677
2025-09-03 10:52:54,221 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:45677
2025-09-03 10:52:54,221 - distributed.worker - INFO -          dashboard at:          10.6.105.13:45017
2025-09-03 10:52:54,221 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,221 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,221 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,221 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,221 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-_c5y346j
2025-09-03 10:52:54,221 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,226 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:41947
2025-09-03 10:52:54,226 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:41947
2025-09-03 10:52:54,226 - distributed.worker - INFO -          dashboard at:          10.6.105.13:45475
2025-09-03 10:52:54,226 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,226 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,226 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,226 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,226 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-jjce_nbg
2025-09-03 10:52:54,226 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,230 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:39063
2025-09-03 10:52:54,230 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:39063
2025-09-03 10:52:54,231 - distributed.worker - INFO -          dashboard at:          10.6.105.13:45519
2025-09-03 10:52:54,231 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,231 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,231 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,231 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,231 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ceys1j3k
2025-09-03 10:52:54,231 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,231 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,232 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,233 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,234 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,248 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,249 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,250 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,251 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,264 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,265 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,266 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,267 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,280 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,281 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,282 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,283 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,285 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:39141
2025-09-03 10:52:54,285 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:39141
2025-09-03 10:52:54,285 - distributed.worker - INFO -          dashboard at:          10.6.105.13:42673
2025-09-03 10:52:54,285 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,285 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,285 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,285 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,285 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-552qunkl
2025-09-03 10:52:54,285 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,297 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,298 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,298 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,300 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,313 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,315 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,317 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,329 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,330 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:46745
2025-09-03 10:52:54,330 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:46745
2025-09-03 10:52:54,330 - distributed.worker - INFO -          dashboard at:          10.6.105.13:33679
2025-09-03 10:52:54,330 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,330 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,330 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,331 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,331 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-yf134kwu
2025-09-03 10:52:54,331 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,331 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,331 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,331 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:43971
2025-09-03 10:52:54,331 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:43971
2025-09-03 10:52:54,331 - distributed.worker - INFO -          dashboard at:          10.6.105.13:32895
2025-09-03 10:52:54,331 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,331 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,331 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,331 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,331 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-6ee81o_3
2025-09-03 10:52:54,331 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,332 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:39537
2025-09-03 10:52:54,332 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:39537
2025-09-03 10:52:54,332 - distributed.worker - INFO -          dashboard at:          10.6.105.13:35765
2025-09-03 10:52:54,332 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,332 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,332 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,332 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,332 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,332 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-5zt94_e1
2025-09-03 10:52:54,332 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:37461
2025-09-03 10:52:54,334 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:37461
2025-09-03 10:52:54,334 - distributed.worker - INFO -          dashboard at:          10.6.105.13:33223
2025-09-03 10:52:54,334 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,334 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,334 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,334 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-y5ff5jlu
2025-09-03 10:52:54,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,336 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:43203
2025-09-03 10:52:54,336 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:43203
2025-09-03 10:52:54,336 - distributed.worker - INFO -          dashboard at:          10.6.105.13:42399
2025-09-03 10:52:54,336 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,336 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,336 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,336 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,336 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-etcszvtx
2025-09-03 10:52:54,336 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,355 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:43567
2025-09-03 10:52:54,355 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:43567
2025-09-03 10:52:54,355 - distributed.worker - INFO -          dashboard at:          10.6.105.13:41819
2025-09-03 10:52:54,355 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:40641
2025-09-03 10:52:54,355 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,355 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,355 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:40641
2025-09-03 10:52:54,355 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,355 - distributed.worker - INFO -          dashboard at:          10.6.105.13:34495
2025-09-03 10:52:54,355 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,355 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,355 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-j3zvvs6z
2025-09-03 10:52:54,355 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,355 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,355 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,355 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,355 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-4c6ka9tf
2025-09-03 10:52:54,355 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,357 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:46115
2025-09-03 10:52:54,357 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:46115
2025-09-03 10:52:54,357 - distributed.worker - INFO -          dashboard at:          10.6.105.13:37763
2025-09-03 10:52:54,357 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,357 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,357 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,357 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,357 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-hrbjue66
2025-09-03 10:52:54,357 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:44247
2025-09-03 10:52:54,357 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,357 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:44247
2025-09-03 10:52:54,357 - distributed.worker - INFO -          dashboard at:          10.6.105.13:46183
2025-09-03 10:52:54,357 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,357 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,357 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,357 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,357 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ge7xftk3
2025-09-03 10:52:54,357 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,359 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:42587
2025-09-03 10:52:54,360 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:42587
2025-09-03 10:52:54,360 - distributed.worker - INFO -          dashboard at:          10.6.105.13:38719
2025-09-03 10:52:54,360 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,360 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,360 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,360 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,360 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-stebdx25
2025-09-03 10:52:54,360 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,362 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:46511
2025-09-03 10:52:54,363 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:46511
2025-09-03 10:52:54,363 - distributed.worker - INFO -          dashboard at:          10.6.105.13:44851
2025-09-03 10:52:54,363 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,363 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,363 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,363 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,363 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-1dodjspw
2025-09-03 10:52:54,363 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,363 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:35641
2025-09-03 10:52:54,363 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:35641
2025-09-03 10:52:54,363 - distributed.worker - INFO -          dashboard at:          10.6.105.13:39243
2025-09-03 10:52:54,363 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,363 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,363 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:38377
2025-09-03 10:52:54,363 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,363 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,363 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:38377
2025-09-03 10:52:54,363 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-344pztz8
2025-09-03 10:52:54,363 - distributed.worker - INFO -          dashboard at:          10.6.105.13:44363
2025-09-03 10:52:54,363 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,363 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,363 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,364 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,364 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,364 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ur4vzaj1
2025-09-03 10:52:54,364 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,370 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:45105
2025-09-03 10:52:54,371 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:45105
2025-09-03 10:52:54,371 - distributed.worker - INFO -          dashboard at:          10.6.105.13:34627
2025-09-03 10:52:54,371 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,371 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,371 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,371 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,371 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-vdvneg16
2025-09-03 10:52:54,371 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,376 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:42293
2025-09-03 10:52:54,376 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:42293
2025-09-03 10:52:54,376 - distributed.worker - INFO -          dashboard at:          10.6.105.13:36375
2025-09-03 10:52:54,376 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,376 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,376 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,376 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,376 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-15517_7g
2025-09-03 10:52:54,376 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,378 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:35853
2025-09-03 10:52:54,378 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:35853
2025-09-03 10:52:54,378 - distributed.worker - INFO -          dashboard at:          10.6.105.13:42663
2025-09-03 10:52:54,378 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,378 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,378 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,378 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,378 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ar2x8cub
2025-09-03 10:52:54,378 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,379 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:42915
2025-09-03 10:52:54,379 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:42915
2025-09-03 10:52:54,379 - distributed.worker - INFO -          dashboard at:          10.6.105.13:34337
2025-09-03 10:52:54,379 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,379 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,379 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,379 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,379 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-2iqcg5i_
2025-09-03 10:52:54,379 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,381 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:35525
2025-09-03 10:52:54,382 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:35525
2025-09-03 10:52:54,382 - distributed.worker - INFO -          dashboard at:          10.6.105.13:43935
2025-09-03 10:52:54,382 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,382 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,382 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,382 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,382 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-p578kb0a
2025-09-03 10:52:54,382 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,382 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:44043
2025-09-03 10:52:54,382 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:44043
2025-09-03 10:52:54,382 - distributed.worker - INFO -          dashboard at:          10.6.105.13:42251
2025-09-03 10:52:54,382 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,382 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,382 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,382 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,382 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-9csh9wqr
2025-09-03 10:52:54,382 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,382 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:43995
2025-09-03 10:52:54,382 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:43995
2025-09-03 10:52:54,382 - distributed.worker - INFO -          dashboard at:          10.6.105.13:37635
2025-09-03 10:52:54,382 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,383 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,383 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,383 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,383 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-zrvsgh2f
2025-09-03 10:52:54,383 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,383 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:41111
2025-09-03 10:52:54,384 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:41111
2025-09-03 10:52:54,384 - distributed.worker - INFO -          dashboard at:          10.6.105.13:37407
2025-09-03 10:52:54,384 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,384 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,384 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,384 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,384 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-luw35eu0
2025-09-03 10:52:54,384 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,385 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:44857
2025-09-03 10:52:54,385 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:44857
2025-09-03 10:52:54,385 - distributed.worker - INFO -          dashboard at:          10.6.105.13:38505
2025-09-03 10:52:54,385 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,385 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,385 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,385 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,385 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-7el8q6f0
2025-09-03 10:52:54,385 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,391 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:36337
2025-09-03 10:52:54,391 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:36337
2025-09-03 10:52:54,391 - distributed.worker - INFO -          dashboard at:          10.6.105.13:41887
2025-09-03 10:52:54,391 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,391 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,391 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,391 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,391 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-olem1lm8
2025-09-03 10:52:54,391 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,395 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:39479
2025-09-03 10:52:54,395 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:39479
2025-09-03 10:52:54,395 - distributed.worker - INFO -          dashboard at:          10.6.105.13:43511
2025-09-03 10:52:54,395 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,395 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,395 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,395 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,395 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-7hji1c4q
2025-09-03 10:52:54,395 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,396 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:40289
2025-09-03 10:52:54,396 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:40289
2025-09-03 10:52:54,396 - distributed.worker - INFO -          dashboard at:          10.6.105.13:34987
2025-09-03 10:52:54,396 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,396 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,396 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,396 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,396 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-kn9y7czl
2025-09-03 10:52:54,396 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,398 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:46251
2025-09-03 10:52:54,398 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:46251
2025-09-03 10:52:54,398 - distributed.worker - INFO -          dashboard at:          10.6.105.13:46277
2025-09-03 10:52:54,398 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,398 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,398 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,398 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,398 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-brsn5fv_
2025-09-03 10:52:54,398 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,399 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:36907
2025-09-03 10:52:54,399 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:36907
2025-09-03 10:52:54,399 - distributed.worker - INFO -          dashboard at:          10.6.105.13:45875
2025-09-03 10:52:54,399 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,399 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,399 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,399 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,399 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-m0oyej6f
2025-09-03 10:52:54,399 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,404 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:46281
2025-09-03 10:52:54,405 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:46281
2025-09-03 10:52:54,405 - distributed.worker - INFO -          dashboard at:          10.6.105.13:37925
2025-09-03 10:52:54,405 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,405 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,405 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,405 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,405 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-wdvp4vgp
2025-09-03 10:52:54,405 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,406 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:40975
2025-09-03 10:52:54,406 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:40975
2025-09-03 10:52:54,406 - distributed.worker - INFO -          dashboard at:          10.6.105.13:38993
2025-09-03 10:52:54,406 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,406 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,406 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,406 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,406 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-99ett9c4
2025-09-03 10:52:54,406 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,406 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:33523
2025-09-03 10:52:54,407 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:33523
2025-09-03 10:52:54,407 - distributed.worker - INFO -          dashboard at:          10.6.105.13:37631
2025-09-03 10:52:54,407 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,407 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,407 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,407 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,407 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-6gl1r1f3
2025-09-03 10:52:54,407 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,407 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:36709
2025-09-03 10:52:54,407 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:36709
2025-09-03 10:52:54,407 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:38623
2025-09-03 10:52:54,407 - distributed.worker - INFO -          dashboard at:          10.6.105.13:43251
2025-09-03 10:52:54,407 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,407 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:38623
2025-09-03 10:52:54,407 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,407 - distributed.worker - INFO -          dashboard at:          10.6.105.13:46243
2025-09-03 10:52:54,407 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,407 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,407 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,407 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,407 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,407 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-01ib9g7w
2025-09-03 10:52:54,407 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,407 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,407 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-8rbl5edt
2025-09-03 10:52:54,407 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,407 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:42283
2025-09-03 10:52:54,408 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:42283
2025-09-03 10:52:54,408 - distributed.worker - INFO -          dashboard at:          10.6.105.13:39657
2025-09-03 10:52:54,408 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,408 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,408 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,408 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-l15ybxlc
2025-09-03 10:52:54,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,412 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:42765
2025-09-03 10:52:54,412 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:42765
2025-09-03 10:52:54,412 - distributed.worker - INFO -          dashboard at:          10.6.105.13:38599
2025-09-03 10:52:54,412 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,412 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,412 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,412 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,412 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-drmic106
2025-09-03 10:52:54,413 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,415 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:34229
2025-09-03 10:52:54,415 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:44453
2025-09-03 10:52:54,415 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:34229
2025-09-03 10:52:54,415 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:42619
2025-09-03 10:52:54,415 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:44453
2025-09-03 10:52:54,415 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:46585
2025-09-03 10:52:54,415 - distributed.worker - INFO -          dashboard at:          10.6.105.13:33141
2025-09-03 10:52:54,415 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:42619
2025-09-03 10:52:54,415 - distributed.worker - INFO -          dashboard at:          10.6.105.13:42785
2025-09-03 10:52:54,415 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,415 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:46585
2025-09-03 10:52:54,415 - distributed.worker - INFO -          dashboard at:          10.6.105.13:35833
2025-09-03 10:52:54,415 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,415 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,415 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,415 - distributed.worker - INFO -          dashboard at:          10.6.105.13:34519
2025-09-03 10:52:54,415 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,415 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,415 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,415 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,415 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,415 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,415 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,415 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,415 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,415 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,415 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,415 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-u88u5t0w
2025-09-03 10:52:54,415 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,415 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-wr0smfb9
2025-09-03 10:52:54,415 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-hzcwav1v
2025-09-03 10:52:54,415 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,415 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-g1bqxx5e
2025-09-03 10:52:54,415 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,415 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,415 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,462 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:46193
2025-09-03 10:52:54,462 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:46193
2025-09-03 10:52:54,462 - distributed.worker - INFO -          dashboard at:          10.6.105.13:46363
2025-09-03 10:52:54,462 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,462 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,462 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,462 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,462 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-_pa_o7fx
2025-09-03 10:52:54,462 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,480 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:33773
2025-09-03 10:52:54,480 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:33773
2025-09-03 10:52:54,480 - distributed.worker - INFO -          dashboard at:          10.6.105.13:44565
2025-09-03 10:52:54,480 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,480 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,480 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,480 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,480 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-e06zmutx
2025-09-03 10:52:54,480 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,530 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,531 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,531 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,533 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,575 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:35743
2025-09-03 10:52:54,575 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:35743
2025-09-03 10:52:54,576 - distributed.worker - INFO -          dashboard at:          10.6.105.13:43363
2025-09-03 10:52:54,576 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,576 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,576 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,576 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,576 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-w5vhqf_v
2025-09-03 10:52:54,576 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,581 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:40691
2025-09-03 10:52:54,581 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:40691
2025-09-03 10:52:54,581 - distributed.worker - INFO -          dashboard at:          10.6.105.13:42843
2025-09-03 10:52:54,581 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,581 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,581 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,581 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,581 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-4fbguodg
2025-09-03 10:52:54,581 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,582 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:44893
2025-09-03 10:52:54,582 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:44893
2025-09-03 10:52:54,582 - distributed.worker - INFO -          dashboard at:          10.6.105.13:34615
2025-09-03 10:52:54,582 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,582 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,582 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,582 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,582 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-i470_i1y
2025-09-03 10:52:54,582 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,586 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:35303
2025-09-03 10:52:54,586 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:35303
2025-09-03 10:52:54,586 - distributed.worker - INFO -          dashboard at:          10.6.105.13:39921
2025-09-03 10:52:54,586 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,586 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,586 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,586 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,586 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-5jvo99x1
2025-09-03 10:52:54,586 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,595 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:41521
2025-09-03 10:52:54,595 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:41521
2025-09-03 10:52:54,595 - distributed.worker - INFO -          dashboard at:          10.6.105.13:41855
2025-09-03 10:52:54,595 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,595 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,595 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,595 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,595 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-1854z74u
2025-09-03 10:52:54,595 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,598 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:43461
2025-09-03 10:52:54,598 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:43461
2025-09-03 10:52:54,598 - distributed.worker - INFO -          dashboard at:          10.6.105.13:40613
2025-09-03 10:52:54,598 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,598 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,598 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,598 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,598 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-eyr762p6
2025-09-03 10:52:54,598 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,599 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:43599
2025-09-03 10:52:54,599 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:43599
2025-09-03 10:52:54,599 - distributed.worker - INFO -          dashboard at:          10.6.105.13:42389
2025-09-03 10:52:54,599 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,599 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,600 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,600 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,600 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ik3e_ru2
2025-09-03 10:52:54,600 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,605 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:35767
2025-09-03 10:52:54,605 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:35767
2025-09-03 10:52:54,605 - distributed.worker - INFO -          dashboard at:          10.6.105.13:41839
2025-09-03 10:52:54,606 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,606 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,606 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,606 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,606 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-aawerom9
2025-09-03 10:52:54,606 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,606 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.13:43683
2025-09-03 10:52:54,606 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.13:43683
2025-09-03 10:52:54,606 - distributed.worker - INFO -          dashboard at:          10.6.105.13:36867
2025-09-03 10:52:54,606 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,606 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,606 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,606 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,606 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-mfzo3a0k
2025-09-03 10:52:54,606 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,884 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,886 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,886 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,887 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,934 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,935 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,935 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,937 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,950 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,951 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,952 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,953 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,966 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,967 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,967 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,969 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,983 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:54,984 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,984 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,986 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:54,999 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:55,000 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,001 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,002 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:55,015 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:55,017 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,017 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,019 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:55,031 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:55,033 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,033 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,035 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:55,210 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:55,212 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,212 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,214 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:55,454 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:55,456 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,456 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,458 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:55,504 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:55,506 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,506 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,509 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,274 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,275 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,275 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,277 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,307 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,308 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,310 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,323 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,324 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,325 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,326 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,340 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,341 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,341 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,343 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,423 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,424 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,424 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,426 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,456 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,457 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,458 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,459 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,472 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,473 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,473 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,475 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,488 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,490 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,490 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,492 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,521 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,523 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,523 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,525 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:56,931 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:56,932 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,932 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,934 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,013 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,014 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,014 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,016 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,029 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,030 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,031 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,033 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,046 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,047 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,047 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,049 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,062 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,064 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,064 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,066 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,079 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,080 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,080 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,082 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,156 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,158 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,158 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,160 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,173 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,175 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,175 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,177 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,190 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,191 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,191 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,193 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,223 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,225 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,225 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,227 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,240 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,241 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,241 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,243 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,256 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,257 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,257 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,259 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,272 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,274 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,274 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,276 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,289 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,290 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,290 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,292 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,306 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,308 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,310 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,323 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,324 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,324 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,326 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,339 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,341 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,341 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,343 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,355 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,357 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,357 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,359 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,372 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,373 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,373 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,375 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,404 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,405 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,405 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,407 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,437 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,439 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,439 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,441 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,455 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,456 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,457 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,458 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,472 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,473 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,474 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,475 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,506 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,507 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,509 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,522 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,524 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,526 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,538 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,540 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,540 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,542 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,572 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,573 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,573 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,575 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,589 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,590 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,590 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,592 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,605 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,606 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,607 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,609 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,622 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,623 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,623 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,625 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,638 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,639 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,639 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,641 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,655 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,656 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,656 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,658 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,671 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,672 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,672 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,675 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,688 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,689 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,690 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,691 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,816 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,817 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,817 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,819 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,833 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,834 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,834 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,836 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,850 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,851 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,851 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,853 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,866 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,868 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,868 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,870 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,704 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,706 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,706 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,707 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:02,641 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:02,643 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:02,643 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:02,645 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:04,570 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:04,572 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:04,572 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:04,573 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:07,973 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:07,974 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:07,974 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:07,976 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:10,193 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:10,195 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:10,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:10,197 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:10,210 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:10,212 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:10,212 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:10,214 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:10,227 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:10,229 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:10,229 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:10,230 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:10,244 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:10,246 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:10,246 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:10,248 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:10,261 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:10,262 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:10,263 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:10,264 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:22,767 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:22,990 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:22,993 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,044 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,062 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,063 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,078 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,100 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,131 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,781 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,801 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,810 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,815 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:23,802 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:24,434 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:24,452 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:24,463 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:24,482 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:24,483 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:24,499 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:25,537 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:25,538 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,539 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:25,540 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:25,536 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:25,890 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:25,942 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:25,956 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:25,971 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:25,990 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:26,005 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:26,461 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:26,511 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:27,279 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:27,476 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:27,517 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:27,518 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:27,518 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:27,520 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:27,528 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:27,594 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:27,596 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:27,596 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:27,598 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:27,937 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:28,180 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:28,527 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:28,547 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:28,577 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:28,596 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:28,612 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:28,626 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:28,642 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:28,661 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:28,821 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:28,839 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:28,855 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:28,871 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:29,807 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:29,808 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:29,808 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:29,810 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:29,827 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:29,828 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:29,828 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:29,830 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:32,435 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:32,437 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:32,437 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:32,438 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:32,455 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:32,456 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:32,457 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:32,459 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:33,444 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:33,445 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:33,445 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:33,447 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:33,462 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:33,463 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:33,463 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:33,465 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:33,601 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:33,603 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:33,603 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:33,605 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:33,647 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:41,232 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:41,254 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:41,266 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:46,326 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:46,328 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:46,329 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:46,330 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:46,380 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:46,382 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:46,382 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:46,384 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:46,398 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:46,399 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:46,400 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:46,401 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:46,416 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:46,417 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:46,417 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:46,419 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:46,433 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:46,434 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:46,434 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:46,436 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:56:38,107 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,109 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,169 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,173 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,184 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,188 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,248 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,253 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,435 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,441 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,690 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,691 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,695 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,696 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,699 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,700 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,750 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,755 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,758 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,759 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,096 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,100 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,355 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,360 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,401 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,402 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,580 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,585 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,950 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,956 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,115 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,117 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,787 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,789 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,149 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,154 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,187 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,192 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,230 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,233 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,236 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,241 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,482 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,487 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,953 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,954 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,965 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,972 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,282 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,283 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,572 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,573 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,748 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,749 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,780 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,782 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,040 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,041 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,174 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,181 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,215 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,217 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,433 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,434 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,701 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,702 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,705 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,711 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,900 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,906 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,071 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,072 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,121 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,123 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,153 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,154 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,339 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,341 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,397 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,398 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,726 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,727 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,747 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,749 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,806 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,812 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,833 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,835 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,045 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,052 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,235 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,240 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,339 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,344 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,507 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,509 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,936 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,941 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,318 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,319 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,439 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,441 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,552 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,554 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,556 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,558 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,499 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,504 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,681 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,682 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,132 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,133 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,210 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,212 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,363 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,368 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,425 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,431 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,474 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,479 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,565 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,566 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,642 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,644 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,080 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,082 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,141 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,145 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,161 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,163 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,439 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,443 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,743 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,749 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,809 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,810 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,106 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,107 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,117 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,121 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,475 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,476 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,536 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,537 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,890 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,892 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,124 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,129 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,248 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,253 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,311 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,312 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,475 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,477 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,572 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,578 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,864 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,865 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,886 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,891 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,933 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,940 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:53,541 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:53,546 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:54,351 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:54,353 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:54,509 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:54,511 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:55,824 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:55,831 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:56,262 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:56,267 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:56,430 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:56,431 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:56,670 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:56,672 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:57,474 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:57,476 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:57,802 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:57,803 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:58,167 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:58,172 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:58,528 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:58,530 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:58,785 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:58,792 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:58,979 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:58,983 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:59,641 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:59,645 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:59,806 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:59,811 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:00,081 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:00,087 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:00,357 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:00,362 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:00,369 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:00,374 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:01,453 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:01,458 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:02,893 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:02,895 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:02,982 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:02,984 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:04,254 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:04,259 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:04,793 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:04,799 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:09,478 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:09,480 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,001 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,003 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,025 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,027 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,031 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,033 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,334 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,336 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,339 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,340 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,341 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,342 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,363 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,365 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,366 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,367 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,368 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,368 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,494 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,496 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,514 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,516 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,519 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,522 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,527 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,527 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,529 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,529 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,617 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,620 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,622 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,624 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,637 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,640 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,639 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,641 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,665 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,667 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,744 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,747 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,877 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,878 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,879 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,880 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,891 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,894 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,115 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,117 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,163 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,164 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,165 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,166 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,261 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,263 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,274 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,275 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,275 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,276 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,277 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,282 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,288 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,290 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,296 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,298 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,311 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,313 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,317 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,321 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,380 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,382 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,382 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,384 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,405 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,407 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,409 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,411 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,413 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,415 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,419 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,421 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,473 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,475 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,492 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,494 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,506 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,508 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,515 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,518 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,531 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,533 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,536 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,538 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,538 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,540 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,565 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,567 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,575 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,575 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,577 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,577 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,596 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,598 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,599 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,600 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,607 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,607 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,609 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,608 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,609 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,611 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,625 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,627 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,633 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,634 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,635 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,636 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,636 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,638 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,833 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,836 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,857 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,859 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,026 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,031 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,141 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,143 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,146 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,153 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,159 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,161 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,181 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,183 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,203 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,208 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,213 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,217 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,245 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,249 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,250 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,252 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,269 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,271 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,355 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,357 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,368 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,370 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,382 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,386 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,447 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,449 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,575 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,577 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,586 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,588 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,589 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,591 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,648 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,650 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,668 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,670 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,692 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,694 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,700 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,702 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,706 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,708 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,746 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,749 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,754 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,755 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,819 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,821 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,917 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,919 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,951 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,953 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,053 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,055 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,100 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,102 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,130 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,132 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,169 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,175 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,175 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,177 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,298 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,303 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,384 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,386 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,689 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,690 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,691 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,691 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:14,676 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:14,679 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:14,763 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:14,765 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:14,942 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:14,947 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:15,544 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:15,546 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:15,979 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:15,981 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:16,344 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:16,346 - distributed.utils - INFO - Reload module qme_vars from .py file
