Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-05 09:11:31,992 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:37743'
2025-09-05 09:11:32,000 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:44345'
2025-09-05 09:11:32,004 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:43183'
2025-09-05 09:11:32,008 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:39213'
2025-09-05 09:11:32,012 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:36377'
2025-09-05 09:11:32,016 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:42067'
2025-09-05 09:11:32,021 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:33115'
2025-09-05 09:11:32,025 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:44527'
2025-09-05 09:11:32,030 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:35959'
2025-09-05 09:11:32,034 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:34609'
2025-09-05 09:11:32,039 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:33485'
2025-09-05 09:11:32,042 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:37001'
2025-09-05 09:11:32,046 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:37223'
2025-09-05 09:11:32,050 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:33019'
2025-09-05 09:11:32,054 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:33917'
2025-09-05 09:11:32,059 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:38829'
2025-09-05 09:11:32,064 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:40503'
2025-09-05 09:11:32,069 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:39827'
2025-09-05 09:11:32,073 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:36647'
2025-09-05 09:11:32,077 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:35675'
2025-09-05 09:11:32,080 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:37801'
2025-09-05 09:11:32,159 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:45247'
2025-09-05 09:11:32,163 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:37039'
2025-09-05 09:11:32,167 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:45249'
2025-09-05 09:11:32,173 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:46261'
2025-09-05 09:11:32,176 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:46329'
2025-09-05 09:11:32,181 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:39795'
2025-09-05 09:11:32,186 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:34789'
2025-09-05 09:11:32,190 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:33147'
2025-09-05 09:11:32,194 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:35559'
2025-09-05 09:11:32,199 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:37579'
2025-09-05 09:11:32,203 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:44885'
2025-09-05 09:11:32,207 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:45433'
2025-09-05 09:11:32,212 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:43375'
2025-09-05 09:11:32,220 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:41083'
2025-09-05 09:11:32,224 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:37277'
2025-09-05 09:11:32,229 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:36735'
2025-09-05 09:11:32,233 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:39695'
2025-09-05 09:11:32,238 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:35111'
2025-09-05 09:11:32,241 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:36185'
2025-09-05 09:11:32,246 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:42569'
2025-09-05 09:11:32,249 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:43041'
2025-09-05 09:11:32,256 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:32995'
2025-09-05 09:11:32,259 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:46309'
2025-09-05 09:11:32,264 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:45425'
2025-09-05 09:11:32,269 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:34553'
2025-09-05 09:11:32,273 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:46027'
2025-09-05 09:11:32,277 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:33369'
2025-09-05 09:11:32,282 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:39631'
2025-09-05 09:11:32,287 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:46439'
2025-09-05 09:11:32,291 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:40899'
2025-09-05 09:11:32,296 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.44:38249'
2025-09-05 09:11:33,264 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:38281
2025-09-05 09:11:33,264 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:37985
2025-09-05 09:11:33,264 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:44339
2025-09-05 09:11:33,264 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:38281
2025-09-05 09:11:33,264 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:35443
2025-09-05 09:11:33,264 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:37985
2025-09-05 09:11:33,264 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:39545
2025-09-05 09:11:33,264 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:44339
2025-09-05 09:11:33,264 - distributed.worker - INFO -          dashboard at:          10.6.101.44:40949
2025-09-05 09:11:33,264 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:35443
2025-09-05 09:11:33,264 - distributed.worker - INFO -          dashboard at:          10.6.101.44:46435
2025-09-05 09:11:33,264 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:39545
2025-09-05 09:11:33,264 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,264 - distributed.worker - INFO -          dashboard at:          10.6.101.44:45815
2025-09-05 09:11:33,264 - distributed.worker - INFO -          dashboard at:          10.6.101.44:46175
2025-09-05 09:11:33,264 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,264 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,264 - distributed.worker - INFO -          dashboard at:          10.6.101.44:46713
2025-09-05 09:11:33,264 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,264 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,264 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,264 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,264 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,264 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,264 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,264 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,264 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,264 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,264 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,264 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,264 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,264 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,264 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-8u58oa1l
2025-09-05 09:11:33,264 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,264 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,264 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-k5fnwsc9
2025-09-05 09:11:33,264 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,264 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,264 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-mkez26oc
2025-09-05 09:11:33,264 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-i2n2daw1
2025-09-05 09:11:33,264 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,264 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-3moxx1e0
2025-09-05 09:11:33,264 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,264 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,264 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,291 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,291 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,291 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,292 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,294 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,294 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:34991
2025-09-05 09:11:33,294 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:34991
2025-09-05 09:11:33,294 - distributed.worker - INFO -          dashboard at:          10.6.101.44:44467
2025-09-05 09:11:33,294 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,294 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,294 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,294 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,294 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,294 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,294 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-xuh2c2g_
2025-09-05 09:11:33,294 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,295 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,295 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,296 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,296 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,297 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:43291
2025-09-05 09:11:33,297 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:43291
2025-09-05 09:11:33,297 - distributed.worker - INFO -          dashboard at:          10.6.101.44:35307
2025-09-05 09:11:33,297 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,297 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,297 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,297 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,297 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-shprjiab
2025-09-05 09:11:33,297 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,297 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,298 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,299 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,299 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,300 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,300 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,302 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,302 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,303 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,312 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:39973
2025-09-05 09:11:33,312 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:39973
2025-09-05 09:11:33,312 - distributed.worker - INFO -          dashboard at:          10.6.101.44:43387
2025-09-05 09:11:33,313 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,313 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,313 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,313 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,313 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-ibuey1l_
2025-09-05 09:11:33,313 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,313 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:36543
2025-09-05 09:11:33,313 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:36543
2025-09-05 09:11:33,313 - distributed.worker - INFO -          dashboard at:          10.6.101.44:39553
2025-09-05 09:11:33,313 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,313 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,314 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,314 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,314 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-efrha3r5
2025-09-05 09:11:33,314 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,318 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:44349
2025-09-05 09:11:33,318 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:44349
2025-09-05 09:11:33,318 - distributed.worker - INFO -          dashboard at:          10.6.101.44:45957
2025-09-05 09:11:33,318 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,318 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,318 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,318 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,318 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-zjm7gcgx
2025-09-05 09:11:33,318 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,319 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:45639
2025-09-05 09:11:33,320 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:45639
2025-09-05 09:11:33,320 - distributed.worker - INFO -          dashboard at:          10.6.101.44:39471
2025-09-05 09:11:33,320 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,320 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,320 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,320 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,320 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-4va7r1l8
2025-09-05 09:11:33,320 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,320 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,321 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,321 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,323 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,323 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,324 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,324 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,326 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,328 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:32953
2025-09-05 09:11:33,328 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:32953
2025-09-05 09:11:33,328 - distributed.worker - INFO -          dashboard at:          10.6.101.44:32779
2025-09-05 09:11:33,328 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,328 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,328 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,328 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,328 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-yoke0p6e
2025-09-05 09:11:33,328 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,328 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:35931
2025-09-05 09:11:33,328 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:35931
2025-09-05 09:11:33,329 - distributed.worker - INFO -          dashboard at:          10.6.101.44:42543
2025-09-05 09:11:33,329 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,329 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,329 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,329 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,329 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-n6mhdt9m
2025-09-05 09:11:33,329 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,329 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:37189
2025-09-05 09:11:33,329 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:37189
2025-09-05 09:11:33,329 - distributed.worker - INFO -          dashboard at:          10.6.101.44:43273
2025-09-05 09:11:33,329 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,329 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,329 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,329 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,329 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-r482rsm3
2025-09-05 09:11:33,329 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,330 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:46775
2025-09-05 09:11:33,330 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:46775
2025-09-05 09:11:33,330 - distributed.worker - INFO -          dashboard at:          10.6.101.44:36083
2025-09-05 09:11:33,330 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,330 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,330 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,330 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,330 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-3q09tgvg
2025-09-05 09:11:33,330 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,330 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:41641
2025-09-05 09:11:33,331 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:41641
2025-09-05 09:11:33,331 - distributed.worker - INFO -          dashboard at:          10.6.101.44:44351
2025-09-05 09:11:33,331 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,331 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,331 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,331 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,331 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-al4sbu99
2025-09-05 09:11:33,331 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,334 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:37379
2025-09-05 09:11:33,334 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:37379
2025-09-05 09:11:33,334 - distributed.worker - INFO -          dashboard at:          10.6.101.44:36429
2025-09-05 09:11:33,334 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,334 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,334 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,334 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-2mj9x4c0
2025-09-05 09:11:33,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,334 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:38919
2025-09-05 09:11:33,334 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:38919
2025-09-05 09:11:33,334 - distributed.worker - INFO -          dashboard at:          10.6.101.44:45401
2025-09-05 09:11:33,334 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,334 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,334 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,334 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-1rox7df0
2025-09-05 09:11:33,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,334 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,335 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,335 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,337 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,337 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:36881
2025-09-05 09:11:33,337 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,337 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:36881
2025-09-05 09:11:33,337 - distributed.worker - INFO -          dashboard at:          10.6.101.44:42931
2025-09-05 09:11:33,337 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,337 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,337 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,337 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,337 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,337 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,337 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-8qe6n5mv
2025-09-05 09:11:33,337 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,338 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,339 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,340 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:44347
2025-09-05 09:11:33,340 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:44347
2025-09-05 09:11:33,340 - distributed.worker - INFO -          dashboard at:          10.6.101.44:40991
2025-09-05 09:11:33,340 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,340 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,340 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,340 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,340 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,340 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,340 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-vrxew3z0
2025-09-05 09:11:33,340 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,341 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,341 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,341 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,341 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,342 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,342 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:36823
2025-09-05 09:11:33,342 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:36823
2025-09-05 09:11:33,342 - distributed.worker - INFO -          dashboard at:          10.6.101.44:40671
2025-09-05 09:11:33,342 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,342 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,342 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,342 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,342 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-_kc328d5
2025-09-05 09:11:33,342 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,344 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,345 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,345 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,346 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,349 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,349 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,350 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,350 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,350 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,350 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,351 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,351 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,351 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,352 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,352 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,353 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,353 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,353 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,354 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,354 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,355 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,355 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,355 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,355 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,356 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,356 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,356 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,357 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,358 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,359 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,359 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,360 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,362 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,362 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,362 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,363 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,363 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,363 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,364 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,365 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,419 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:45955
2025-09-05 09:11:33,419 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:45955
2025-09-05 09:11:33,420 - distributed.worker - INFO -          dashboard at:          10.6.101.44:46123
2025-09-05 09:11:33,420 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,420 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,420 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,420 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,420 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-fv52z9k1
2025-09-05 09:11:33,420 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,442 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,443 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,444 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,445 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,469 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:37847
2025-09-05 09:11:33,470 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:37847
2025-09-05 09:11:33,470 - distributed.worker - INFO -          dashboard at:          10.6.101.44:44835
2025-09-05 09:11:33,470 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,470 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,470 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,470 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,470 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-d24ui9yu
2025-09-05 09:11:33,470 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,471 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:36623
2025-09-05 09:11:33,471 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:36623
2025-09-05 09:11:33,471 - distributed.worker - INFO -          dashboard at:          10.6.101.44:34721
2025-09-05 09:11:33,471 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,471 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,471 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,471 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,471 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-pni6g9jd
2025-09-05 09:11:33,471 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,491 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,492 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,492 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,494 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,495 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,496 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,496 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,498 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,547 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:37221
2025-09-05 09:11:33,548 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:37221
2025-09-05 09:11:33,548 - distributed.worker - INFO -          dashboard at:          10.6.101.44:34723
2025-09-05 09:11:33,548 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,548 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,548 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,548 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,548 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-7o3mfy73
2025-09-05 09:11:33,548 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,559 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:41377
2025-09-05 09:11:33,559 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:41377
2025-09-05 09:11:33,559 - distributed.worker - INFO -          dashboard at:          10.6.101.44:40429
2025-09-05 09:11:33,559 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,559 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,560 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,560 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,560 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-mks0d4b4
2025-09-05 09:11:33,560 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,561 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:37623
2025-09-05 09:11:33,561 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:37623
2025-09-05 09:11:33,561 - distributed.worker - INFO -          dashboard at:          10.6.101.44:44021
2025-09-05 09:11:33,561 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,561 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,561 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,561 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,561 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-bunawkky
2025-09-05 09:11:33,561 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,570 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,571 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,571 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,572 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,580 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,581 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,581 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,582 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,583 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,584 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,584 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,585 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:46617
2025-09-05 09:11:33,585 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:46617
2025-09-05 09:11:33,585 - distributed.worker - INFO -          dashboard at:          10.6.101.44:36353
2025-09-05 09:11:33,585 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,585 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,585 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,585 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,585 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-ziwvbo1z
2025-09-05 09:11:33,585 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,586 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,594 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:45107
2025-09-05 09:11:33,594 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:45107
2025-09-05 09:11:33,594 - distributed.worker - INFO -          dashboard at:          10.6.101.44:43781
2025-09-05 09:11:33,594 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,594 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,594 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,594 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,594 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-wegkggip
2025-09-05 09:11:33,594 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,595 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,596 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,596 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,596 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,606 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,607 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,607 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,607 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,754 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:38613
2025-09-05 09:11:33,754 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:38613
2025-09-05 09:11:33,754 - distributed.worker - INFO -          dashboard at:          10.6.101.44:36689
2025-09-05 09:11:33,754 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,754 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,754 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,754 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,754 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-5sqanfgp
2025-09-05 09:11:33,755 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,774 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:38789
2025-09-05 09:11:33,774 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:38789
2025-09-05 09:11:33,774 - distributed.worker - INFO -          dashboard at:          10.6.101.44:36849
2025-09-05 09:11:33,774 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,774 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,774 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,774 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,774 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-18zqvgfs
2025-09-05 09:11:33,774 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,774 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:35169
2025-09-05 09:11:33,775 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:35169
2025-09-05 09:11:33,775 - distributed.worker - INFO -          dashboard at:          10.6.101.44:46725
2025-09-05 09:11:33,775 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,775 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,775 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,775 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,775 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-sqoooopa
2025-09-05 09:11:33,775 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,778 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,779 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,779 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,781 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,784 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:45999
2025-09-05 09:11:33,784 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:45999
2025-09-05 09:11:33,784 - distributed.worker - INFO -          dashboard at:          10.6.101.44:33667
2025-09-05 09:11:33,784 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,784 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,784 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,784 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,784 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-q446qyan
2025-09-05 09:11:33,784 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,785 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:43719
2025-09-05 09:11:33,785 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:43719
2025-09-05 09:11:33,785 - distributed.worker - INFO -          dashboard at:          10.6.101.44:37363
2025-09-05 09:11:33,785 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,785 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,785 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,785 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,785 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-vi8_dfgc
2025-09-05 09:11:33,785 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,791 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:33595
2025-09-05 09:11:33,791 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:33595
2025-09-05 09:11:33,791 - distributed.worker - INFO -          dashboard at:          10.6.101.44:43255
2025-09-05 09:11:33,791 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,791 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,792 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,792 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,792 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-kxk5tkwm
2025-09-05 09:11:33,792 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,795 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:44781
2025-09-05 09:11:33,795 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:44781
2025-09-05 09:11:33,795 - distributed.worker - INFO -          dashboard at:          10.6.101.44:32777
2025-09-05 09:11:33,795 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,795 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:38605
2025-09-05 09:11:33,795 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,795 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,795 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:38605
2025-09-05 09:11:33,795 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,795 - distributed.worker - INFO -          dashboard at:          10.6.101.44:33033
2025-09-05 09:11:33,795 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-a1r0v3fw
2025-09-05 09:11:33,795 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,795 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,795 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,795 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,795 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,795 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-4q7ejh1h
2025-09-05 09:11:33,795 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,798 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,799 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,800 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,801 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,802 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,803 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,803 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,804 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,806 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:44043
2025-09-05 09:11:33,806 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:44043
2025-09-05 09:11:33,806 - distributed.worker - INFO -          dashboard at:          10.6.101.44:36399
2025-09-05 09:11:33,806 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,806 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,806 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,806 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,806 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-xbr0y3oj
2025-09-05 09:11:33,806 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,807 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,807 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:42437
2025-09-05 09:11:33,807 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:42437
2025-09-05 09:11:33,807 - distributed.worker - INFO -          dashboard at:          10.6.101.44:37711
2025-09-05 09:11:33,807 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,807 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,807 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,807 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,807 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-k42lve_b
2025-09-05 09:11:33,807 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,807 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,808 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,809 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,809 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,810 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,810 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,811 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,813 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:37211
2025-09-05 09:11:33,813 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:37211
2025-09-05 09:11:33,813 - distributed.worker - INFO -          dashboard at:          10.6.101.44:36609
2025-09-05 09:11:33,813 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,813 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,813 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,813 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,813 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-hso56h0u
2025-09-05 09:11:33,814 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,814 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,815 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:39753
2025-09-05 09:11:33,815 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,815 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:39753
2025-09-05 09:11:33,815 - distributed.worker - INFO -          dashboard at:          10.6.101.44:43099
2025-09-05 09:11:33,815 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,815 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,815 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,815 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,815 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:43491
2025-09-05 09:11:33,816 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,816 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:43491
2025-09-05 09:11:33,816 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-orr_7glw
2025-09-05 09:11:33,816 - distributed.worker - INFO -          dashboard at:          10.6.101.44:36589
2025-09-05 09:11:33,816 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,816 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,816 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,816 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,816 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,816 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-0k1au2_b
2025-09-05 09:11:33,816 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,817 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,818 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:36351
2025-09-05 09:11:33,818 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:36351
2025-09-05 09:11:33,818 - distributed.worker - INFO -          dashboard at:          10.6.101.44:44315
2025-09-05 09:11:33,818 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,818 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,818 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,818 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,818 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-zgf0eer7
2025-09-05 09:11:33,818 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,818 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:36641
2025-09-05 09:11:33,818 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:36641
2025-09-05 09:11:33,818 - distributed.worker - INFO -          dashboard at:          10.6.101.44:35153
2025-09-05 09:11:33,818 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,818 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,818 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,818 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,818 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-unqqxzrh
2025-09-05 09:11:33,818 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,821 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,822 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,822 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,823 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,823 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:35193
2025-09-05 09:11:33,824 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:35193
2025-09-05 09:11:33,824 - distributed.worker - INFO -          dashboard at:          10.6.101.44:44423
2025-09-05 09:11:33,824 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,824 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,824 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:36915
2025-09-05 09:11:33,824 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,824 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,824 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:36915
2025-09-05 09:11:33,824 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,824 - distributed.worker - INFO -          dashboard at:          10.6.101.44:39749
2025-09-05 09:11:33,824 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-wlen1z6v
2025-09-05 09:11:33,824 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,824 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,824 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,824 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,824 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,824 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,824 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-4503a0z4
2025-09-05 09:11:33,824 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,824 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,825 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,826 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:39735
2025-09-05 09:11:33,826 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:39735
2025-09-05 09:11:33,827 - distributed.worker - INFO -          dashboard at:          10.6.101.44:35221
2025-09-05 09:11:33,827 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,827 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,827 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,827 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,827 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-k5vm_37q
2025-09-05 09:11:33,827 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,828 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:42905
2025-09-05 09:11:33,828 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:42905
2025-09-05 09:11:33,828 - distributed.worker - INFO -          dashboard at:          10.6.101.44:34195
2025-09-05 09:11:33,828 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,828 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,828 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,828 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,828 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-fcx8y2ia
2025-09-05 09:11:33,828 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,829 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,830 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,830 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,830 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:33079
2025-09-05 09:11:33,831 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:33079
2025-09-05 09:11:33,831 - distributed.worker - INFO -          dashboard at:          10.6.101.44:40437
2025-09-05 09:11:33,831 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,831 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,831 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,831 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,831 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-qrduhexq
2025-09-05 09:11:33,831 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,831 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,831 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,831 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,832 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,833 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,833 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,834 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:40983
2025-09-05 09:11:33,834 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:40983
2025-09-05 09:11:33,834 - distributed.worker - INFO -          dashboard at:          10.6.101.44:37811
2025-09-05 09:11:33,834 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,834 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,834 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,834 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,834 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,834 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-ve4izark
2025-09-05 09:11:33,835 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,835 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,835 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,836 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,836 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,837 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,837 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:34791
2025-09-05 09:11:33,837 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,837 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:34791
2025-09-05 09:11:33,837 - distributed.worker - INFO -          dashboard at:          10.6.101.44:46323
2025-09-05 09:11:33,837 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,837 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,837 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,837 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.44:33741
2025-09-05 09:11:33,837 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,837 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.44:33741
2025-09-05 09:11:33,837 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-lpzzfenq
2025-09-05 09:11:33,837 - distributed.worker - INFO -          dashboard at:          10.6.101.44:41767
2025-09-05 09:11:33,837 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,837 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,837 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,837 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:33,837 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:33,837 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-6l1rnpul
2025-09-05 09:11:33,837 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,838 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,838 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,838 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,838 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,839 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,839 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,840 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,840 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,841 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,841 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,841 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,842 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,843 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,844 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,844 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,846 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,846 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,847 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,847 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,848 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,848 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,848 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,849 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,849 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,850 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,850 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,850 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,851 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,851 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,852 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,852 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,853 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,855 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,856 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,856 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,856 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,857 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,857 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,858 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:33,858 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,859 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:33,859 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:33,859 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:33,860 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:12:06,610 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,610 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,611 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,611 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,611 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,611 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,612 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,611 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,612 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,612 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,612 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,612 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,612 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,613 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,613 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,613 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,613 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,613 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,613 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,613 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,613 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,613 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,614 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,614 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,614 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,614 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,614 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,614 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,614 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,614 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,614 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,614 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,614 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,614 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,615 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,615 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,615 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,615 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,615 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,615 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,615 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,615 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,615 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,615 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,615 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,615 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,615 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,615 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,616 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,616 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,616 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,616 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,616 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,614 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,616 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,616 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,616 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,616 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,616 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,616 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,617 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,617 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,617 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,617 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,615 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,615 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,615 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,615 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,615 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,617 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,617 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,618 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,618 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,618 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,617 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,618 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,618 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,618 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,618 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,618 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,619 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,621 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,619 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,619 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,620 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,620 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,619 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,621 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,624 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,625 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,625 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,626 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,626 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,626 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,627 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,628 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,628 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,629 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,629 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,629 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,630 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,630 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,630 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,638 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:09,411 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,411 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,411 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,411 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,412 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,412 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,412 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,412 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,412 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,412 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,412 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,412 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,412 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,412 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,413 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,413 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,413 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,413 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,413 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,413 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,413 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,413 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,414 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,414 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,414 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,415 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,415 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,414 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,415 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,415 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,415 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,414 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,414 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,414 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,414 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,415 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,414 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,415 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,415 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,415 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,415 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,415 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,415 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,415 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,416 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,415 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,416 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,416 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,416 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,415 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,415 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,415 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,416 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,416 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,416 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,414 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,416 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,416 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,416 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,416 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,416 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,415 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,416 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,417 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,417 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,417 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,416 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,417 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,417 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,416 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,417 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,417 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,417 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,417 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,418 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,418 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,418 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,418 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,418 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,418 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,418 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,418 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,417 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,417 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,419 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,419 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,419 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,419 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,419 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,419 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,417 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,419 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,420 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,420 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,420 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,420 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,420 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,420 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,420 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,420 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,420 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,421 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,422 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,422 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,791 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,791 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,791 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,791 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,791 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,792 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,792 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,792 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,792 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,792 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,792 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,792 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,792 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,793 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,793 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,793 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,793 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,793 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,793 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,793 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,794 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,794 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,794 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,794 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,794 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,794 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,794 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,794 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,794 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,794 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,795 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,795 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,795 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,795 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,795 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,795 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,795 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,795 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,795 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,795 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,795 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,796 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,796 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,796 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,796 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,796 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,796 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,796 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,796 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,796 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,797 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,797 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,797 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,797 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,797 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,797 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,797 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,797 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,797 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,797 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,797 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,797 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,797 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,797 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,797 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,797 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,797 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,797 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,798 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,798 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,798 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,798 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,798 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,798 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,798 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,798 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,798 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,799 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,799 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,799 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,799 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,799 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,800 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,800 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,800 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,800 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,800 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,800 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,801 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,801 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,801 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,801 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,801 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,801 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,801 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,801 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,801 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,801 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,801 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,801 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,801 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,801 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,801 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,802 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:10,252 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,252 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,252 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,252 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,252 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,252 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,253 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,253 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,253 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,253 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,253 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,253 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,253 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,253 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,253 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,253 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,254 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,254 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,254 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,254 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,254 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,254 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,254 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,254 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,254 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,255 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,255 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,255 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,255 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,255 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,255 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,255 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,255 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,255 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,255 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,255 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,255 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,255 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,255 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,255 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,255 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,255 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,255 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,256 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,256 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,256 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,256 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,256 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,256 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,256 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,256 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,256 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,256 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,256 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,256 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,256 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,256 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,256 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,256 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,256 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,257 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,256 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,257 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,257 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,257 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,257 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,257 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,257 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,257 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,257 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,257 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,257 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,257 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,257 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,257 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,257 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,257 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,258 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,258 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,258 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,258 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,258 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,258 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,258 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,259 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,259 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,259 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,259 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,259 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,259 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,259 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,260 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,260 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,260 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,260 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,260 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,261 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,261 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,261 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,261 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,261 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,261 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,261 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,270 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:16:16,108 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,132 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,135 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,137 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:16,162 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,096 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:48,980 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:49,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:49,752 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:50,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,479 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,481 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:54,675 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:55,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:56,511 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:57,056 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:57,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,143 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,663 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:01,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:01,218 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:01,218 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:01,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:01,231 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:01,402 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:01,533 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:01,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:01,540 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:02,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:02,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:02,384 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:02,454 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:02,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:02,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:05,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:05,399 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:05,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:09,158 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:09,161 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:09,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:09,629 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:10,084 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:10,298 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:10,301 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:10,926 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:11,052 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:11,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:12,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:13,085 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:13,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:13,881 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:14,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:14,520 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:14,759 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,913 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,317 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,494 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,830 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,045 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,161 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,198 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,569 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:20,641 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:21,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:21,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:21,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:21,483 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:21,675 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:21,677 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:21,732 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:21,732 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:25,023 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:25,126 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:25,129 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:29,349 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:29,354 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:29,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:29,935 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:30,470 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:33,606 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:33,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:33,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:33,744 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:33,745 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:33,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,478 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,226 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,787 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,394 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,398 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,402 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,480 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,483 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,618 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,650 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,658 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,969 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,988 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:37,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:37,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:37,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:37,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:37,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:37,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:37,460 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,699 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,703 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:39,310 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:40,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:41,206 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:42,120 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:43,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:50,113 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:50,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:50,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:51,028 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:51,813 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,296 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,352 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,641 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,856 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,858 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,978 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,004 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,007 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,281 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,283 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,698 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,997 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,251 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:55,528 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:56,691 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:57,036 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:57,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:57,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:57,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:00,469 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:05,000 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:12,751 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:12,922 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:13,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:14,308 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:14,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:14,939 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,470 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,755 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,373 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,716 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,747 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,754 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,162 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,267 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,268 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,311 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,514 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,516 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,520 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,982 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:18,063 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:18,132 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,160 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,602 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,895 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,178 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,178 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,592 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,592 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,713 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,182 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,853 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:38,058 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:39,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:39,119 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:39,415 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:39,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:39,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:39,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:40,369 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:43,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:43,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,383 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,561 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,021 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,022 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,207 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,513 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,514 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,826 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,158 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,269 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,911 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,916 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,344 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:49,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:49,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:50,075 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:50,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:52,520 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:52,521 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:53,092 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:53,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:53,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:53,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:54,932 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:54,935 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:56,384 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:56,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:57,221 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:06,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:07,757 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:07,761 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:07,764 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:08,898 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,093 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,095 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,080 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,082 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,267 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,575 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,668 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,969 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,969 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,970 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:12,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:12,426 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:12,759 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:12,761 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:15,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:15,130 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:15,204 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:15,205 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:15,347 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:16,478 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:16,478 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:16,480 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:16,480 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:16,513 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:17,247 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:17,250 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:17,872 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:18,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:18,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:18,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:18,428 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:23,207 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:33,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:34,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:34,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:34,815 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:34,818 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:35,148 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:35,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:35,771 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:35,892 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:36,525 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:36,822 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:36,823 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:37,614 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:37,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:37,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:37,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,093 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,267 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,275 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,277 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,066 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,856 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,926 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,929 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,936 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,005 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,447 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,450 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,451 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,881 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,935 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,969 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,969 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,974 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,155 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,176 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,227 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,299 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,671 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:46,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:46,516 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,456 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,516 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,521 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,522 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,524 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,767 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,792 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,201 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,207 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,552 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,861 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:51,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:54,099 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:54,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:55,373 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:56,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:56,952 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:00,244 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:00,247 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:00,248 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:01,779 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:02,456 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:02,466 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:03,515 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:03,520 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:03,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:11,284 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,284 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,284 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,284 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,284 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,284 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,284 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,284 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,284 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,284 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,284 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,284 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,284 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,284 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,284 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,286 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:39735. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,284 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,284 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,285 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:42905. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,284 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,284 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,287 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:40983. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,286 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,284 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,287 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:37623. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,284 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,287 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:37221. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,284 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,284 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,284 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,284 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,287 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:39545. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,284 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,284 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,287 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:43491. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,287 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:44781. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,287 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:36881. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,287 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:41377. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,287 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:43291. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,288 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:34791. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,288 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:36641. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,288 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:38919. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,285 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:35193. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,288 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:43719. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,286 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,286 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:35169. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,288 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:37847. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,286 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,286 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:33079. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,289 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:39753. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,289 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:38613. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,289 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:34991. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,287 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,289 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:44339. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,289 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:38605. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,289 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:39973. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,289 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:33595. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,289 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:36351. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,290 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:36623. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,290 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:44349. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,286 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,290 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:42437. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,290 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:38789. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,290 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:45955. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,290 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:46775. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,291 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:45107. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,291 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:35931. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,291 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:41641. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,301 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:32995'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.45:40159, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.41:36685, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2617, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.49:36741, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.63:40773, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,306 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.49:33919, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,310 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:35559'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,311 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:46309'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,311 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:37579'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,312 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:34789'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,312 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2038, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,312 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3451, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,312 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:44345'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,312 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2485, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,312 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3447, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,312 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:33917'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,313 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1378, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,313 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1739, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,313 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:43041'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,313 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1389, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,313 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 4013, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,313 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.50:37433, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,313 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.66:40711, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,313 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1507, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,313 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:38249'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,314 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 5994, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,314 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:44885'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,314 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:36647'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,314 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.66:44873, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,314 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2704, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,315 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,315 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.55:36181, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,315 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,315 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:45425'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,315 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,315 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1394, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,315 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,315 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2712, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,315 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,315 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.44:32953, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,315 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,315 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,315 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1382, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,315 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,315 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:40503'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,315 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,315 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.45:41443, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,315 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2049, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,316 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.55:44019, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,316 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:36185'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,315 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 4015, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,316 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,316 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 908, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,316 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2607, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,316 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,316 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2050, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,316 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2622, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,316 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,316 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,316 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2032, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,316 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,316 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,316 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,317 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,317 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,317 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,317 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,317 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3990, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,317 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,317 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,317 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,317 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,317 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,317 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2464, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,317 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,317 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,317 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,317 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,317 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,317 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,317 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,317 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,318 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,318 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,318 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,318 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,318 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,318 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,318 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1291, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,318 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,318 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,318 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1380, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,319 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,319 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,319 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,320 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,321 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,322 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,322 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,322 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,324 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:42569'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,325 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:37039'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,325 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:46261'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,326 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:35111'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,326 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:33369'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,326 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:43375'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,326 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3248, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,326 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.64:38415, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,326 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1463, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,326 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:36735'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,327 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3477, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,327 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1859, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,327 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3786, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,327 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:37801'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,327 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1962, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,327 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:33115'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,327 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3922, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,327 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:37223'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,328 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2616, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,328 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3945, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,328 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:46027'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,328 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1795, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,328 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:33147'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,328 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:39631'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,329 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:39795'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,329 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:33019'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,329 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:45433'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,329 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3494, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,329 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.58:46017, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,329 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2935, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,329 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2855, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,329 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:35959'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,329 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2352, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,329 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1402, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,329 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2714, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,329 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3389, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,330 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.55:44231, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,330 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:45247'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,330 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2465, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,330 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2663, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,330 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.61:37255, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,330 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2697, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,330 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2482, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,330 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:41083'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,330 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.39:37771, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,330 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1832, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,330 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2658, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,330 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.55:39895, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,331 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,331 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 546, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,330 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:37001'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,331 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,330 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.37:45535, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,331 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.45:40159, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,331 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.44:37211, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,331 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,331 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,331 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 5310, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,331 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.54:40569, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,331 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:39695'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,331 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.39:38313, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,331 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2517, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,331 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,331 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,331 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.60:36717, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,331 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,331 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3961, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,331 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,331 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,331 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,331 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 5485, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,331 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:38829'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,331 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3388, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,331 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1591, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,331 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,331 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2713, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,331 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 5547, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,331 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 947, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,332 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,332 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,332 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,332 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,332 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,332 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,332 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3754, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,332 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,332 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2440, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,332 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,332 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,332 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,332 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,332 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,332 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3294, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,332 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2928, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,332 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,332 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,332 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,332 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,333 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2132, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,333 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2777, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,333 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.61:43631, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,334 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2763, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,336 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,610 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,614 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:33741. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,617 - distributed.worker - ERROR - failed during get data with tcp://10.6.101.44:33741 -> tcp://10.6.101.47:39645
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.101.44:33741 remote=tcp://10.6.101.47:51028>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-05 09:20:11,624 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.44:52236 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:11,627 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:46329'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,628 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1393, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,628 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,628 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,628 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,628 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,629 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:12,389 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:13,555 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:13,879 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:14,186 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:14,255 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:14,257 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:37211. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:14,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:14,267 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.44:52142 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.101.44:52142 remote=tcp://10.6.101.37:8753>: Stream is closed
2025-09-05 09:20:14,273 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:34553'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:14,274 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:14,274 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:14,274 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:14,274 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:14,274 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:14,281 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:14,393 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,793 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:14,798 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:37189. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:14,801 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:14,806 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.44:51914 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:14,811 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:39827'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:14,812 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1379, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:14,812 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.44:45639, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:14,812 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.38:32801, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:14,812 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.67:44853, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:14,812 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:14,813 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:14,813 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:14,813 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:14,813 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:14,836 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:43041'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,842 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:43041' closed.
2025-09-05 09:20:14,866 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:14,869 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:37379. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:14,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:14,883 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.44:51954 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:14,898 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:36377'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:14,899 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 5484, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:14,900 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:14,900 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:14,900 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:14,900 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:14,900 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:14,932 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:14,934 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:44347. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:14,935 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:14,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:14,950 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.44:51988 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:14,954 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:42067'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:14,955 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:14,955 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:14,955 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:14,955 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:14,955 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:14,962 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:14,988 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:14,990 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:36543. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:14,990 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:14,993 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:44043. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:14,992 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:14,994 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:36915. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:14,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:15,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:15,004 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:15,004 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.44:51880 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:15,008 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:34609'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,009 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:15,010 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:15,010 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:15,010 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:15,010 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:15,009 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.44:52186 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:15,009 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.44:52112 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:15,013 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:46439'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,014 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:15,014 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:45249'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,014 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:15,014 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:15,014 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:15,014 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:15,014 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:15,015 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:15,015 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:15,015 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:15,015 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:15,017 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,020 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,024 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,226 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:15,289 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:37985. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,291 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,295 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,297 - distributed.worker - ERROR - failed during get data with tcp://10.6.101.44:37985 -> tcp://10.6.101.64:40187
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.101.44:37985 remote=tcp://10.6.101.64:54854>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-05 09:20:15,303 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:15,309 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.44:51812 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:15,311 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:39213'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,312 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:15,313 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:15,313 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:15,313 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:15,313 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:15,316 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:15,323 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,548 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:15,550 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:46617. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,559 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:15,562 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:15,567 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.44:52042 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:15,572 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:37277'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,573 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:15,573 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:15,573 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:15,574 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:15,574 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:15,576 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:15,583 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,710 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,710 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,710 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,714 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,729 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,878 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:15,881 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:36823. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,882 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:15,892 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:15,897 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.44:51996 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:15,911 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:35675'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,911 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:15,911 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:15,911 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:15,911 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:15,911 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:15,914 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14afdaf7df90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:15,921 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,950 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:45433'. Reason: nanny-close-gracefully
2025-09-05 09:20:15,951 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:45433' closed.
2025-09-05 09:20:16,112 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,183 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,191 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:16,273 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:37801'. Reason: nanny-close-gracefully
2025-09-05 09:20:16,274 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:37801' closed.
2025-09-05 09:20:16,285 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:16,357 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:16,360 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:45639. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:16,374 - distributed.worker - ERROR - failed during get data with tcp://10.6.101.44:45639 -> tcp://10.6.101.44:37189
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.101.44:45639 remote=tcp://10.6.101.44:42844>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-05 09:20:16,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:16,382 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.44:51888 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:16,399 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:33485'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:16,400 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:16,400 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:16,400 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:16,400 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:16,400 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:16,404 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:16,411 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,585 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:37001'. Reason: nanny-close-gracefully
2025-09-05 09:20:16,586 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:37001' closed.
2025-09-05 09:20:16,594 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,692 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:34553'. Reason: nanny-close-gracefully
2025-09-05 09:20:16,693 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:34553' closed.
2025-09-05 09:20:16,940 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:16,965 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,021 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,024 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,028 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,231 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,295 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,299 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,316 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:17,318 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:45999. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:17,319 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:17,333 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,335 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.44:52096 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:17,339 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:40899'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:17,340 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:17,340 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:17,340 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:17,340 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:17,340 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:17,343 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:17,348 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,361 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:42067'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,362 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:42067' closed.
2025-09-05 09:20:17,375 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:46309'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,376 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:46309' closed.
2025-09-05 09:20:17,416 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,490 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:45249'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,491 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:45249' closed.
2025-09-05 09:20:17,500 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:34609'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,501 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:34609' closed.
2025-09-05 09:20:17,504 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:46439'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,505 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:46439' closed.
2025-09-05 09:20:17,587 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,695 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:37039'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,698 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:35111'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,698 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:37039' closed.
2025-09-05 09:20:17,699 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:35111' closed.
2025-09-05 09:20:17,702 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:33019'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,703 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:33019' closed.
2025-09-05 09:20:17,714 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,714 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,717 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,718 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,719 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,717 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:17,720 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:32953. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:17,726 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,727 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,730 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:17,733 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,734 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.44:51898 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:17,738 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:44527'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:17,739 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1525, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:17,739 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:17,739 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:17,739 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:17,740 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:17,740 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:17,742 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14ae5bfd9250>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:17,755 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:39213'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,756 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:39213' closed.
2025-09-05 09:20:17,784 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,785 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,855 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,925 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,951 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,951 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,059 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:37277'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,060 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:37277' closed.
2025-09-05 09:20:18,095 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:35959'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,096 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:35959' closed.
2025-09-05 09:20:18,116 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,142 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:39827'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,143 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:39827' closed.
2025-09-05 09:20:18,187 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,207 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:33917'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,208 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:43375'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,209 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:33917' closed.
2025-09-05 09:20:18,209 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:43375' closed.
2025-09-05 09:20:18,214 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:46027'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,215 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:46027' closed.
2025-09-05 09:20:18,328 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:18,331 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:35443. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,329 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:18,332 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.44:38281. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,332 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,332 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:18,343 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:18,346 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.44:51836 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:18,350 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:43183'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,348 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.44:51804 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:18,351 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:18,351 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:18,351 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:18,351 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:18,351 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:18,352 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:35675'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,353 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:35675' closed.
2025-09-05 09:20:18,354 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.44:37743'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,354 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:18,355 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:18,355 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:18,355 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:18,355 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:18,353 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1500501a0050>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:18,359 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,357 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14e9f893e690>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:18,362 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,414 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,457 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,530 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:39631'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,531 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:39631' closed.
2025-09-05 09:20:18,566 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:38249'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,568 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:38249' closed.
2025-09-05 09:20:18,599 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,635 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,791 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:33485'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,791 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:33485' closed.
2025-09-05 09:20:18,972 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:46329'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,973 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:46329' closed.
2025-09-05 09:20:18,987 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,191 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,323 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,351 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,420 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,712 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:36377'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,713 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:36377' closed.
2025-09-05 09:20:19,721 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,723 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,730 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,731 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,735 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:40899'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,736 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:40899' closed.
2025-09-05 09:20:19,788 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,789 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,814 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:36647'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,815 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:36647' closed.
2025-09-05 09:20:19,839 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,858 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,860 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,950 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,951 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,955 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,955 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,078 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,106 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,112 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:34789'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,113 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:34789' closed.
2025-09-05 09:20:20,116 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:42569'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,117 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:42569' closed.
2025-09-05 09:20:20,118 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,121 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:40503'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,123 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:40503' closed.
2025-09-05 09:20:20,152 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:45425'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,153 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:45425' closed.
2025-09-05 09:20:20,260 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:33147'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,261 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:33147' closed.
2025-09-05 09:20:20,271 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:46261'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,272 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:46261' closed.
2025-09-05 09:20:20,336 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,336 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,357 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:37579'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,358 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:37579' closed.
2025-09-05 09:20:20,362 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,365 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,391 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:37223'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,392 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:37223' closed.
2025-09-05 09:20:20,456 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:32995'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,457 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:32995' closed.
2025-09-05 09:20:20,461 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,639 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,721 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:39795'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,722 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:39795' closed.
2025-09-05 09:20:20,733 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:35559'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,734 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:35559' closed.
2025-09-05 09:20:20,758 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:43183'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,759 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:43183' closed.
2025-09-05 09:20:20,812 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:37743'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,813 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:37743' closed.
2025-09-05 09:20:20,897 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:33369'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,898 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:33369' closed.
2025-09-05 09:20:20,991 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,032 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:44345'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,033 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:44345' closed.
2025-09-05 09:20:21,194 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,385 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:39695'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,386 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:39695' closed.
2025-09-05 09:20:21,561 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:36735'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,562 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:36735' closed.
2025-09-05 09:20:21,843 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,863 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,954 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,082 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,110 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,123 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,237 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:44527'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,238 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:44527' closed.
2025-09-05 09:20:22,261 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:38829'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,261 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:38829' closed.
2025-09-05 09:20:22,348 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:33115'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,349 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:36185'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,349 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:33115' closed.
2025-09-05 09:20:22,349 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:36185' closed.
2025-09-05 09:20:22,528 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:45247'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,530 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:44885'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,530 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:45247' closed.
2025-09-05 09:20:22,531 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:44885' closed.
2025-09-05 09:20:22,545 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.44:41083'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,546 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.44:41083' closed.
2025-09-05 09:20:22,549 - distributed.dask_worker - INFO - End worker
