Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-03 10:31:56,144 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:39433'
2025-09-03 10:31:56,155 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:43759'
2025-09-03 10:31:56,160 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:33849'
2025-09-03 10:31:56,165 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:39613'
2025-09-03 10:31:56,171 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:34995'
2025-09-03 10:31:56,175 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:38861'
2025-09-03 10:31:56,179 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:46153'
2025-09-03 10:31:56,185 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:43263'
2025-09-03 10:31:56,191 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:46757'
2025-09-03 10:31:56,196 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:34053'
2025-09-03 10:31:56,202 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:33241'
2025-09-03 10:31:56,207 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:38531'
2025-09-03 10:31:56,213 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:36995'
2025-09-03 10:31:56,218 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:38983'
2025-09-03 10:31:56,221 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:40409'
2025-09-03 10:31:56,226 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:39483'
2025-09-03 10:31:56,230 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:40601'
2025-09-03 10:31:56,236 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:37095'
2025-09-03 10:31:56,271 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:42959'
2025-09-03 10:31:56,275 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:33635'
2025-09-03 10:31:56,279 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:35131'
2025-09-03 10:31:56,283 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:39675'
2025-09-03 10:31:56,288 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:42713'
2025-09-03 10:31:56,293 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:40941'
2025-09-03 10:31:56,297 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:35621'
2025-09-03 10:31:56,303 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:43185'
2025-09-03 10:31:56,308 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:32881'
2025-09-03 10:31:56,479 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:38003'
2025-09-03 10:31:57,127 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:34913
2025-09-03 10:31:57,127 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:34913
2025-09-03 10:31:57,127 - distributed.worker - INFO -          dashboard at:          10.6.102.34:40581
2025-09-03 10:31:57,127 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,127 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,127 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,127 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,127 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-k9sf9n5e
2025-09-03 10:31:57,127 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,199 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:45833
2025-09-03 10:31:57,199 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:45833
2025-09-03 10:31:57,199 - distributed.worker - INFO -          dashboard at:          10.6.102.34:41653
2025-09-03 10:31:57,199 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,199 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,199 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,199 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,199 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-yqwj5o0l
2025-09-03 10:31:57,199 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,222 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:33553
2025-09-03 10:31:57,222 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:33553
2025-09-03 10:31:57,222 - distributed.worker - INFO -          dashboard at:          10.6.102.34:44635
2025-09-03 10:31:57,222 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,223 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,223 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,223 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,223 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-l1y6u421
2025-09-03 10:31:57,223 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,224 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:45239
2025-09-03 10:31:57,224 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:45239
2025-09-03 10:31:57,224 - distributed.worker - INFO -          dashboard at:          10.6.102.34:44935
2025-09-03 10:31:57,224 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,224 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,224 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,224 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,224 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-2t3c63hx
2025-09-03 10:31:57,224 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,226 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:41709
2025-09-03 10:31:57,226 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:41709
2025-09-03 10:31:57,226 - distributed.worker - INFO -          dashboard at:          10.6.102.34:33029
2025-09-03 10:31:57,226 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,227 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,227 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,227 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,227 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-9jslw3fs
2025-09-03 10:31:57,227 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,265 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:46613
2025-09-03 10:31:57,265 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:46613
2025-09-03 10:31:57,265 - distributed.worker - INFO -          dashboard at:          10.6.102.34:40115
2025-09-03 10:31:57,265 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,265 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,265 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,265 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,265 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-2zh1j16f
2025-09-03 10:31:57,265 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,266 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:42149
2025-09-03 10:31:57,266 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:42149
2025-09-03 10:31:57,266 - distributed.worker - INFO -          dashboard at:          10.6.102.34:34683
2025-09-03 10:31:57,266 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,266 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,266 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,266 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,266 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-733garxv
2025-09-03 10:31:57,266 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,269 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:46031
2025-09-03 10:31:57,270 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:46031
2025-09-03 10:31:57,270 - distributed.worker - INFO -          dashboard at:          10.6.102.34:37973
2025-09-03 10:31:57,270 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,270 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,270 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,270 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,270 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-zjt8k8d1
2025-09-03 10:31:57,270 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,272 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:43899
2025-09-03 10:31:57,272 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:43899
2025-09-03 10:31:57,272 - distributed.worker - INFO -          dashboard at:          10.6.102.34:38147
2025-09-03 10:31:57,272 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,272 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,272 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,272 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,272 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-91dzmmr2
2025-09-03 10:31:57,272 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,275 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:37277
2025-09-03 10:31:57,275 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:37277
2025-09-03 10:31:57,275 - distributed.worker - INFO -          dashboard at:          10.6.102.34:43341
2025-09-03 10:31:57,275 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,275 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,275 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,275 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,275 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-d6hlss2f
2025-09-03 10:31:57,275 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,277 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:35447
2025-09-03 10:31:57,277 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:35447
2025-09-03 10:31:57,277 - distributed.worker - INFO -          dashboard at:          10.6.102.34:46617
2025-09-03 10:31:57,277 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,277 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,277 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,277 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,277 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-kwu4rdqe
2025-09-03 10:31:57,277 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,279 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:36313
2025-09-03 10:31:57,279 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:36313
2025-09-03 10:31:57,279 - distributed.worker - INFO -          dashboard at:          10.6.102.34:34921
2025-09-03 10:31:57,279 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,279 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,279 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,279 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,279 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ju7zneeg
2025-09-03 10:31:57,280 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,280 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:33435
2025-09-03 10:31:57,280 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:33435
2025-09-03 10:31:57,280 - distributed.worker - INFO -          dashboard at:          10.6.102.34:41967
2025-09-03 10:31:57,281 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,281 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,281 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,281 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,281 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-tex1sw_n
2025-09-03 10:31:57,281 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,282 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:35213
2025-09-03 10:31:57,282 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:35213
2025-09-03 10:31:57,282 - distributed.worker - INFO -          dashboard at:          10.6.102.34:41469
2025-09-03 10:31:57,282 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,282 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,282 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,282 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,282 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-e22ijzy2
2025-09-03 10:31:57,282 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,283 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:37395
2025-09-03 10:31:57,283 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:37395
2025-09-03 10:31:57,283 - distributed.worker - INFO -          dashboard at:          10.6.102.34:46503
2025-09-03 10:31:57,283 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,283 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,283 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,283 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,283 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-beqcz374
2025-09-03 10:31:57,283 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:36111
2025-09-03 10:31:57,283 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,283 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:36111
2025-09-03 10:31:57,283 - distributed.worker - INFO -          dashboard at:          10.6.102.34:44135
2025-09-03 10:31:57,284 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,284 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,284 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,284 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,284 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-xyciaz5u
2025-09-03 10:31:57,284 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,287 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:42269
2025-09-03 10:31:57,288 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:42269
2025-09-03 10:31:57,288 - distributed.worker - INFO -          dashboard at:          10.6.102.34:36159
2025-09-03 10:31:57,288 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,288 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,288 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,288 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,288 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-n5kcc45s
2025-09-03 10:31:57,288 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,288 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:33187
2025-09-03 10:31:57,288 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:33187
2025-09-03 10:31:57,288 - distributed.worker - INFO -          dashboard at:          10.6.102.34:43265
2025-09-03 10:31:57,288 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,288 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,288 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,288 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,288 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-91qjbqf9
2025-09-03 10:31:57,288 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,289 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:39133
2025-09-03 10:31:57,289 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:39133
2025-09-03 10:31:57,289 - distributed.worker - INFO -          dashboard at:          10.6.102.34:33009
2025-09-03 10:31:57,289 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,289 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,289 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,289 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,289 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-r29z_vq2
2025-09-03 10:31:57,289 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,308 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:34591
2025-09-03 10:31:57,308 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:34591
2025-09-03 10:31:57,308 - distributed.worker - INFO -          dashboard at:          10.6.102.34:44701
2025-09-03 10:31:57,308 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,308 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,308 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,308 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-kvenivvq
2025-09-03 10:31:57,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,308 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:43675
2025-09-03 10:31:57,309 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:43675
2025-09-03 10:31:57,309 - distributed.worker - INFO -          dashboard at:          10.6.102.34:46225
2025-09-03 10:31:57,309 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,309 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:45547
2025-09-03 10:31:57,309 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,309 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,309 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:45547
2025-09-03 10:31:57,309 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,309 - distributed.worker - INFO -          dashboard at:          10.6.102.34:40509
2025-09-03 10:31:57,309 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-p2le6rl_
2025-09-03 10:31:57,309 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,309 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,309 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,309 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,309 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,309 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-pihr9vkh
2025-09-03 10:31:57,309 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,317 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:45055
2025-09-03 10:31:57,317 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:45055
2025-09-03 10:31:57,317 - distributed.worker - INFO -          dashboard at:          10.6.102.34:39603
2025-09-03 10:31:57,317 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:39623
2025-09-03 10:31:57,317 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,317 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,317 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:39623
2025-09-03 10:31:57,317 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,317 - distributed.worker - INFO -          dashboard at:          10.6.102.34:44245
2025-09-03 10:31:57,317 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,317 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,317 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-6xiqqkaj
2025-09-03 10:31:57,317 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,317 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,317 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,317 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,317 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-8_ju7a3_
2025-09-03 10:31:57,317 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,319 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:42035
2025-09-03 10:31:57,319 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:42035
2025-09-03 10:31:57,319 - distributed.worker - INFO -          dashboard at:          10.6.102.34:45587
2025-09-03 10:31:57,319 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,320 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,320 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,320 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,320 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-3s5erzpu
2025-09-03 10:31:57,320 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,348 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:41235
2025-09-03 10:31:57,348 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:41235
2025-09-03 10:31:57,348 - distributed.worker - INFO -          dashboard at:          10.6.102.34:39405
2025-09-03 10:31:57,348 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:38533
2025-09-03 10:31:57,348 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,348 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,348 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:38533
2025-09-03 10:31:57,348 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,348 - distributed.worker - INFO -          dashboard at:          10.6.102.34:33047
2025-09-03 10:31:57,348 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,348 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,348 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-dgrjj5yf
2025-09-03 10:31:57,348 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,348 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,348 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,348 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,348 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-feo9hptm
2025-09-03 10:31:57,348 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,364 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:33711
2025-09-03 10:31:57,364 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:33711
2025-09-03 10:31:57,364 - distributed.worker - INFO -          dashboard at:          10.6.102.34:43753
2025-09-03 10:31:57,364 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,364 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,364 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:57,364 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:57,364 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-cc8z_be2
2025-09-03 10:31:57,364 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,550 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:42517'
2025-09-03 10:31:57,553 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:33069'
2025-09-03 10:31:57,557 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:32811'
2025-09-03 10:31:57,561 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:38723'
2025-09-03 10:31:57,567 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:46861'
2025-09-03 10:31:57,571 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:38809'
2025-09-03 10:31:57,575 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:40667'
2025-09-03 10:31:57,580 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:40599'
2025-09-03 10:31:57,584 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:46547'
2025-09-03 10:31:57,588 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:34245'
2025-09-03 10:31:57,594 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:38619'
2025-09-03 10:31:57,602 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:37891'
2025-09-03 10:31:57,607 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:41599'
2025-09-03 10:31:57,612 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:42771'
2025-09-03 10:31:57,617 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:37937'
2025-09-03 10:31:57,622 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:43233'
2025-09-03 10:31:57,626 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:35909'
2025-09-03 10:31:57,632 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:33725'
2025-09-03 10:31:57,637 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:35211'
2025-09-03 10:31:57,641 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:42597'
2025-09-03 10:31:57,647 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:40733'
2025-09-03 10:31:57,652 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:33247'
2025-09-03 10:31:57,656 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:42797'
2025-09-03 10:31:57,660 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:33745'
2025-09-03 10:31:57,666 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:38707'
2025-09-03 10:31:57,672 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:41577'
2025-09-03 10:31:57,676 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:36641'
2025-09-03 10:31:57,681 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:37267'
2025-09-03 10:31:57,686 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:33363'
2025-09-03 10:31:57,689 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:41671'
2025-09-03 10:31:57,695 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:33225'
2025-09-03 10:31:57,700 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:41891'
2025-09-03 10:31:57,705 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:33257'
2025-09-03 10:31:57,709 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:43113'
2025-09-03 10:31:57,713 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:37007'
2025-09-03 10:31:57,718 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:36099'
2025-09-03 10:31:57,737 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:38559'
2025-09-03 10:31:57,742 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:43851'
2025-09-03 10:31:57,745 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:41707'
2025-09-03 10:31:57,749 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:37147'
2025-09-03 10:31:57,753 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:38545'
2025-09-03 10:31:57,756 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:43465'
2025-09-03 10:31:57,761 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:43369'
2025-09-03 10:31:57,764 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:33455'
2025-09-03 10:31:57,769 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:38457'
2025-09-03 10:31:57,774 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:40777'
2025-09-03 10:31:57,778 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:39063'
2025-09-03 10:31:57,784 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:44469'
2025-09-03 10:31:57,789 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:37803'
2025-09-03 10:31:57,794 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:40535'
2025-09-03 10:31:57,799 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:43453'
2025-09-03 10:31:57,804 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:36971'
2025-09-03 10:31:57,810 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:36069'
2025-09-03 10:31:57,816 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:46415'
2025-09-03 10:31:57,820 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:38577'
2025-09-03 10:31:57,825 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:44985'
2025-09-03 10:31:57,831 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:36187'
2025-09-03 10:31:57,836 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:38227'
2025-09-03 10:31:57,840 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:34455'
2025-09-03 10:31:57,846 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:46257'
2025-09-03 10:31:57,851 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:39751'
2025-09-03 10:31:57,856 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:44313'
2025-09-03 10:31:57,861 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:38011'
2025-09-03 10:31:57,868 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:37411'
2025-09-03 10:31:57,872 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:43141'
2025-09-03 10:31:57,878 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:39761'
2025-09-03 10:31:57,882 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:45339'
2025-09-03 10:31:57,887 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:39711'
2025-09-03 10:31:57,890 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:44089'
2025-09-03 10:31:58,666 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:45305
2025-09-03 10:31:58,666 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:45305
2025-09-03 10:31:58,666 - distributed.worker - INFO -          dashboard at:          10.6.102.34:44403
2025-09-03 10:31:58,666 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,666 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,666 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,666 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,666 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-mjlzrvot
2025-09-03 10:31:58,666 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,679 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:37593
2025-09-03 10:31:58,679 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:37593
2025-09-03 10:31:58,679 - distributed.worker - INFO -          dashboard at:          10.6.102.34:36137
2025-09-03 10:31:58,679 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,679 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,679 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,679 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,679 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-v4g7kjc4
2025-09-03 10:31:58,679 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,681 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:35933
2025-09-03 10:31:58,682 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:35933
2025-09-03 10:31:58,682 - distributed.worker - INFO -          dashboard at:          10.6.102.34:42575
2025-09-03 10:31:58,682 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,682 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,682 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,682 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,682 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-212ceihu
2025-09-03 10:31:58,682 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,690 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:40827
2025-09-03 10:31:58,690 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:40827
2025-09-03 10:31:58,690 - distributed.worker - INFO -          dashboard at:          10.6.102.34:42081
2025-09-03 10:31:58,690 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,690 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,690 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,690 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,690 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-sny_o0vm
2025-09-03 10:31:58,690 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,692 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:34855
2025-09-03 10:31:58,692 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:34855
2025-09-03 10:31:58,692 - distributed.worker - INFO -          dashboard at:          10.6.102.34:39427
2025-09-03 10:31:58,692 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,692 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,692 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,692 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,692 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-0up4ch3i
2025-09-03 10:31:58,692 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,692 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:40063
2025-09-03 10:31:58,692 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:40063
2025-09-03 10:31:58,692 - distributed.worker - INFO -          dashboard at:          10.6.102.34:38175
2025-09-03 10:31:58,693 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,693 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,693 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,693 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,693 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-a8vmh1sc
2025-09-03 10:31:58,693 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,713 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:44553
2025-09-03 10:31:58,713 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:44553
2025-09-03 10:31:58,713 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:42173
2025-09-03 10:31:58,713 - distributed.worker - INFO -          dashboard at:          10.6.102.34:36619
2025-09-03 10:31:58,713 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:42173
2025-09-03 10:31:58,713 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,713 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,713 - distributed.worker - INFO -          dashboard at:          10.6.102.34:46583
2025-09-03 10:31:58,713 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,713 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,713 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,713 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,713 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:58,713 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-34y33zok
2025-09-03 10:31:58,713 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:58,713 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,713 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-lw9irqv3
2025-09-03 10:31:58,713 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,022 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:39151
2025-09-03 10:31:59,023 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:39151
2025-09-03 10:31:59,023 - distributed.worker - INFO -          dashboard at:          10.6.102.34:34471
2025-09-03 10:31:59,023 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,023 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,023 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,023 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,023 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-y1adm_h7
2025-09-03 10:31:59,023 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,061 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:35337
2025-09-03 10:31:59,062 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:35337
2025-09-03 10:31:59,062 - distributed.worker - INFO -          dashboard at:          10.6.102.34:44423
2025-09-03 10:31:59,062 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,062 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,062 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,062 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,062 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-hls52lxq
2025-09-03 10:31:59,062 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,064 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:41167
2025-09-03 10:31:59,064 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:41167
2025-09-03 10:31:59,064 - distributed.worker - INFO -          dashboard at:          10.6.102.34:32965
2025-09-03 10:31:59,064 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,064 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,064 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,064 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,064 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-6go59n2z
2025-09-03 10:31:59,064 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,094 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:36133
2025-09-03 10:31:59,095 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:36133
2025-09-03 10:31:59,095 - distributed.worker - INFO -          dashboard at:          10.6.102.34:38213
2025-09-03 10:31:59,095 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,095 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,095 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,095 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,095 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-vi5dfti5
2025-09-03 10:31:59,095 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,095 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:37917
2025-09-03 10:31:59,095 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:37917
2025-09-03 10:31:59,095 - distributed.worker - INFO -          dashboard at:          10.6.102.34:38927
2025-09-03 10:31:59,095 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,095 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,095 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,095 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,096 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-da3y8lt3
2025-09-03 10:31:59,096 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,109 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:34459
2025-09-03 10:31:59,109 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:34459
2025-09-03 10:31:59,109 - distributed.worker - INFO -          dashboard at:          10.6.102.34:39489
2025-09-03 10:31:59,109 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,109 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,109 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,109 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,109 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-2qmloxf8
2025-09-03 10:31:59,110 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,112 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:42997
2025-09-03 10:31:59,112 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:42997
2025-09-03 10:31:59,112 - distributed.worker - INFO -          dashboard at:          10.6.102.34:43051
2025-09-03 10:31:59,112 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,112 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,112 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,112 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,112 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-hthb_ha0
2025-09-03 10:31:59,112 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,115 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:46691
2025-09-03 10:31:59,115 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:46691
2025-09-03 10:31:59,115 - distributed.worker - INFO -          dashboard at:          10.6.102.34:44105
2025-09-03 10:31:59,115 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,115 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,115 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,115 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,115 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-f94x3f3w
2025-09-03 10:31:59,115 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,122 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:42151
2025-09-03 10:31:59,122 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:42151
2025-09-03 10:31:59,122 - distributed.worker - INFO -          dashboard at:          10.6.102.34:38133
2025-09-03 10:31:59,122 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,122 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,122 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,122 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,122 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-k0onxsqb
2025-09-03 10:31:59,122 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,126 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:36355
2025-09-03 10:31:59,126 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:36355
2025-09-03 10:31:59,126 - distributed.worker - INFO -          dashboard at:          10.6.102.34:39941
2025-09-03 10:31:59,126 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,126 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,126 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,126 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,126 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-wdq43146
2025-09-03 10:31:59,126 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,141 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:36841
2025-09-03 10:31:59,141 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:36841
2025-09-03 10:31:59,142 - distributed.worker - INFO -          dashboard at:          10.6.102.34:37149
2025-09-03 10:31:59,142 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,142 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,142 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,142 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,142 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-_4_l3zgv
2025-09-03 10:31:59,142 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,148 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:38583
2025-09-03 10:31:59,148 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:38583
2025-09-03 10:31:59,148 - distributed.worker - INFO -          dashboard at:          10.6.102.34:46683
2025-09-03 10:31:59,148 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,148 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,148 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,148 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,148 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-cd6097os
2025-09-03 10:31:59,148 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,171 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:44609
2025-09-03 10:31:59,171 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:44609
2025-09-03 10:31:59,171 - distributed.worker - INFO -          dashboard at:          10.6.102.34:34719
2025-09-03 10:31:59,171 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,171 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,171 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,171 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,171 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-crbrz26p
2025-09-03 10:31:59,172 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,192 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:38521
2025-09-03 10:31:59,192 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:38521
2025-09-03 10:31:59,192 - distributed.worker - INFO -          dashboard at:          10.6.102.34:42697
2025-09-03 10:31:59,192 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,192 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,192 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,192 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,192 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-6qhv9dzm
2025-09-03 10:31:59,192 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,248 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:33365
2025-09-03 10:31:59,248 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:33365
2025-09-03 10:31:59,248 - distributed.worker - INFO -          dashboard at:          10.6.102.34:34491
2025-09-03 10:31:59,248 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,248 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,248 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,248 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,248 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-jfbuijcp
2025-09-03 10:31:59,248 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,319 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:44303'
2025-09-03 10:31:59,323 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:39189'
2025-09-03 10:31:59,327 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:36299'
2025-09-03 10:31:59,332 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:40967'
2025-09-03 10:31:59,335 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:40953'
2025-09-03 10:31:59,340 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:42951'
2025-09-03 10:31:59,342 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:40111
2025-09-03 10:31:59,342 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:40111
2025-09-03 10:31:59,342 - distributed.worker - INFO -          dashboard at:          10.6.102.34:44755
2025-09-03 10:31:59,342 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,342 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,342 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,342 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,342 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-i7xemp2y
2025-09-03 10:31:59,342 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,343 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.34:40803'
2025-09-03 10:31:59,343 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:36709
2025-09-03 10:31:59,343 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:36709
2025-09-03 10:31:59,343 - distributed.worker - INFO -          dashboard at:          10.6.102.34:37585
2025-09-03 10:31:59,343 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,344 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,344 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,344 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,344 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-33i29ubh
2025-09-03 10:31:59,344 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,357 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:36195
2025-09-03 10:31:59,357 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:36195
2025-09-03 10:31:59,357 - distributed.worker - INFO -          dashboard at:          10.6.102.34:37953
2025-09-03 10:31:59,357 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,357 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,357 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,358 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,358 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-z99ca022
2025-09-03 10:31:59,358 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,360 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:39403
2025-09-03 10:31:59,360 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:39403
2025-09-03 10:31:59,361 - distributed.worker - INFO -          dashboard at:          10.6.102.34:38985
2025-09-03 10:31:59,361 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,361 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,361 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:42031
2025-09-03 10:31:59,361 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,361 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,361 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:42031
2025-09-03 10:31:59,361 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-g435pdhv
2025-09-03 10:31:59,361 - distributed.worker - INFO -          dashboard at:          10.6.102.34:46337
2025-09-03 10:31:59,361 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,361 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,361 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,361 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,361 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,361 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-3fvbvbms
2025-09-03 10:31:59,361 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,368 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:39827
2025-09-03 10:31:59,368 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:39827
2025-09-03 10:31:59,368 - distributed.worker - INFO -          dashboard at:          10.6.102.34:46539
2025-09-03 10:31:59,368 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,368 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,368 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,368 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,368 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-uddtjcy4
2025-09-03 10:31:59,368 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,378 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:44031
2025-09-03 10:31:59,378 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:44031
2025-09-03 10:31:59,378 - distributed.worker - INFO -          dashboard at:          10.6.102.34:43621
2025-09-03 10:31:59,378 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,378 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,378 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,378 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,378 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-w9xoldir
2025-09-03 10:31:59,378 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,378 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:59,379 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,379 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,381 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:59,384 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:44049
2025-09-03 10:31:59,384 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:44049
2025-09-03 10:31:59,384 - distributed.worker - INFO -          dashboard at:          10.6.102.34:41379
2025-09-03 10:31:59,384 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,384 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,384 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,384 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,384 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-lx8on8by
2025-09-03 10:31:59,384 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,385 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:38153
2025-09-03 10:31:59,385 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:38153
2025-09-03 10:31:59,385 - distributed.worker - INFO -          dashboard at:          10.6.102.34:43079
2025-09-03 10:31:59,385 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,385 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,385 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,385 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,385 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ianedavq
2025-09-03 10:31:59,386 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,387 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:43381
2025-09-03 10:31:59,387 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:43381
2025-09-03 10:31:59,387 - distributed.worker - INFO -          dashboard at:          10.6.102.34:42271
2025-09-03 10:31:59,387 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,387 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,387 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,387 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,388 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-7bmd8yhp
2025-09-03 10:31:59,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,390 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:45155
2025-09-03 10:31:59,390 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:45155
2025-09-03 10:31:59,390 - distributed.worker - INFO -          dashboard at:          10.6.102.34:40507
2025-09-03 10:31:59,390 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,390 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,390 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,390 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,390 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-eiuko0yo
2025-09-03 10:31:59,390 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,392 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:37283
2025-09-03 10:31:59,392 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:37283
2025-09-03 10:31:59,392 - distributed.worker - INFO -          dashboard at:          10.6.102.34:42861
2025-09-03 10:31:59,392 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,392 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,392 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,392 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,392 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-jsq4igoy
2025-09-03 10:31:59,392 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,394 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:45893
2025-09-03 10:31:59,394 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:45893
2025-09-03 10:31:59,394 - distributed.worker - INFO -          dashboard at:          10.6.102.34:46783
2025-09-03 10:31:59,394 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,394 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,394 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,394 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,394 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-jd25z_46
2025-09-03 10:31:59,394 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,396 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:45715
2025-09-03 10:31:59,396 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:45715
2025-09-03 10:31:59,396 - distributed.worker - INFO -          dashboard at:          10.6.102.34:46577
2025-09-03 10:31:59,396 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,396 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,396 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,396 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,396 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-8770p67g
2025-09-03 10:31:59,396 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,398 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:41433
2025-09-03 10:31:59,398 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:41433
2025-09-03 10:31:59,398 - distributed.worker - INFO -          dashboard at:          10.6.102.34:35665
2025-09-03 10:31:59,398 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,398 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,398 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,398 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,398 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-blxq3gt4
2025-09-03 10:31:59,398 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,401 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:44785
2025-09-03 10:31:59,401 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:44785
2025-09-03 10:31:59,401 - distributed.worker - INFO -          dashboard at:          10.6.102.34:46455
2025-09-03 10:31:59,401 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,401 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,401 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,401 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,401 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-301g4vdh
2025-09-03 10:31:59,401 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,403 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:34317
2025-09-03 10:31:59,403 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:34317
2025-09-03 10:31:59,403 - distributed.worker - INFO -          dashboard at:          10.6.102.34:39351
2025-09-03 10:31:59,403 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,403 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,403 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,403 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,403 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-49511zdt
2025-09-03 10:31:59,403 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,405 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:40113
2025-09-03 10:31:59,405 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:40113
2025-09-03 10:31:59,405 - distributed.worker - INFO -          dashboard at:          10.6.102.34:45627
2025-09-03 10:31:59,405 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,405 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,405 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,405 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,405 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-35etfq9a
2025-09-03 10:31:59,405 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,406 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:33647
2025-09-03 10:31:59,406 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:33647
2025-09-03 10:31:59,406 - distributed.worker - INFO -          dashboard at:          10.6.102.34:39081
2025-09-03 10:31:59,406 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,406 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,406 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,407 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,407 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-sml4afdd
2025-09-03 10:31:59,407 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,408 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:45357
2025-09-03 10:31:59,408 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:45357
2025-09-03 10:31:59,408 - distributed.worker - INFO -          dashboard at:          10.6.102.34:40149
2025-09-03 10:31:59,408 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,408 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,408 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,408 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ssz46p_9
2025-09-03 10:31:59,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,409 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:37255
2025-09-03 10:31:59,409 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:37255
2025-09-03 10:31:59,409 - distributed.worker - INFO -          dashboard at:          10.6.102.34:41503
2025-09-03 10:31:59,409 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,409 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,409 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,409 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,409 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-au_uroaf
2025-09-03 10:31:59,410 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,410 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:41047
2025-09-03 10:31:59,410 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:41047
2025-09-03 10:31:59,410 - distributed.worker - INFO -          dashboard at:          10.6.102.34:43055
2025-09-03 10:31:59,410 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,410 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,410 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,410 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,411 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-anh6jqe2
2025-09-03 10:31:59,411 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,411 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:34541
2025-09-03 10:31:59,411 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:34541
2025-09-03 10:31:59,411 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:42153
2025-09-03 10:31:59,411 - distributed.worker - INFO -          dashboard at:          10.6.102.34:42285
2025-09-03 10:31:59,411 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:42153
2025-09-03 10:31:59,411 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,412 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,412 - distributed.worker - INFO -          dashboard at:          10.6.102.34:46591
2025-09-03 10:31:59,412 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,412 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,412 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,412 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,412 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,412 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-93svj5wk
2025-09-03 10:31:59,412 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,412 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,412 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:38849
2025-09-03 10:31:59,412 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-do0kbi61
2025-09-03 10:31:59,412 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:38849
2025-09-03 10:31:59,412 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,412 - distributed.worker - INFO -          dashboard at:          10.6.102.34:39911
2025-09-03 10:31:59,412 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,412 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,412 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,412 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,412 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-yztprz21
2025-09-03 10:31:59,412 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,413 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:40665
2025-09-03 10:31:59,413 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:40665
2025-09-03 10:31:59,413 - distributed.worker - INFO -          dashboard at:          10.6.102.34:42457
2025-09-03 10:31:59,413 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,413 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:34347
2025-09-03 10:31:59,413 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,413 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,413 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:34347
2025-09-03 10:31:59,413 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,413 - distributed.worker - INFO -          dashboard at:          10.6.102.34:39153
2025-09-03 10:31:59,413 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-1wkpfqfp
2025-09-03 10:31:59,413 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,413 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,413 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,413 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,413 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,413 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-wr8d0gk3
2025-09-03 10:31:59,413 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,414 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:44517
2025-09-03 10:31:59,414 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:44517
2025-09-03 10:31:59,414 - distributed.worker - INFO -          dashboard at:          10.6.102.34:46719
2025-09-03 10:31:59,414 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,414 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,414 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,414 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,414 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-mdmvwheb
2025-09-03 10:31:59,414 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,414 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:44361
2025-09-03 10:31:59,414 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:44361
2025-09-03 10:31:59,414 - distributed.worker - INFO -          dashboard at:          10.6.102.34:34743
2025-09-03 10:31:59,414 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,414 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,414 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,414 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,414 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-3veyxne_
2025-09-03 10:31:59,414 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,415 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:42443
2025-09-03 10:31:59,415 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:42443
2025-09-03 10:31:59,415 - distributed.worker - INFO -          dashboard at:          10.6.102.34:45595
2025-09-03 10:31:59,415 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,415 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,415 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,415 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,415 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-3gpjsbib
2025-09-03 10:31:59,415 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,415 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:33721
2025-09-03 10:31:59,415 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:33721
2025-09-03 10:31:59,415 - distributed.worker - INFO -          dashboard at:          10.6.102.34:46453
2025-09-03 10:31:59,415 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,416 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,416 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,416 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,416 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-_341lvrg
2025-09-03 10:31:59,416 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:45365
2025-09-03 10:31:59,416 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,416 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:45365
2025-09-03 10:31:59,416 - distributed.worker - INFO -          dashboard at:          10.6.102.34:46237
2025-09-03 10:31:59,416 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,416 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,416 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,416 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,416 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-8ua6ieji
2025-09-03 10:31:59,416 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,416 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:36445
2025-09-03 10:31:59,416 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:36445
2025-09-03 10:31:59,416 - distributed.worker - INFO -          dashboard at:          10.6.102.34:43287
2025-09-03 10:31:59,416 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,416 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,416 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,416 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,416 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-h4fdr3xc
2025-09-03 10:31:59,416 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,417 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:33921
2025-09-03 10:31:59,417 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:33921
2025-09-03 10:31:59,417 - distributed.worker - INFO -          dashboard at:          10.6.102.34:33543
2025-09-03 10:31:59,417 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,417 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,417 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,417 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,417 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-qknzwvy5
2025-09-03 10:31:59,417 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,418 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:36345
2025-09-03 10:31:59,418 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:36345
2025-09-03 10:31:59,418 - distributed.worker - INFO -          dashboard at:          10.6.102.34:39895
2025-09-03 10:31:59,418 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,418 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,418 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,418 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,418 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-7ih2lcwc
2025-09-03 10:31:59,418 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,419 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:40043
2025-09-03 10:31:59,419 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:40043
2025-09-03 10:31:59,419 - distributed.worker - INFO -          dashboard at:          10.6.102.34:44045
2025-09-03 10:31:59,419 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,419 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,419 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,419 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,419 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-7wuq6eq8
2025-09-03 10:31:59,419 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,420 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:37991
2025-09-03 10:31:59,421 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:37991
2025-09-03 10:31:59,421 - distributed.worker - INFO -          dashboard at:          10.6.102.34:45455
2025-09-03 10:31:59,421 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,421 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,421 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,421 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,421 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-605ym5q8
2025-09-03 10:31:59,421 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,421 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:43659
2025-09-03 10:31:59,421 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:43659
2025-09-03 10:31:59,421 - distributed.worker - INFO -          dashboard at:          10.6.102.34:46105
2025-09-03 10:31:59,421 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,421 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,421 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,421 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,421 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-xx67eky6
2025-09-03 10:31:59,421 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,422 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:43715
2025-09-03 10:31:59,422 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:43715
2025-09-03 10:31:59,422 - distributed.worker - INFO -          dashboard at:          10.6.102.34:43969
2025-09-03 10:31:59,422 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,422 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,422 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,422 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,422 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-dpexiz4b
2025-09-03 10:31:59,422 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,422 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:40645
2025-09-03 10:31:59,422 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:40645
2025-09-03 10:31:59,422 - distributed.worker - INFO -          dashboard at:          10.6.102.34:38613
2025-09-03 10:31:59,422 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,422 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,422 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,422 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,422 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-p80i020t
2025-09-03 10:31:59,422 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,423 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:41361
2025-09-03 10:31:59,423 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:41361
2025-09-03 10:31:59,423 - distributed.worker - INFO -          dashboard at:          10.6.102.34:44065
2025-09-03 10:31:59,423 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,423 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,423 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,423 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,423 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-dqe1xsuq
2025-09-03 10:31:59,423 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,424 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:38145
2025-09-03 10:31:59,424 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:38145
2025-09-03 10:31:59,424 - distributed.worker - INFO -          dashboard at:          10.6.102.34:43271
2025-09-03 10:31:59,424 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,424 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,424 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,424 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,424 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-r4ioovwr
2025-09-03 10:31:59,424 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,424 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:37027
2025-09-03 10:31:59,424 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:37027
2025-09-03 10:31:59,424 - distributed.worker - INFO -          dashboard at:          10.6.102.34:40491
2025-09-03 10:31:59,424 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,424 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,424 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,425 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,425 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-zz16shju
2025-09-03 10:31:59,425 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,427 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:32841
2025-09-03 10:31:59,427 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:32841
2025-09-03 10:31:59,427 - distributed.worker - INFO -          dashboard at:          10.6.102.34:37865
2025-09-03 10:31:59,427 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,427 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,427 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,427 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,427 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-1c7rfozd
2025-09-03 10:31:59,427 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,429 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:38513
2025-09-03 10:31:59,429 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:38513
2025-09-03 10:31:59,429 - distributed.worker - INFO -          dashboard at:          10.6.102.34:38989
2025-09-03 10:31:59,429 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,429 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,429 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,429 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,429 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-uctjpkov
2025-09-03 10:31:59,429 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,450 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:37521
2025-09-03 10:31:59,450 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:37521
2025-09-03 10:31:59,450 - distributed.worker - INFO -          dashboard at:          10.6.102.34:35957
2025-09-03 10:31:59,450 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,450 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,450 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:59,450 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:59,450 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-4gab6i32
2025-09-03 10:31:59,450 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,465 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:59,466 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,466 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,468 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:59,480 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:59,481 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,482 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,483 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:59,509 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:59,511 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,511 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,512 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:59,523 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:59,525 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,525 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,526 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:59,538 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:59,539 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,539 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,541 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:59,568 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:59,569 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,569 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,570 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:59,582 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:59,583 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,584 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,585 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:59,903 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:59,904 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,905 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,906 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:00,131 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:41969
2025-09-03 10:32:00,131 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:41969
2025-09-03 10:32:00,131 - distributed.worker - INFO -          dashboard at:          10.6.102.34:44835
2025-09-03 10:32:00,132 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,132 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,132 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,132 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,132 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-awg9e8tn
2025-09-03 10:32:00,132 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,135 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:41549
2025-09-03 10:32:00,135 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:41549
2025-09-03 10:32:00,135 - distributed.worker - INFO -          dashboard at:          10.6.102.34:43727
2025-09-03 10:32:00,136 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,136 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,136 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,136 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,136 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-iftxysdl
2025-09-03 10:32:00,136 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,140 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:41407
2025-09-03 10:32:00,140 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:41407
2025-09-03 10:32:00,141 - distributed.worker - INFO -          dashboard at:          10.6.102.34:41949
2025-09-03 10:32:00,141 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,141 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,141 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,141 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,141 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-srhjl19n
2025-09-03 10:32:00,141 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,142 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:35617
2025-09-03 10:32:00,142 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:45697
2025-09-03 10:32:00,142 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:35617
2025-09-03 10:32:00,142 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:45697
2025-09-03 10:32:00,142 - distributed.worker - INFO -          dashboard at:          10.6.102.34:35387
2025-09-03 10:32:00,142 - distributed.worker - INFO -          dashboard at:          10.6.102.34:44637
2025-09-03 10:32:00,142 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,142 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,142 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,142 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,142 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,142 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,142 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,142 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,142 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-j3zd3u4c
2025-09-03 10:32:00,142 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ov_rcln9
2025-09-03 10:32:00,142 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,142 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,149 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:33011
2025-09-03 10:32:00,149 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:33011
2025-09-03 10:32:00,149 - distributed.worker - INFO -          dashboard at:          10.6.102.34:32907
2025-09-03 10:32:00,149 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,149 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,149 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,149 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,149 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ii9xiica
2025-09-03 10:32:00,149 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,152 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.34:41651
2025-09-03 10:32:00,153 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.34:41651
2025-09-03 10:32:00,153 - distributed.worker - INFO -          dashboard at:          10.6.102.34:40773
2025-09-03 10:32:00,153 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,153 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,153 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:32:00,153 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:32:00,153 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-cp1eumdm
2025-09-03 10:32:00,153 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,585 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:00,586 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,586 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,588 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:00,629 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:00,630 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,630 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,631 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:00,940 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:00,941 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,941 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,943 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:00,954 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:00,955 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,955 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,957 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,291 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,292 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,292 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,294 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,308 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,309 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,310 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,312 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,323 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,324 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,325 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,327 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,338 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,339 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,339 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,341 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,353 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,354 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,354 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,356 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,368 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,370 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,370 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,372 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,383 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,384 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,384 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,386 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,398 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,399 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,400 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,402 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,412 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,414 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,414 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,416 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,427 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,429 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,429 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,431 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,442 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,443 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,444 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,446 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,457 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,458 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,458 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,460 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,471 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,472 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,473 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,475 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,486 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,488 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,488 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,490 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,501 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,503 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,503 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,505 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,516 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,517 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,517 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,519 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,531 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,533 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,533 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,535 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,546 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,548 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,548 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,550 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,561 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,562 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,562 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,564 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,576 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,577 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,577 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,579 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,590 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,591 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,592 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,594 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,605 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,606 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,606 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,608 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,620 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,622 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,622 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,624 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,635 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,636 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,636 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,638 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,650 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,651 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,651 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,653 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,664 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,666 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,666 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,668 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,679 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,681 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,681 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,683 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,695 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,696 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,696 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,698 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:01,709 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:01,711 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:01,711 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:01,713 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:04,519 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:04,520 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:04,520 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:04,522 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,261 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,262 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,262 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,264 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,276 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,278 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,278 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,280 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,292 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,293 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,293 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,295 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,308 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,310 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,310 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,311 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,324 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,326 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,326 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,328 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,340 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,341 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,342 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,343 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:11,356 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:11,358 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:11,358 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:11,360 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:21,915 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:21,916 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:21,917 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:21,918 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:21,932 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:21,933 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:21,934 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:21,935 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:21,948 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:21,949 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:21,949 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:21,951 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:27,201 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,224 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,225 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,228 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,267 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,268 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,270 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,274 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,276 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,279 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,281 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,282 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,284 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,285 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,285 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,290 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,290 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,290 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,310 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,310 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,310 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,318 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,319 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,321 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,343 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,345 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,359 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,362 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,362 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,369 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,379 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,384 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,387 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,390 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,392 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,394 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,396 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,398 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,400 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,402 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:29,402 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,404 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,404 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:29,405 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,405 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:29,407 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,408 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,409 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,472 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:29,474 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,474 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:29,476 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:29,507 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:29,509 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,509 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:29,511 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:29,524 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:29,526 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,526 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:29,528 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:29,918 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:29,920 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,920 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:29,922 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:29,935 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:29,936 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,936 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:29,938 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:29,952 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:29,954 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,954 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:29,956 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:29,986 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:29,988 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:29,988 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:29,990 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:30,004 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:30,005 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:30,006 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:30,007 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:30,133 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:30,138 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:30,142 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:30,144 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:30,144 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:30,151 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:30,155 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:30,573 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:30,589 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:30,910 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:31,592 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:31,634 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:32,296 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:32,314 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:32,330 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:32,343 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:32,358 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:32,612 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:32,626 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:32,640 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:32,656 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:32,672 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:32,685 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:35,271 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:35,272 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:35,273 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:35,275 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:35,288 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:35,290 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:35,290 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:35,292 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:35,305 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:35,307 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:35,307 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:35,308 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:35,322 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:35,324 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:35,324 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:35,325 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:35,339 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:35,340 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:35,340 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:35,342 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:35,356 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:35,358 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:35,358 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:35,360 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:35,373 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:35,375 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:35,375 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:35,377 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:35,393 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:35,396 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:35,397 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:35,400 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:35,407 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:35,409 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:35,409 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:35,411 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:35,425 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:35,426 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:35,427 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:35,428 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:35,441 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:35,443 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:35,443 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:35,445 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:35,458 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:35,460 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:35,460 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:35,462 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:35,480 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:35,485 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:35,485 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:35,491 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:35,492 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:35,493 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:35,494 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:35,495 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:35,571 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:35,573 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:35,573 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:35,574 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:35,591 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:35,593 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:35,593 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:35,595 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:35,611 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:35,615 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:35,615 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:35,621 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:35,626 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:35,628 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:35,628 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:35,629 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:36,038 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:36,040 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:36,040 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:36,041 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:36,055 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:36,057 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:36,057 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:36,059 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:36,072 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:36,074 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:36,074 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:36,075 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:36,090 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:36,091 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:36,091 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:36,093 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:36,107 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:36,109 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:36,109 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:36,110 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:36,124 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:36,126 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:36,126 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:36,127 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:36,141 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:36,142 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:36,142 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:36,143 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:36,159 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:36,160 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:36,160 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:36,162 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:36,176 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:36,177 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:36,178 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:36,179 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:37,544 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:37,545 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:37,546 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:37,547 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:42,266 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:51,732 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,733 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,733 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,735 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:52,899 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:52,900 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:52,900 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:52,902 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:52,916 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:52,917 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:52,917 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:52,919 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:52,922 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:52,934 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:52,935 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:52,935 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:52,937 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:52,937 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:52,952 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:52,953 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:52,953 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:52,955 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:52,953 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:52,969 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:52,970 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:52,970 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:52,972 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:52,986 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:52,987 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:52,987 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:52,989 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:53,003 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:53,005 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:53,005 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:53,007 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:57,727 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:57,729 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:57,729 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:57,731 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:57,746 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:57,748 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:57,748 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:57,750 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:57,765 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:57,766 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:57,767 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:57,768 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:57,782 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:57,784 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:57,784 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:57,786 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:57,800 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:57,802 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:57,802 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:57,804 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:57,821 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:57,825 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:57,825 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:57,830 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:33:01,573 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:33:01,589 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:35:51,096 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,098 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,490 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,491 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,528 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,529 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,620 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,622 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,630 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,631 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,813 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,820 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,881 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,886 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,913 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,919 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,954 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,960 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,973 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,976 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,078 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,080 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,095 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,096 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,123 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,125 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,237 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,240 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,040 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,042 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,124 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,126 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,144 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,146 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,149 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,152 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,154 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,154 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,158 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,163 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,166 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,168 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,319 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,321 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,323 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,324 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,325 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,331 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,370 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,376 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,467 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,469 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,505 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,510 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,518 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,523 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,914 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,919 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,959 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,961 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,988 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,990 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,998 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,000 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,008 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,020 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,051 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,057 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,057 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,062 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,113 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,118 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,347 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,356 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,362 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,368 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,373 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,376 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,416 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,422 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,667 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,671 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,822 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,827 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,887 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,888 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,923 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,925 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,955 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,957 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,973 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,981 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,169 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,173 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,176 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,179 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,222 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,228 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,408 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,409 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,430 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,436 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,731 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,733 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,833 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,835 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,965 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,966 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,016 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,018 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,027 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,033 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,329 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,334 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,418 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,420 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,712 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,714 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,714 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,717 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,742 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,747 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,747 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,753 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,942 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,947 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,052 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,054 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,221 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,223 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,231 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,232 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,292 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,293 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,471 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,476 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,481 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,484 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,532 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,537 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,706 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,708 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,891 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,892 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,938 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,939 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,011 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,017 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,157 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,161 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,290 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,295 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,298 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,303 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,429 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,430 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,495 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,497 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,577 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,582 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,717 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,722 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,790 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,791 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,853 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,860 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,944 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,945 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,022 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,026 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,921 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,923 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,172 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,173 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,229 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,234 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,624 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,625 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,629 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,634 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,159 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,164 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,086 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,088 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,398 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,401 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,618 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,620 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,711 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,714 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:03,088 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:03,092 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:03,573 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:03,580 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:04,478 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:04,483 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:04,550 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:04,555 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:04,868 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:04,873 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:05,138 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:05,143 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:09,361 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:09,366 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:14,032 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:14,035 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:14,711 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:14,712 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:22,890 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,891 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,892 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,893 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,898 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,899 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,900 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,901 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,928 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,929 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,931 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,931 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,941 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,943 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,943 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,945 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,946 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,947 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,953 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,956 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,954 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,958 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,959 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,961 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,391 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,392 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,393 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,394 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,407 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,410 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,410 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,413 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,493 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,495 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,498 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,500 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,505 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,507 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,507 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,509 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,509 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,511 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,512 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,514 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,517 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,524 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,524 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,526 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,528 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,528 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,530 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,530 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,529 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,535 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,541 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,543 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,548 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,549 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,550 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,549 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,550 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,551 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,551 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,552 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,553 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,554 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,554 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,554 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,556 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,557 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,561 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,563 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,565 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,566 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,782 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,783 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,784 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,785 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,802 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,803 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,804 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,804 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,805 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,806 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,829 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,834 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,839 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,841 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,844 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,845 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,844 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,847 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,845 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,847 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,848 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,849 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,849 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,850 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,849 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,854 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,866 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,874 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,876 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,878 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,879 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,881 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,884 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,885 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,886 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,887 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,912 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,914 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,031 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,031 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,033 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,033 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,034 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,036 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,035 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,040 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,038 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,043 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,078 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,080 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,080 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,082 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,095 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,097 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,118 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,120 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,123 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,124 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,125 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,126 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,235 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,237 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,341 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,343 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,344 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,346 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,390 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,392 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,404 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,406 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,439 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,441 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,456 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,465 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,514 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,516 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,595 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,595 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,600 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,600 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,634 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,640 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,641 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,642 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,663 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,665 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,666 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,670 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,671 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,672 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,693 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,694 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,746 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,748 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,750 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,751 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,755 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,757 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,787 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,789 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,889 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,891 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,896 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,898 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,925 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,927 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,931 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,933 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,947 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,949 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,987 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,988 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,989 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,989 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,990 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,991 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,015 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,016 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,020 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,024 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,028 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,029 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,651 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,653 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,155 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,157 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,159 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,161 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,274 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,276 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,299 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,302 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,509 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,511 - distributed.utils - INFO - Reload module qme_vars from .py file
