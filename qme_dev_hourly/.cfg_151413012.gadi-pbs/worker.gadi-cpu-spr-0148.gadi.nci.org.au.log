Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-10-02 09:24:18,386 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:40421'
2025-10-02 09:24:18,396 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:44061'
2025-10-02 09:24:18,400 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:36287'
2025-10-02 09:24:18,405 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:45277'
2025-10-02 09:24:18,409 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:39639'
2025-10-02 09:24:18,414 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:33659'
2025-10-02 09:24:18,420 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:35335'
2025-10-02 09:24:18,424 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:42507'
2025-10-02 09:24:18,429 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:36185'
2025-10-02 09:24:18,434 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:38591'
2025-10-02 09:24:18,439 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:43409'
2025-10-02 09:24:18,443 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:46413'
2025-10-02 09:24:18,447 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:36867'
2025-10-02 09:24:18,451 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:33493'
2025-10-02 09:24:18,455 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:45669'
2025-10-02 09:24:18,459 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:42487'
2025-10-02 09:24:18,463 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:40941'
2025-10-02 09:24:18,467 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:33167'
2025-10-02 09:24:18,472 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:44875'
2025-10-02 09:24:18,476 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:37781'
2025-10-02 09:24:18,563 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:41295'
2025-10-02 09:24:18,567 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:34035'
2025-10-02 09:24:18,572 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:46473'
2025-10-02 09:24:18,577 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:38779'
2025-10-02 09:24:18,581 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:41037'
2025-10-02 09:24:18,586 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:46829'
2025-10-02 09:24:18,590 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:40309'
2025-10-02 09:24:18,595 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:43347'
2025-10-02 09:24:18,600 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:40611'
2025-10-02 09:24:18,604 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:39015'
2025-10-02 09:24:18,608 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:42657'
2025-10-02 09:24:18,612 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:37783'
2025-10-02 09:24:18,616 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:44253'
2025-10-02 09:24:18,621 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:35339'
2025-10-02 09:24:18,626 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:42385'
2025-10-02 09:24:18,630 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:33695'
2025-10-02 09:24:18,634 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:35705'
2025-10-02 09:24:18,639 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:46237'
2025-10-02 09:24:18,643 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:46025'
2025-10-02 09:24:18,648 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:40163'
2025-10-02 09:24:18,652 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:46009'
2025-10-02 09:24:18,655 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:36641'
2025-10-02 09:24:18,658 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:42365'
2025-10-02 09:24:18,663 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:42421'
2025-10-02 09:24:18,665 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:35995'
2025-10-02 09:24:18,670 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:34877'
2025-10-02 09:24:18,673 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:34083'
2025-10-02 09:24:18,676 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:42325'
2025-10-02 09:24:18,681 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:37281'
2025-10-02 09:24:18,685 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:42587'
2025-10-02 09:24:18,688 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:36917'
2025-10-02 09:24:18,693 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.4:40591'
2025-10-02 09:24:19,753 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:43635
2025-10-02 09:24:19,753 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:43833
2025-10-02 09:24:19,753 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:33717
2025-10-02 09:24:19,753 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:39603
2025-10-02 09:24:19,753 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:33945
2025-10-02 09:24:19,753 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:36311
2025-10-02 09:24:19,753 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:46805
2025-10-02 09:24:19,753 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:43635
2025-10-02 09:24:19,753 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:40729
2025-10-02 09:24:19,753 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:46011
2025-10-02 09:24:19,753 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:40461
2025-10-02 09:24:19,753 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:32961
2025-10-02 09:24:19,753 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:40951
2025-10-02 09:24:19,753 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:35653
2025-10-02 09:24:19,753 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:37037
2025-10-02 09:24:19,753 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:36723
2025-10-02 09:24:19,753 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:40491
2025-10-02 09:24:19,753 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:43833
2025-10-02 09:24:19,753 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:33717
2025-10-02 09:24:19,753 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:39603
2025-10-02 09:24:19,753 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:35947
2025-10-02 09:24:19,753 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:33945
2025-10-02 09:24:19,753 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:36311
2025-10-02 09:24:19,754 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:46805
2025-10-02 09:24:19,754 - distributed.worker - INFO -          dashboard at:            10.6.83.4:34667
2025-10-02 09:24:19,754 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:40729
2025-10-02 09:24:19,754 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:46011
2025-10-02 09:24:19,754 - distributed.worker - INFO -          dashboard at:            10.6.83.4:38489
2025-10-02 09:24:19,754 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:40461
2025-10-02 09:24:19,754 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:32961
2025-10-02 09:24:19,754 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:40951
2025-10-02 09:24:19,754 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:35653
2025-10-02 09:24:19,754 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:37037
2025-10-02 09:24:19,754 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:36723
2025-10-02 09:24:19,754 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:40491
2025-10-02 09:24:19,754 - distributed.worker - INFO -          dashboard at:            10.6.83.4:45791
2025-10-02 09:24:19,754 - distributed.worker - INFO -          dashboard at:            10.6.83.4:46673
2025-10-02 09:24:19,754 - distributed.worker - INFO -          dashboard at:            10.6.83.4:36611
2025-10-02 09:24:19,754 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:35947
2025-10-02 09:24:19,754 - distributed.worker - INFO -          dashboard at:            10.6.83.4:39187
2025-10-02 09:24:19,754 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,754 - distributed.worker - INFO -          dashboard at:            10.6.83.4:46641
2025-10-02 09:24:19,754 - distributed.worker - INFO -          dashboard at:            10.6.83.4:40809
2025-10-02 09:24:19,754 - distributed.worker - INFO -          dashboard at:            10.6.83.4:35125
2025-10-02 09:24:19,754 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,754 - distributed.worker - INFO -          dashboard at:            10.6.83.4:37203
2025-10-02 09:24:19,754 - distributed.worker - INFO -          dashboard at:            10.6.83.4:44357
2025-10-02 09:24:19,754 - distributed.worker - INFO -          dashboard at:            10.6.83.4:46683
2025-10-02 09:24:19,754 - distributed.worker - INFO -          dashboard at:            10.6.83.4:45335
2025-10-02 09:24:19,754 - distributed.worker - INFO -          dashboard at:            10.6.83.4:33587
2025-10-02 09:24:19,754 - distributed.worker - INFO -          dashboard at:            10.6.83.4:34883
2025-10-02 09:24:19,754 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,754 - distributed.worker - INFO -          dashboard at:            10.6.83.4:34141
2025-10-02 09:24:19,754 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,754 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,754 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,754 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,754 - distributed.worker - INFO -          dashboard at:            10.6.83.4:39695
2025-10-02 09:24:19,754 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,754 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,754 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,754 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,754 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,754 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,754 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,754 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,754 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,754 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,754 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,754 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,754 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,754 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,754 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,754 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,754 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,754 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,754 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,754 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,754 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,754 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,754 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,754 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,754 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,754 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,754 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,754 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,754 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,754 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,754 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,754 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,754 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,754 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,754 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,754 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,754 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,754 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,754 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,754 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,754 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,754 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,754 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,754 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,754 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,754 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,754 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,754 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,754 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,754 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,754 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,754 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-c2nv_wv4
2025-10-02 09:24:19,754 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,754 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,754 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,754 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,754 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-tvx_0z1q
2025-10-02 09:24:19,754 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,754 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,754 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,754 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,754 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,754 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-l5p5uxs8
2025-10-02 09:24:19,754 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-ajwfur4g
2025-10-02 09:24:19,754 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-1meiyxzg
2025-10-02 09:24:19,755 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-3hkhj_zs
2025-10-02 09:24:19,755 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,755 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-egjvd4ux
2025-10-02 09:24:19,755 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,755 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-j_hlrm60
2025-10-02 09:24:19,755 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-3tos0y8l
2025-10-02 09:24:19,755 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-27tjh_te
2025-10-02 09:24:19,755 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-nwnuqz36
2025-10-02 09:24:19,755 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,755 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-eotg_dra
2025-10-02 09:24:19,755 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-3hs5w5rw
2025-10-02 09:24:19,755 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-z32i17qg
2025-10-02 09:24:19,755 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-d1cim7xr
2025-10-02 09:24:19,755 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-07z2uxjz
2025-10-02 09:24:19,755 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,755 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,755 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,755 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,755 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-68qteaww
2025-10-02 09:24:19,755 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,755 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,755 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,755 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,755 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,755 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,755 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,755 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,755 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,755 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,755 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,755 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:36811
2025-10-02 09:24:19,755 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:36811
2025-10-02 09:24:19,755 - distributed.worker - INFO -          dashboard at:            10.6.83.4:43159
2025-10-02 09:24:19,755 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,755 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,755 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,755 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,755 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-jvjqm06s
2025-10-02 09:24:19,755 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,761 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:41627
2025-10-02 09:24:19,761 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:41627
2025-10-02 09:24:19,761 - distributed.worker - INFO -          dashboard at:            10.6.83.4:37641
2025-10-02 09:24:19,761 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,762 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,762 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,762 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,762 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-uamg9nff
2025-10-02 09:24:19,762 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,763 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:42923
2025-10-02 09:24:19,763 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:42923
2025-10-02 09:24:19,763 - distributed.worker - INFO -          dashboard at:            10.6.83.4:43029
2025-10-02 09:24:19,763 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,763 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,763 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,763 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,763 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-_l014ofl
2025-10-02 09:24:19,763 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,778 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,778 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,779 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,779 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,780 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:45633
2025-10-02 09:24:19,780 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:45633
2025-10-02 09:24:19,780 - distributed.worker - INFO -          dashboard at:            10.6.83.4:41525
2025-10-02 09:24:19,780 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,780 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,780 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,780 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,780 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,780 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-5vt7vagf
2025-10-02 09:24:19,780 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,780 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,780 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,781 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,781 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,782 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,782 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,782 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,783 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,784 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,784 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,784 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,785 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,786 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,786 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,787 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,787 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,787 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,787 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,788 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,788 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,789 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,789 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,790 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,791 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,792 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,792 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,792 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,793 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,794 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,794 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,794 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,795 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,795 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,795 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,796 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,796 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,796 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,797 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,797 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:37111
2025-10-02 09:24:19,797 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:37111
2025-10-02 09:24:19,797 - distributed.worker - INFO -          dashboard at:            10.6.83.4:42825
2025-10-02 09:24:19,797 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,797 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,797 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,797 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,797 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,797 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-0sikr4p2
2025-10-02 09:24:19,797 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,797 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,798 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,798 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,798 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,798 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,799 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,799 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,799 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,800 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,800 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,800 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,800 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,801 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,801 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,801 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,802 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,803 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,803 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,803 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,803 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,804 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,804 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,804 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,805 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,805 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,805 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,805 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,806 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,806 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,806 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,806 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,806 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,807 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,808 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,808 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,808 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:43665
2025-10-02 09:24:19,809 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:43665
2025-10-02 09:24:19,809 - distributed.worker - INFO -          dashboard at:            10.6.83.4:34147
2025-10-02 09:24:19,809 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,809 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,809 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,809 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,809 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-6zvoaggh
2025-10-02 09:24:19,809 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,809 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,820 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,821 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,821 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,822 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,823 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,825 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,825 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,826 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,826 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,827 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,827 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,828 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,844 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:41907
2025-10-02 09:24:19,844 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:41907
2025-10-02 09:24:19,844 - distributed.worker - INFO -          dashboard at:            10.6.83.4:35275
2025-10-02 09:24:19,844 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,844 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,844 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,844 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,844 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-202w9yvg
2025-10-02 09:24:19,844 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,854 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,855 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,855 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,855 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,861 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:36629
2025-10-02 09:24:19,861 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:36629
2025-10-02 09:24:19,861 - distributed.worker - INFO -          dashboard at:            10.6.83.4:38643
2025-10-02 09:24:19,861 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,861 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,861 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,861 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,861 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-u5619kuq
2025-10-02 09:24:19,862 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,862 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:45631
2025-10-02 09:24:19,862 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:45631
2025-10-02 09:24:19,862 - distributed.worker - INFO -          dashboard at:            10.6.83.4:43349
2025-10-02 09:24:19,862 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,862 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,862 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,863 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,863 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-a6xi2xum
2025-10-02 09:24:19,863 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,874 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,875 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,875 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,875 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,882 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,883 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,883 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,885 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,891 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:35483
2025-10-02 09:24:19,891 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:35483
2025-10-02 09:24:19,891 - distributed.worker - INFO -          dashboard at:            10.6.83.4:35897
2025-10-02 09:24:19,891 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,891 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,891 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,891 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,892 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-8a_55mun
2025-10-02 09:24:19,892 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,897 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:37519
2025-10-02 09:24:19,897 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:37519
2025-10-02 09:24:19,897 - distributed.worker - INFO -          dashboard at:            10.6.83.4:39871
2025-10-02 09:24:19,897 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,897 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,897 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,898 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,898 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-_io8z_96
2025-10-02 09:24:19,898 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,912 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,913 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,913 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,914 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,916 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,917 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,917 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,918 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,920 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:40649
2025-10-02 09:24:19,920 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:40649
2025-10-02 09:24:19,920 - distributed.worker - INFO -          dashboard at:            10.6.83.4:35499
2025-10-02 09:24:19,921 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,921 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,921 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,921 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,921 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-qleey76b
2025-10-02 09:24:19,921 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,936 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:40373
2025-10-02 09:24:19,936 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:40373
2025-10-02 09:24:19,936 - distributed.worker - INFO -          dashboard at:            10.6.83.4:42161
2025-10-02 09:24:19,936 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,936 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,936 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,936 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,936 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-5h75mq58
2025-10-02 09:24:19,936 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,940 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:36489
2025-10-02 09:24:19,941 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:36489
2025-10-02 09:24:19,941 - distributed.worker - INFO -          dashboard at:            10.6.83.4:37333
2025-10-02 09:24:19,941 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,941 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,941 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,941 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,941 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-hjl_39i3
2025-10-02 09:24:19,941 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,942 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,943 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,943 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,944 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,953 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:45157
2025-10-02 09:24:19,953 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:45157
2025-10-02 09:24:19,953 - distributed.worker - INFO -          dashboard at:            10.6.83.4:41333
2025-10-02 09:24:19,953 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,953 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,953 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,953 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,953 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-yw7f0kws
2025-10-02 09:24:19,953 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,957 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,958 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,958 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,960 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,962 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,963 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,963 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,965 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,972 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:43657
2025-10-02 09:24:19,972 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:40127
2025-10-02 09:24:19,972 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:43657
2025-10-02 09:24:19,972 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:40127
2025-10-02 09:24:19,972 - distributed.worker - INFO -          dashboard at:            10.6.83.4:45233
2025-10-02 09:24:19,972 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,972 - distributed.worker - INFO -          dashboard at:            10.6.83.4:38021
2025-10-02 09:24:19,972 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,972 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,972 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,972 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,972 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,972 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,972 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,972 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-pojpm3w1
2025-10-02 09:24:19,972 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-p9q4ptlj
2025-10-02 09:24:19,972 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,972 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,974 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,975 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,976 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,977 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,992 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,993 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,993 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,994 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:19,994 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:19,995 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,996 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:45705
2025-10-02 09:24:19,996 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:45705
2025-10-02 09:24:19,996 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:46425
2025-10-02 09:24:19,996 - distributed.worker - INFO -          dashboard at:            10.6.83.4:42953
2025-10-02 09:24:19,996 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:46425
2025-10-02 09:24:19,996 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,996 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,996 - distributed.worker - INFO -          dashboard at:            10.6.83.4:42001
2025-10-02 09:24:19,996 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,996 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:19,996 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,996 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,996 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-m4zyxe7a
2025-10-02 09:24:19,996 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:19,996 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,996 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:19,996 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-odkycgjv
2025-10-02 09:24:19,996 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:19,997 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:20,014 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:20,015 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,015 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,017 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:20,017 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:20,018 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,018 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,019 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:20,139 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:39801
2025-10-02 09:24:20,139 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:39801
2025-10-02 09:24:20,139 - distributed.worker - INFO -          dashboard at:            10.6.83.4:39717
2025-10-02 09:24:20,139 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,139 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,139 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:20,139 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:20,139 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-2z8hnji5
2025-10-02 09:24:20,139 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,139 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:46269
2025-10-02 09:24:20,139 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:46269
2025-10-02 09:24:20,139 - distributed.worker - INFO -          dashboard at:            10.6.83.4:33961
2025-10-02 09:24:20,140 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,140 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,140 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:20,140 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:20,140 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-xjkwr43u
2025-10-02 09:24:20,140 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,147 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:34651
2025-10-02 09:24:20,147 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:34651
2025-10-02 09:24:20,147 - distributed.worker - INFO -          dashboard at:            10.6.83.4:33905
2025-10-02 09:24:20,147 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,147 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,147 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:20,147 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:20,147 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-kf3bn3uf
2025-10-02 09:24:20,147 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,147 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:46195
2025-10-02 09:24:20,147 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:46195
2025-10-02 09:24:20,148 - distributed.worker - INFO -          dashboard at:            10.6.83.4:40731
2025-10-02 09:24:20,148 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,148 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,148 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:20,148 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:20,148 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-ng3vlilv
2025-10-02 09:24:20,148 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,154 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:34393
2025-10-02 09:24:20,155 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:34393
2025-10-02 09:24:20,155 - distributed.worker - INFO -          dashboard at:            10.6.83.4:44169
2025-10-02 09:24:20,155 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,155 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,155 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:20,155 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:20,155 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-vwj64n58
2025-10-02 09:24:20,155 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,157 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:44483
2025-10-02 09:24:20,157 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:44483
2025-10-02 09:24:20,157 - distributed.worker - INFO -          dashboard at:            10.6.83.4:41053
2025-10-02 09:24:20,157 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,157 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,157 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:20,157 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:20,157 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-jxtj7pe9
2025-10-02 09:24:20,157 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,158 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:42597
2025-10-02 09:24:20,158 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:42597
2025-10-02 09:24:20,158 - distributed.worker - INFO -          dashboard at:            10.6.83.4:44635
2025-10-02 09:24:20,158 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,158 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,158 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:20,158 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:20,158 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-qna1x_vp
2025-10-02 09:24:20,158 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,159 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:38181
2025-10-02 09:24:20,159 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:38181
2025-10-02 09:24:20,159 - distributed.worker - INFO -          dashboard at:            10.6.83.4:46437
2025-10-02 09:24:20,160 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,160 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,160 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:20,160 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:20,160 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-gzoc2tou
2025-10-02 09:24:20,160 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,160 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:20,161 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,161 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,163 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:34637
2025-10-02 09:24:20,163 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:34637
2025-10-02 09:24:20,163 - distributed.worker - INFO -          dashboard at:            10.6.83.4:40049
2025-10-02 09:24:20,163 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,163 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,163 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:20,163 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:20,163 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:20,163 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-ofslmx9i
2025-10-02 09:24:20,163 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,164 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:20,164 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:32781
2025-10-02 09:24:20,164 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:32781
2025-10-02 09:24:20,164 - distributed.worker - INFO -          dashboard at:            10.6.83.4:39223
2025-10-02 09:24:20,164 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,164 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,164 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:20,164 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:20,165 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-opsaq663
2025-10-02 09:24:20,165 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,165 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,165 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,166 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:20,166 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:36121
2025-10-02 09:24:20,166 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:36121
2025-10-02 09:24:20,166 - distributed.worker - INFO -          dashboard at:            10.6.83.4:44281
2025-10-02 09:24:20,166 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,166 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,166 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:20,166 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:20,166 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-3fz7s6mw
2025-10-02 09:24:20,166 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,168 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:20,169 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,169 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,170 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:20,171 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:20,172 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,172 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,173 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:20,173 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:20,174 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,174 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,174 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:34663
2025-10-02 09:24:20,174 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:34663
2025-10-02 09:24:20,174 - distributed.worker - INFO -          dashboard at:            10.6.83.4:33335
2025-10-02 09:24:20,174 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,174 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,174 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:20,174 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:20,174 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-10bir3i3
2025-10-02 09:24:20,174 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,175 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:20,175 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:33429
2025-10-02 09:24:20,175 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:33429
2025-10-02 09:24:20,175 - distributed.worker - INFO -          dashboard at:            10.6.83.4:35599
2025-10-02 09:24:20,175 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,175 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,175 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:20,175 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:20,175 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-2gni8rti
2025-10-02 09:24:20,175 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,177 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:20,177 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,177 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,179 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:20,180 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:35403
2025-10-02 09:24:20,180 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:35403
2025-10-02 09:24:20,180 - distributed.worker - INFO -          dashboard at:            10.6.83.4:42147
2025-10-02 09:24:20,180 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,180 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,180 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:20,180 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:20,180 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-czwk2arb
2025-10-02 09:24:20,180 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,181 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:20,181 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:38231
2025-10-02 09:24:20,181 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:38231
2025-10-02 09:24:20,181 - distributed.worker - INFO -          dashboard at:            10.6.83.4:36663
2025-10-02 09:24:20,181 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,181 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,181 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:20,181 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:20,181 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-2dljnqkn
2025-10-02 09:24:20,182 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,182 - distributed.worker - INFO -       Start worker at:      tcp://10.6.83.4:35049
2025-10-02 09:24:20,182 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,182 - distributed.worker - INFO -          Listening to:      tcp://10.6.83.4:35049
2025-10-02 09:24:20,182 - distributed.worker - INFO -          dashboard at:            10.6.83.4:37963
2025-10-02 09:24:20,182 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,182 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,182 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,182 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:20,182 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:20,182 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-8ysipf4i
2025-10-02 09:24:20,182 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,183 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:20,183 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:20,184 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,184 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,185 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:20,186 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:20,186 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,186 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,188 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:20,188 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:20,189 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:20,189 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,189 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,190 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,190 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,191 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:20,192 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:20,194 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:20,195 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,195 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:20,195 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,196 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,196 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,197 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:20,198 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:20,210 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:20,211 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:20,211 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,211 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,211 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:20,212 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,212 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,212 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:20,212 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:20,213 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:20,214 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:20,214 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:56,409 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,409 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,409 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,409 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,410 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,410 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,410 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,410 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,410 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,411 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,411 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,411 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,411 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,411 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,411 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,411 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,411 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,411 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,412 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,412 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,412 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,412 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,412 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,412 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,412 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,412 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,413 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,413 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,413 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,413 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,413 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,413 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,413 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,413 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,413 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,413 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,414 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,413 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,414 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,414 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,414 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,414 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,414 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,414 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,412 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,414 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,415 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,414 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,415 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,415 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,415 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,413 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,415 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,415 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,413 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,416 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,416 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,413 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,414 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,416 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,416 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,416 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,416 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,417 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,417 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,415 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,417 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,415 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,418 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,415 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,416 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,416 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,417 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,417 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,417 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,417 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,418 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,418 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,419 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,419 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,420 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,423 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,418 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,424 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,422 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,425 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,425 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,426 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,427 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,427 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,427 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,427 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,429 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,429 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,429 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,429 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,429 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,429 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,430 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,431 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,431 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,434 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,438 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,438 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:59,165 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,165 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,165 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,165 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,165 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,165 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,166 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,166 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,166 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,166 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,166 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,166 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,166 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,166 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,166 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,166 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,167 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,167 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,167 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,167 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,167 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,167 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,167 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,167 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,167 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,168 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,168 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,168 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,168 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,169 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,168 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,169 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,168 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,169 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,169 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,169 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,168 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,169 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,169 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,169 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,168 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,169 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,169 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,169 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,169 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,168 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,169 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,169 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,169 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,169 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,169 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,170 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,169 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,169 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,169 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,170 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,169 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,170 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,169 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,170 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,170 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,169 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,170 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,170 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,170 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,170 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,170 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,170 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,170 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,170 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,170 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,170 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,170 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,170 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,170 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,171 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,170 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,171 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,170 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,171 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,171 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,171 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,171 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,171 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,171 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,171 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,172 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,172 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,172 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,172 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,172 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,172 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,172 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,173 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,173 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,173 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,173 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,173 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,174 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,174 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,174 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,174 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,174 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,174 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,573 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,573 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,573 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,573 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,573 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,573 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,573 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,573 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,573 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,574 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,574 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,574 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,574 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,574 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,574 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,575 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,575 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,575 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,575 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,575 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,575 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,575 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,575 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,575 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,575 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,576 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,576 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,576 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,576 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,576 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,576 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,576 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,576 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,576 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,577 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,577 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,577 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,577 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,577 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,577 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,577 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,577 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,577 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,577 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,577 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,578 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,578 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,578 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,578 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,578 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,578 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,578 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,578 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,578 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,578 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,578 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,578 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,578 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,578 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,578 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,578 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,579 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,579 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,579 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,579 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,579 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,579 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,579 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,579 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,579 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,579 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,580 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,580 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,580 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,580 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,580 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,580 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,580 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,580 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,581 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,581 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,581 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,581 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,581 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,581 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,581 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,581 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,581 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,581 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,581 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,581 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,582 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,582 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,582 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,582 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,582 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,583 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,583 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,583 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,583 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,583 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,583 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,583 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,583 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:25:00,000 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,000 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,000 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,000 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,000 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,000 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,000 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,000 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,000 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,000 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,000 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,001 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,001 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,001 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,001 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,001 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,001 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,002 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,002 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,002 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,002 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,002 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,002 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,002 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,002 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,002 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,002 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,002 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,002 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,002 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,003 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,003 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,003 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,003 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,003 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,003 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,003 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,003 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,003 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,003 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,003 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,003 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,003 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,003 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,003 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,003 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,003 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,003 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,004 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,004 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,004 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,004 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,004 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,004 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,004 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,004 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,004 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,004 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,004 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,004 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,004 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,004 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,004 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,004 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,004 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,004 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,005 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,005 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,005 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,005 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,005 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,005 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,005 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,005 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,005 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,005 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,005 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,005 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,006 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,006 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,006 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,006 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,006 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,006 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,006 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,006 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,005 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:25:00,006 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,007 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,007 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,007 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,007 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,007 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,008 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,008 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,008 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,008 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,008 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,008 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,008 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,008 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,008 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,008 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:00,010 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:24,134 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,134 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,134 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,134 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,214 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,221 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,221 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,221 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,222 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,222 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,222 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,222 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,224 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,225 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,225 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,226 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,227 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,231 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,231 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,233 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,233 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,234 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,234 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,237 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,237 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,238 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,243 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,243 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 19:25:43,483 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,552 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,483 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,483 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,556 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:46011. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,483 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,556 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:39603. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,483 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,484 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,556 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:36811. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,484 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,556 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:38181. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,484 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,484 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,557 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:36723. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,483 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,557 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:40127. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,557 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:40951. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,483 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,557 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:43635. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,483 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,557 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:35947. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,558 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:43657. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,483 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,558 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:34663. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,483 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,558 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:39801. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,483 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,558 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:37519. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,483 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,483 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,559 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:46805. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,483 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,559 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:41627. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,483 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,559 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:40461. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,559 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:33429. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,483 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,484 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,560 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:45633. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,484 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,560 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:40373. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,484 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,560 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:36489. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,483 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,560 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:35483. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,484 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,560 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:37037. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,561 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:43833. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,483 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,561 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:36311. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,483 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,525 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,561 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:36629. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,561 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:46195. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,528 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,561 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:34651. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,537 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,537 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,562 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:32961. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,551 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,539 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,554 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,559 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.4:48998 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,566 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:44875'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,567 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:45705. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,567 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:45157. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,563 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.4:48954 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,567 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:40491. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,568 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:37111. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,568 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:42597. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,568 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,568 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:46425. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,568 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,569 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,569 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,569 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,571 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:33717. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,571 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:43409'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,572 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:41295'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,572 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,569 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,572 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:34877'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,572 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,572 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,572 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,572 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:45277'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,572 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,573 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,573 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,573 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,573 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,573 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,573 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,573 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,567 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.4:48972 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,573 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,573 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,567 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.4:48968 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,573 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,573 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,573 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,574 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,574 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,574 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,574 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,575 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,564 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,578 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:36121. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,578 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,579 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:35335'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,579 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,579 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,580 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:40421'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,580 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:42421'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,580 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,580 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:33167'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,580 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,580 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,580 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:44253'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,581 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,581 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,581 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:46009'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,581 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,581 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,581 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,581 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,581 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,581 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:36917'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,581 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,581 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,581 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,581 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,581 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:40941'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,581 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,581 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,581 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,582 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,582 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:44061'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,582 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,582 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,582 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,582 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,582 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,582 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:34393. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,582 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,582 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:33659'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,582 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,582 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,582 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,582 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,582 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,582 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:33695'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,582 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,582 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,583 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:39639'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,583 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,583 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,583 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,583 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,583 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,583 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:35339'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,583 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,583 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,583 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,583 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,583 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,583 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,583 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:36641'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,583 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,583 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,583 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,583 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,583 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,583 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,583 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,583 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:39015'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,583 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,583 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,584 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,584 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:45669'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,584 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,584 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,584 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,584 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,584 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:46413'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,584 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,584 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,584 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:42587'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,584 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,584 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,584 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,584 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,584 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:42657'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,584 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,584 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,584 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,585 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,585 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,585 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,585 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:46237'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,585 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,585 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,585 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,585 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,585 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,585 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,585 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:42507'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,585 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,585 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,585 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,585 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,585 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,585 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,585 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,585 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,585 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,585 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,585 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,586 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,585 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,586 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,586 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,586 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,586 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,586 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,586 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,586 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,586 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,586 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,586 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,586 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,586 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,586 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,586 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,587 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,587 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,587 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,587 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,587 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,587 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,588 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,589 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:46025'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,589 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,589 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,589 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,590 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,590 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,590 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,586 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1468a7054e10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:43,593 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,593 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,594 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,594 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,596 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,589 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14d45a357250>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:43,596 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,597 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,597 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,598 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,598 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,598 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,598 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,599 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,599 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,601 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,601 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,602 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,610 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,653 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,662 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,665 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:35049. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,671 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:32781. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,694 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,703 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:35403. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,720 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:43347'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,714 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,724 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,731 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:38231. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,732 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:40729. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,736 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 12, 8, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:43,740 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,749 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:43665. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,778 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,783 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,785 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:36287'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,786 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:45631. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,788 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,793 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:36867'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,794 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,799 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,802 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 14, 9, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:43,804 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,809 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 12, 12, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:43,530 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.4:48956 remote=tcp://10.6.83.1:8728>: Stream is closed
2025-10-02 19:25:43,820 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,828 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:35653. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,835 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,844 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:40649. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,837 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,854 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:46269. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,601 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.4:49006 remote=tcp://10.6.83.1:8728>: Stream is closed
2025-10-02 19:25:43,859 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:33493'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,877 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:41037'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,880 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,897 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:42923. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,909 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 5, 9, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:43,921 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:35705'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,646 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.4:48984 remote=tcp://10.6.83.1:8728>: Stream is closed
2025-10-02 19:25:43,928 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,931 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 3, 8, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:43,937 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:33945. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,934 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,943 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:41907. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,957 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:42325'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,963 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,968 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,974 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,979 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,984 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,994 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:44,002 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:34637. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,009 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 6, 5, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,046 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,049 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:40591'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,051 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,057 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,062 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,067 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,085 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:37781'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,090 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:38591'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,098 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:40611'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,105 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,111 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,116 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,119 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 16, 11, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,122 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,123 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,125 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,127 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 10, 6, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,127 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,127 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,128 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,128 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,128 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,128 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,126 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:44,128 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,130 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,133 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,133 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 6, 9, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,134 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,134 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.4:44483. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,136 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,136 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:40309'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,138 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,139 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,141 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,144 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,144 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,144 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:36185'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,145 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:46473'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,146 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 13, 1, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,146 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,146 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,147 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,147 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,149 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,152 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,154 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,157 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,173 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 14, 19, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,179 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:42365'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,191 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,192 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 10, 16, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,196 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:38779'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,197 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,202 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,204 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,208 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,210 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,213 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,214 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:37783'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,215 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,218 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,221 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,224 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,226 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,229 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,234 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,240 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,248 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 10, 1, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,254 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,258 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 11, 15, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,259 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,261 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:37281'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,265 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,270 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,273 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 5, 1, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,275 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,279 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,282 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 13, 20, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,284 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,284 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,285 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,285 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,285 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,285 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,290 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,295 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,300 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,303 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:34035'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,310 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:40163'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,311 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:35995'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,313 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 0, 5, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,318 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 7, 18, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,319 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,324 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,330 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,335 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,340 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,341 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:42487'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,355 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 4, 18, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,357 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 5, 4, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,360 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:34083'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,361 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 14, 21, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,382 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,382 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,382 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,383 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,383 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,392 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,398 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,403 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,403 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 16, 20, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,408 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,409 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,414 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,415 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,420 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,425 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,431 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,552 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:46829'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,573 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.4:42385'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,615 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 14, 0, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,621 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,627 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,632 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,637 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,639 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,642 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,644 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,650 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,655 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,660 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,661 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 12, 20, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,667 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,672 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,678 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,683 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,688 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,736 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,742 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,744 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,747 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,750 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,753 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,756 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,758 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,761 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,766 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:45,092 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 13, 7, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,831 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1477e856dad0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:44,980 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1481ca38c8d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:45,131 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14dcc870f790>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:45,461 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:45,467 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:45,472 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:45,478 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:45,483 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:45,244 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14db3e812b10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:45,578 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,579 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,582 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,583 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,584 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,591 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,592 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,592 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,595 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,597 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,598 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,598 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,599 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,600 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,601 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,601 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,601 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,602 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,603 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,605 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,613 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,441 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:45,867 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.4:35335'. Reason: nanny-close-gracefully
2025-10-02 19:25:45,868 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.4:35335' closed.
2025-10-02 19:25:45,886 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.4:43409'. Reason: nanny-close-gracefully
2025-10-02 19:25:45,887 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.4:43409' closed.
2025-10-02 19:25:45,476 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14b086deab90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:45,899 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.4:44875'. Reason: nanny-close-gracefully
2025-10-02 19:25:45,900 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.4:44875' closed.
2025-10-02 19:25:45,901 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.4:45277'. Reason: nanny-close-gracefully
2025-10-02 19:25:45,902 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.4:45277' closed.
2025-10-02 19:25:45,926 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.4:41295'. Reason: nanny-close-gracefully
2025-10-02 19:25:45,927 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.4:41295' closed.
2025-10-02 19:25:45,450 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1486a9f431d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:45,956 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.4:34877'. Reason: nanny-close-gracefully
2025-10-02 19:25:45,957 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.4:34877' closed.
2025-10-02 19:25:45,961 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.4:36917'. Reason: nanny-close-gracefully
2025-10-02 19:25:45,962 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.4:36917' closed.
2025-10-02 19:25:45,965 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.4:33167'. Reason: nanny-close-gracefully
2025-10-02 19:25:45,966 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.4:33167' closed.
2025-10-02 19:25:46,000 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.4:40421'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,001 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.4:40421' closed.
2025-10-02 19:25:46,193 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.4:46009'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,194 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.4:33659'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,200 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.4:42421'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,210 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.4:46009' closed.
2025-10-02 19:25:46,210 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.4:33659' closed.
2025-10-02 19:25:46,215 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.4:42421' closed.
2025-10-02 19:25:46,215 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.4:39015'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,216 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.4:46025'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,216 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.4:35339'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,217 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.4:44253'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,218 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.4:39015' closed.
2025-10-02 19:25:46,218 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.4:46025' closed.
2025-10-02 19:25:46,218 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.4:35339' closed.
2025-10-02 19:25:46,219 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.4:44253' closed.
2025-10-02 19:25:46,235 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.4:46413'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,236 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.4:33695'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,237 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.4:46413' closed.
2025-10-02 19:25:46,240 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.4:33695' closed.
2025-10-02 19:25:46,244 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.4:45669'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,249 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.4:44061'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,253 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.4:45669' closed.
2025-10-02 19:25:46,254 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.4:44061' closed.
2025-10-02 19:25:46,266 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.4:40941'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,267 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.4:40941' closed.
2025-10-02 19:25:46,268 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.4:36641'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,269 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.4:36641' closed.
2025-10-02 19:25:46,278 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.4:42657'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,278 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.4:42657' closed.
2025-10-02 19:25:46,291 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.4:39639'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,292 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.4:39639' closed.
2025-10-02 19:25:46,344 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.4:42507'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,345 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.4:42507' closed.
2025-10-02 19:25:46,379 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.4:42587'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,380 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.4:42587' closed.
2025-10-02 19:25:46,445 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.4:46237'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,446 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.4:46237' closed.
2025-10-02 19:25:50,020 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:52,025 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:52,492 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.4:40163'. Reason: nanny-close-gracefully
2025-10-02 19:25:52,494 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.4:40163' closed.
