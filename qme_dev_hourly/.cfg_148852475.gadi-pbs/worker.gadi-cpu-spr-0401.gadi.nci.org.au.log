Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-05 09:11:29,938 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:42953'
2025-09-05 09:11:29,948 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:44203'
2025-09-05 09:11:29,951 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:44677'
2025-09-05 09:11:29,956 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:37101'
2025-09-05 09:11:29,960 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:34117'
2025-09-05 09:11:29,964 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:40367'
2025-09-05 09:11:29,968 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:39925'
2025-09-05 09:11:29,972 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:34175'
2025-09-05 09:11:29,976 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:38279'
2025-09-05 09:11:29,981 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:42527'
2025-09-05 09:11:29,985 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:35303'
2025-09-05 09:11:29,990 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:38827'
2025-09-05 09:11:29,994 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:36807'
2025-09-05 09:11:29,999 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:46233'
2025-09-05 09:11:30,003 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:42753'
2025-09-05 09:11:30,008 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:37733'
2025-09-05 09:11:30,012 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:39181'
2025-09-05 09:11:30,017 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:40841'
2025-09-05 09:11:30,021 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:41097'
2025-09-05 09:11:30,026 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:35707'
2025-09-05 09:11:30,095 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:42807'
2025-09-05 09:11:30,100 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:37067'
2025-09-05 09:11:30,105 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:42495'
2025-09-05 09:11:30,109 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:38605'
2025-09-05 09:11:30,114 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:33441'
2025-09-05 09:11:30,118 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:41971'
2025-09-05 09:11:30,122 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:37039'
2025-09-05 09:11:30,127 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:40405'
2025-09-05 09:11:30,132 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:40073'
2025-09-05 09:11:30,136 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:35595'
2025-09-05 09:11:30,140 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:43973'
2025-09-05 09:11:30,145 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:34721'
2025-09-05 09:11:30,149 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:36075'
2025-09-05 09:11:30,153 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:44627'
2025-09-05 09:11:30,158 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:36035'
2025-09-05 09:11:30,162 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:39059'
2025-09-05 09:11:30,166 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:33861'
2025-09-05 09:11:30,171 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:37435'
2025-09-05 09:11:30,175 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:38413'
2025-09-05 09:11:30,179 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:41011'
2025-09-05 09:11:30,183 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:39915'
2025-09-05 09:11:30,188 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:41249'
2025-09-05 09:11:30,193 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:46467'
2025-09-05 09:11:30,196 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:41601'
2025-09-05 09:11:30,200 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:35419'
2025-09-05 09:11:30,206 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:36609'
2025-09-05 09:11:30,211 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:40891'
2025-09-05 09:11:30,215 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:41429'
2025-09-05 09:11:30,219 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:33857'
2025-09-05 09:11:30,224 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:40299'
2025-09-05 09:11:30,230 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:36833'
2025-09-05 09:11:30,235 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.41:32985'
2025-09-05 09:11:31,392 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:33401
2025-09-05 09:11:31,392 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:46577
2025-09-05 09:11:31,392 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:46231
2025-09-05 09:11:31,392 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:32981
2025-09-05 09:11:31,392 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:32805
2025-09-05 09:11:31,392 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:40355
2025-09-05 09:11:31,392 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:46663
2025-09-05 09:11:31,392 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:40641
2025-09-05 09:11:31,392 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:41651
2025-09-05 09:11:31,392 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:33017
2025-09-05 09:11:31,392 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:43251
2025-09-05 09:11:31,392 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:38835
2025-09-05 09:11:31,392 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:43297
2025-09-05 09:11:31,392 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:44527
2025-09-05 09:11:31,392 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:36871
2025-09-05 09:11:31,392 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:42463
2025-09-05 09:11:31,392 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:46137
2025-09-05 09:11:31,392 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:43923
2025-09-05 09:11:31,392 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:36687
2025-09-05 09:11:31,392 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:33401
2025-09-05 09:11:31,392 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:36685
2025-09-05 09:11:31,392 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:32935
2025-09-05 09:11:31,392 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:46577
2025-09-05 09:11:31,392 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:46231
2025-09-05 09:11:31,392 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:32981
2025-09-05 09:11:31,392 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:32805
2025-09-05 09:11:31,392 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:40355
2025-09-05 09:11:31,392 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:46663
2025-09-05 09:11:31,392 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:40641
2025-09-05 09:11:31,392 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:41651
2025-09-05 09:11:31,392 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:33017
2025-09-05 09:11:31,392 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:43251
2025-09-05 09:11:31,392 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:38835
2025-09-05 09:11:31,392 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:43297
2025-09-05 09:11:31,392 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:44527
2025-09-05 09:11:31,392 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:36871
2025-09-05 09:11:31,392 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:42463
2025-09-05 09:11:31,392 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:46137
2025-09-05 09:11:31,392 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:43923
2025-09-05 09:11:31,392 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:36687
2025-09-05 09:11:31,392 - distributed.worker - INFO -          dashboard at:          10.6.101.41:45059
2025-09-05 09:11:31,392 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:36685
2025-09-05 09:11:31,392 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:32935
2025-09-05 09:11:31,392 - distributed.worker - INFO -          dashboard at:          10.6.101.41:37225
2025-09-05 09:11:31,392 - distributed.worker - INFO -          dashboard at:          10.6.101.41:39559
2025-09-05 09:11:31,392 - distributed.worker - INFO -          dashboard at:          10.6.101.41:37203
2025-09-05 09:11:31,392 - distributed.worker - INFO -          dashboard at:          10.6.101.41:36921
2025-09-05 09:11:31,392 - distributed.worker - INFO -          dashboard at:          10.6.101.41:35843
2025-09-05 09:11:31,392 - distributed.worker - INFO -          dashboard at:          10.6.101.41:34437
2025-09-05 09:11:31,392 - distributed.worker - INFO -          dashboard at:          10.6.101.41:45957
2025-09-05 09:11:31,392 - distributed.worker - INFO -          dashboard at:          10.6.101.41:44067
2025-09-05 09:11:31,392 - distributed.worker - INFO -          dashboard at:          10.6.101.41:37403
2025-09-05 09:11:31,392 - distributed.worker - INFO -          dashboard at:          10.6.101.41:38819
2025-09-05 09:11:31,392 - distributed.worker - INFO -          dashboard at:          10.6.101.41:33215
2025-09-05 09:11:31,392 - distributed.worker - INFO -          dashboard at:          10.6.101.41:43503
2025-09-05 09:11:31,392 - distributed.worker - INFO -          dashboard at:          10.6.101.41:43893
2025-09-05 09:11:31,392 - distributed.worker - INFO -          dashboard at:          10.6.101.41:44927
2025-09-05 09:11:31,393 - distributed.worker - INFO -          dashboard at:          10.6.101.41:40821
2025-09-05 09:11:31,393 - distributed.worker - INFO -          dashboard at:          10.6.101.41:43595
2025-09-05 09:11:31,393 - distributed.worker - INFO -          dashboard at:          10.6.101.41:43763
2025-09-05 09:11:31,393 - distributed.worker - INFO -          dashboard at:          10.6.101.41:43051
2025-09-05 09:11:31,393 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,393 - distributed.worker - INFO -          dashboard at:          10.6.101.41:36507
2025-09-05 09:11:31,393 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,393 - distributed.worker - INFO -          dashboard at:          10.6.101.41:41867
2025-09-05 09:11:31,393 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,393 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,393 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,393 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,393 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,393 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,393 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,393 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,393 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,393 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,393 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,393 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,393 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,393 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,393 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,393 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,393 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,393 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,393 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,393 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,393 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,393 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,393 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,393 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,393 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,393 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,393 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,393 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,393 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,393 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,393 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,393 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,393 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,393 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,393 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,393 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,393 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,393 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,393 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,393 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,393 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,393 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,393 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,393 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,393 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,393 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,393 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,393 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,393 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,393 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,393 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,393 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,393 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,393 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-rh3vlbut
2025-09-05 09:11:31,393 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,393 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-wl61x8_v
2025-09-05 09:11:31,393 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-it4w8asg
2025-09-05 09:11:31,393 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-euyvt1dz
2025-09-05 09:11:31,393 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,393 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,393 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-qxupy6jh
2025-09-05 09:11:31,393 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-8ssn0fls
2025-09-05 09:11:31,393 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-1morwykn
2025-09-05 09:11:31,393 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-7jk_qtvk
2025-09-05 09:11:31,393 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-22bsb49p
2025-09-05 09:11:31,393 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-1ce7et03
2025-09-05 09:11:31,393 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-2eyngiod
2025-09-05 09:11:31,393 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-wwg7_czm
2025-09-05 09:11:31,393 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-hhtswoln
2025-09-05 09:11:31,393 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-xtm0v91d
2025-09-05 09:11:31,393 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-aue72h20
2025-09-05 09:11:31,393 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-dpdkb6aw
2025-09-05 09:11:31,393 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-45rggjsh
2025-09-05 09:11:31,393 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-oo7djhbt
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-vx3hs2zb
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-p1q6w2we
2025-09-05 09:11:31,393 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-4shvp7r3
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,394 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,394 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,394 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,394 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,394 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,394 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,394 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,400 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:36061
2025-09-05 09:11:31,400 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:36061
2025-09-05 09:11:31,400 - distributed.worker - INFO -          dashboard at:          10.6.101.41:38187
2025-09-05 09:11:31,400 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,400 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,400 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,400 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,400 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-onaf162n
2025-09-05 09:11:31,400 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,414 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,414 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,414 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,415 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,416 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,417 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,417 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,418 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,418 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,419 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,419 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,420 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,420 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,421 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,421 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,422 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,425 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,426 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,426 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,426 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,427 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,428 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,428 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,428 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,428 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,428 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,428 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,429 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,429 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,430 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,430 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,430 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,430 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:36111
2025-09-05 09:11:31,430 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:36111
2025-09-05 09:11:31,430 - distributed.worker - INFO -          dashboard at:          10.6.101.41:46561
2025-09-05 09:11:31,430 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,430 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,430 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,430 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,430 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-xpfqtfe0
2025-09-05 09:11:31,430 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,431 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,431 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,431 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,431 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,432 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,432 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,432 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,432 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,433 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,433 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,433 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,434 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,434 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,434 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,434 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,435 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,436 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,436 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,436 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,436 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,437 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,437 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,437 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,437 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,438 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,438 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:42483
2025-09-05 09:11:31,438 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:42483
2025-09-05 09:11:31,438 - distributed.worker - INFO -          dashboard at:          10.6.101.41:36657
2025-09-05 09:11:31,438 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,438 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,438 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,438 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,438 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-idwbmq70
2025-09-05 09:11:31,438 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,438 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,438 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,438 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,438 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,439 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,439 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,439 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,439 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,439 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,440 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,440 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,440 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,441 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,441 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,441 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,441 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,442 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,442 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,442 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,442 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,443 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,443 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,443 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,443 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,443 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,444 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,445 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,445 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,445 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,445 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,447 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,448 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:35821
2025-09-05 09:11:31,448 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:35821
2025-09-05 09:11:31,448 - distributed.worker - INFO -          dashboard at:          10.6.101.41:33965
2025-09-05 09:11:31,448 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,448 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,448 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,448 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,448 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-1xwj1r76
2025-09-05 09:11:31,448 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,462 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,463 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,463 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,463 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,464 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,464 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,465 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,466 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,470 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,472 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,472 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,473 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,488 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:35283
2025-09-05 09:11:31,488 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:35283
2025-09-05 09:11:31,488 - distributed.worker - INFO -          dashboard at:          10.6.101.41:41759
2025-09-05 09:11:31,488 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,488 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,488 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,488 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,488 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-wy4w08uo
2025-09-05 09:11:31,488 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,507 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:39097
2025-09-05 09:11:31,507 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:39097
2025-09-05 09:11:31,507 - distributed.worker - INFO -          dashboard at:          10.6.101.41:40823
2025-09-05 09:11:31,507 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,507 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,507 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,507 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-ockxtx42
2025-09-05 09:11:31,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,511 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,512 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,512 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,514 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,530 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,531 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:39527
2025-09-05 09:11:31,531 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,531 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:39527
2025-09-05 09:11:31,531 - distributed.worker - INFO -          dashboard at:          10.6.101.41:43999
2025-09-05 09:11:31,531 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,531 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,531 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,531 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,531 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,531 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:42935
2025-09-05 09:11:31,531 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-k4ry9cmp
2025-09-05 09:11:31,531 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:42935
2025-09-05 09:11:31,531 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,531 - distributed.worker - INFO -          dashboard at:          10.6.101.41:42485
2025-09-05 09:11:31,531 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,531 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,531 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,532 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,532 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-3jfdsszi
2025-09-05 09:11:31,532 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,533 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,542 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,542 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,542 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,543 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,545 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,545 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,545 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,546 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,628 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:33483
2025-09-05 09:11:31,628 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:33483
2025-09-05 09:11:31,628 - distributed.worker - INFO -          dashboard at:          10.6.101.41:42461
2025-09-05 09:11:31,628 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,628 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,628 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,628 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,628 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-zjlbu9je
2025-09-05 09:11:31,628 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,633 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:44955
2025-09-05 09:11:31,633 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:44955
2025-09-05 09:11:31,633 - distributed.worker - INFO -          dashboard at:          10.6.101.41:37755
2025-09-05 09:11:31,634 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,634 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,634 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,634 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,634 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-5zqmo9r2
2025-09-05 09:11:31,634 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,637 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:43571
2025-09-05 09:11:31,637 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:43571
2025-09-05 09:11:31,637 - distributed.worker - INFO -          dashboard at:          10.6.101.41:43143
2025-09-05 09:11:31,637 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,637 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,637 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,637 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,637 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-lvkmoo09
2025-09-05 09:11:31,637 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,640 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:40031
2025-09-05 09:11:31,640 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:40031
2025-09-05 09:11:31,640 - distributed.worker - INFO -          dashboard at:          10.6.101.41:33511
2025-09-05 09:11:31,640 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,640 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,640 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,640 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,640 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-d7bebogn
2025-09-05 09:11:31,640 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,642 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:37877
2025-09-05 09:11:31,642 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:37877
2025-09-05 09:11:31,642 - distributed.worker - INFO -          dashboard at:          10.6.101.41:44877
2025-09-05 09:11:31,642 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,642 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,642 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,642 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,642 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-7yf6o37f
2025-09-05 09:11:31,642 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,648 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,649 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,649 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,651 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,652 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,652 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,652 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,653 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,655 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,656 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,656 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,657 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,658 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,659 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,659 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,661 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,665 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,666 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,666 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,668 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,782 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:43833
2025-09-05 09:11:31,782 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:43833
2025-09-05 09:11:31,783 - distributed.worker - INFO -          dashboard at:          10.6.101.41:43897
2025-09-05 09:11:31,783 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,783 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,783 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,783 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,783 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-_r04p1a9
2025-09-05 09:11:31,783 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,790 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:33917
2025-09-05 09:11:31,790 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:33917
2025-09-05 09:11:31,790 - distributed.worker - INFO -          dashboard at:          10.6.101.41:35065
2025-09-05 09:11:31,790 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,790 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,790 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,790 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,790 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-pv150235
2025-09-05 09:11:31,790 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,801 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:34263
2025-09-05 09:11:31,801 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:34263
2025-09-05 09:11:31,801 - distributed.worker - INFO -          dashboard at:          10.6.101.41:39259
2025-09-05 09:11:31,801 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,801 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,801 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,801 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,802 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-embnx_tb
2025-09-05 09:11:31,802 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,804 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,805 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,805 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,807 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,812 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,813 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,813 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,815 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,823 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:41143
2025-09-05 09:11:31,823 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:41143
2025-09-05 09:11:31,824 - distributed.worker - INFO -          dashboard at:          10.6.101.41:38383
2025-09-05 09:11:31,824 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,824 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,824 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,824 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,824 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-ppuicys6
2025-09-05 09:11:31,824 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,824 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:37319
2025-09-05 09:11:31,824 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:37319
2025-09-05 09:11:31,824 - distributed.worker - INFO -          dashboard at:          10.6.101.41:46035
2025-09-05 09:11:31,824 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,824 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,824 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,824 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,824 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-u0cnc5i9
2025-09-05 09:11:31,824 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,824 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,825 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,825 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,826 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:33443
2025-09-05 09:11:31,826 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:33443
2025-09-05 09:11:31,826 - distributed.worker - INFO -          dashboard at:          10.6.101.41:46009
2025-09-05 09:11:31,826 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,826 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,826 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,826 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,826 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-jjnl6_px
2025-09-05 09:11:31,826 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,826 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:34671
2025-09-05 09:11:31,826 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:34671
2025-09-05 09:11:31,827 - distributed.worker - INFO -          dashboard at:          10.6.101.41:38959
2025-09-05 09:11:31,827 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,827 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,827 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,827 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,827 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-54j5vo9r
2025-09-05 09:11:31,827 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,827 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,827 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:39445
2025-09-05 09:11:31,827 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:39445
2025-09-05 09:11:31,827 - distributed.worker - INFO -          dashboard at:          10.6.101.41:40531
2025-09-05 09:11:31,827 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,827 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,827 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,827 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,827 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-x31rv70t
2025-09-05 09:11:31,827 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,829 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:38055
2025-09-05 09:11:31,829 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:38055
2025-09-05 09:11:31,829 - distributed.worker - INFO -          dashboard at:          10.6.101.41:33455
2025-09-05 09:11:31,829 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,829 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,829 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,829 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,829 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-ag9w6s7m
2025-09-05 09:11:31,829 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,831 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:35763
2025-09-05 09:11:31,831 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:35763
2025-09-05 09:11:31,831 - distributed.worker - INFO -          dashboard at:          10.6.101.41:38721
2025-09-05 09:11:31,831 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,831 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,831 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,831 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,831 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-7okvu1g8
2025-09-05 09:11:31,831 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,833 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:43325
2025-09-05 09:11:31,833 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:43325
2025-09-05 09:11:31,833 - distributed.worker - INFO -          dashboard at:          10.6.101.41:42227
2025-09-05 09:11:31,833 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,834 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,834 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,834 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,834 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-cu8gkq_d
2025-09-05 09:11:31,834 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,835 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,835 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,835 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,836 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,837 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:46769
2025-09-05 09:11:31,837 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:46769
2025-09-05 09:11:31,837 - distributed.worker - INFO -          dashboard at:          10.6.101.41:45911
2025-09-05 09:11:31,837 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,837 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,837 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,837 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,838 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-s3i26qtz
2025-09-05 09:11:31,838 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,839 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:43249
2025-09-05 09:11:31,839 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:43249
2025-09-05 09:11:31,839 - distributed.worker - INFO -          dashboard at:          10.6.101.41:40547
2025-09-05 09:11:31,839 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,839 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,839 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,839 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,839 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-26x4r3os
2025-09-05 09:11:31,839 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,840 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,840 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,840 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,841 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,841 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:44385
2025-09-05 09:11:31,841 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:44385
2025-09-05 09:11:31,841 - distributed.worker - INFO -          dashboard at:          10.6.101.41:41191
2025-09-05 09:11:31,841 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,841 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,841 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,841 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,841 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-tirreind
2025-09-05 09:11:31,841 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,844 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:37251
2025-09-05 09:11:31,845 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:37251
2025-09-05 09:11:31,845 - distributed.worker - INFO -          dashboard at:          10.6.101.41:43111
2025-09-05 09:11:31,845 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,845 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,845 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,845 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,845 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-m9i3wi5d
2025-09-05 09:11:31,845 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,845 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,846 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,846 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,847 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,848 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,848 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,848 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,849 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,849 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,850 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,850 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,850 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,851 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:33629
2025-09-05 09:11:31,851 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:33629
2025-09-05 09:11:31,851 - distributed.worker - INFO -          dashboard at:          10.6.101.41:43955
2025-09-05 09:11:31,851 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,851 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,851 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,851 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:41851
2025-09-05 09:11:31,851 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,851 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:41851
2025-09-05 09:11:31,851 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,851 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-y5c9zbev
2025-09-05 09:11:31,851 - distributed.worker - INFO -          dashboard at:          10.6.101.41:38251
2025-09-05 09:11:31,851 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,851 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,851 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,851 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,851 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,851 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,851 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-vejx4ihp
2025-09-05 09:11:31,851 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,851 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,852 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,852 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,853 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.41:35887
2025-09-05 09:11:31,853 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.41:35887
2025-09-05 09:11:31,853 - distributed.worker - INFO -          dashboard at:          10.6.101.41:40729
2025-09-05 09:11:31,853 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,853 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,853 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,853 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,853 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-vs7_w3tb
2025-09-05 09:11:31,853 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,853 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,853 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,853 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,853 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,853 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,854 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,855 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,855 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,855 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,856 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,856 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,859 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,860 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,860 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,861 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,862 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,862 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,862 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,863 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,868 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,869 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,869 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,870 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,872 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,873 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,873 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,874 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,875 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,875 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,875 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,875 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,876 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,876 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,877 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,878 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:12:06,599 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,599 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,599 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,600 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,600 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,600 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,600 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,600 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,600 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,600 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,601 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,601 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,601 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,601 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,601 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,601 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,601 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,601 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,601 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,601 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,602 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,602 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,602 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,602 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,602 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,602 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,602 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,603 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,603 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,603 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,603 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,603 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,603 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,603 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,603 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,603 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,603 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,603 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,603 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,603 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,604 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,604 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,604 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,604 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,604 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,604 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,604 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,604 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,604 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,604 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,604 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,602 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,604 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,604 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,605 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,605 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,604 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,605 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,605 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,605 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,605 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,605 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,605 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,605 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,603 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,605 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,605 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,606 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,604 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,606 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,606 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,606 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,606 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,606 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,607 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,607 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,607 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,608 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,606 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,606 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,606 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,607 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,607 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,608 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,607 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,608 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,608 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,609 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,609 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,613 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,615 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,615 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,617 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,614 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,618 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,618 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,618 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,618 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,619 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,619 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,620 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,619 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,622 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,625 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:09,398 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,399 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,399 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,399 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,399 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,399 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,399 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,399 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,399 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,400 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,400 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,400 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,400 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,400 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,400 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,400 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,400 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,400 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,401 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,401 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,401 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,401 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,402 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,401 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,401 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,401 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,402 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,402 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,402 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,402 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,402 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,402 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,402 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,402 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,401 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,401 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,401 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,402 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,402 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,402 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,402 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,402 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,402 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,402 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,402 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,403 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,402 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,402 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,403 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,402 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,403 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,403 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,403 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,403 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,403 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,403 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,403 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,403 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,403 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,403 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,403 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,404 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,404 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,403 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,404 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,404 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,404 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,403 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,404 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,404 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,404 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,404 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,404 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,405 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,405 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,404 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,404 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,405 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,404 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,405 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,405 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,405 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,405 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,405 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,405 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,405 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,405 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,405 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,405 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,405 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,406 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,406 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,406 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,406 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,406 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,406 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,406 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,406 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,406 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,406 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,406 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,407 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,407 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,407 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,778 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,778 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,778 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,778 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,779 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,779 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,779 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,779 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,779 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,779 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,780 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,780 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,779 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,780 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,780 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,780 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,780 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,780 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,780 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,780 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,781 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,781 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,781 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,781 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,781 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,781 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,781 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,781 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,781 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,782 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,782 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,782 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,782 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,782 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,782 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,782 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,782 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,782 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,782 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,782 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,782 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,783 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,783 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,783 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,783 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,783 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,783 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,783 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,783 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,783 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,783 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,783 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,783 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,783 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,783 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,784 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,784 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,784 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,784 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,784 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,784 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,784 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,784 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,784 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,784 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,784 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,784 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,784 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,785 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,785 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,785 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,785 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,785 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,785 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,785 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,785 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,785 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,786 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,786 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,786 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,786 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,786 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,786 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,786 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,786 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,786 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,787 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,787 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,787 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,787 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,787 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,787 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,787 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,787 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,787 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,787 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,787 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,787 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,787 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,787 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,787 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,787 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,787 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,799 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:10,240 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,240 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,240 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,240 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,240 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,240 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,240 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,240 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,240 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,241 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,241 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,241 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,241 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,241 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,241 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,241 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,241 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,242 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,242 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,242 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,242 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,242 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,242 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,242 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,242 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,242 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,242 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,243 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,243 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,243 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,243 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,243 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,243 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,243 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,243 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,243 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,243 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,243 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,242 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,243 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,243 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,243 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,243 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,243 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,243 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,243 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,243 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,244 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,244 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,244 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,244 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,244 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,244 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,244 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,244 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,244 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,244 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,244 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,244 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,244 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,244 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,244 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,244 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,244 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,245 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,244 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,245 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,245 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,245 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,245 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,245 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,245 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,245 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,245 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,245 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,245 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,245 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,245 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,245 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,246 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,246 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,246 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,246 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,246 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,246 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,246 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,246 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,247 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,247 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,247 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,247 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,247 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,247 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,248 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,248 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,248 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,248 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,248 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,248 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,248 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,248 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,249 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,249 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,249 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:16:17,562 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,573 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,627 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,645 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,656 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,658 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,658 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,658 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,663 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,698 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,750 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,783 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:50,496 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:50,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:50,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:50,532 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:50,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:51,082 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,002 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,861 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,866 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:53,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:54,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:54,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:54,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:54,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:54,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:54,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:55,900 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:55,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:56,767 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,739 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,774 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,777 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,932 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:59,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:01,129 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:01,129 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:01,235 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:01,600 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:02,554 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:03,171 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:03,369 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:03,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:04,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:04,598 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:08,926 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:09,066 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:09,070 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:09,267 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:09,849 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:10,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:10,668 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:10,716 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:11,198 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:11,463 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:12,097 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:12,098 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:12,099 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:13,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:13,173 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:13,541 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:13,903 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:14,946 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,051 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,052 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,053 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,056 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,161 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,166 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,270 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,907 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,459 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,460 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,550 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,739 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,447 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,452 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,655 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,172 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,750 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:19,159 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:19,161 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:19,161 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:19,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:19,605 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:19,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:19,741 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:20,190 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:28,298 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:28,300 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:28,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:28,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:28,603 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:28,988 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:29,181 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:30,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:30,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:30,654 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:30,654 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:30,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:31,023 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:31,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:31,551 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:31,551 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:31,858 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:31,948 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:31,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:32,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:32,417 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:32,784 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:33,755 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:33,995 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,423 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,825 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,295 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,498 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,246 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,140 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:39,585 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:40,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:40,031 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:40,031 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:40,812 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:44,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:45,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:50,275 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:50,751 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:50,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:51,175 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:51,520 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,716 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,306 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,308 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,865 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,998 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,246 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,251 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,251 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:55,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:55,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:55,456 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:55,507 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:56,929 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:56,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:56,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:57,158 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:57,162 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:57,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:57,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:58,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:58,750 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:58,753 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:00,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:00,861 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:01,845 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:08,253 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:08,908 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:10,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:12,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:12,649 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:14,365 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:14,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,066 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,786 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,276 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,285 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,343 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,774 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,774 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,777 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,777 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,978 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,535 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,571 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,573 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,575 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,672 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:18,044 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:18,046 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:18,046 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,031 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,578 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,585 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,589 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,592 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,621 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,685 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,454 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,817 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,826 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,845 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,181 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,400 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,420 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,956 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,510 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,516 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,518 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,518 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,526 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,527 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,682 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,689 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,740 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:38,031 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:42,376 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:42,378 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:42,378 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:42,383 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:42,447 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:42,674 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:43,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:43,017 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:43,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:43,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:43,102 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:43,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,183 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,782 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,695 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,067 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,070 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,338 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,284 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,307 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:49,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:49,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:51,832 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:55,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:55,889 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:55,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:06,362 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:06,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:06,899 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:06,899 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:07,538 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:07,754 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:08,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:08,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:08,173 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:08,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:08,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:08,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:08,898 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:08,899 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,443 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:11,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:12,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:12,812 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:12,814 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,859 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,089 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,089 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,091 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,092 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:15,095 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:15,834 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:15,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:17,573 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:17,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:18,603 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:19,242 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:30,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:32,602 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:36,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:36,292 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:36,293 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:36,297 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:36,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:36,913 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:37,357 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:37,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:37,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,190 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,192 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,548 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,874 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,303 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,318 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,449 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,056 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,285 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,541 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,550 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,049 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,091 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,097 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,412 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,718 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,754 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,898 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,163 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,289 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:44,555 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:45,367 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,747 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,861 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,978 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,979 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,319 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,741 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,747 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,764 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,765 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,767 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,017 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,532 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,791 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,911 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,974 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:51,066 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:51,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:51,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:51,790 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:51,790 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:51,792 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:51,796 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:51,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:52,221 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:53,874 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:54,689 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:57,210 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:58,090 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:58,102 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:58,181 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:58,403 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:58,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:58,628 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:58,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:00,082 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:04,362 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:04,366 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:04,369 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:04,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:11,287 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,287 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,287 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,289 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:43251. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,289 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:32935. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,289 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:33629. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,287 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,289 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:33443. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,289 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:36061. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,290 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:33017. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,290 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:44955. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,290 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:46663. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,291 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:41851. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,291 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:36111. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,289 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,291 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:37251. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,291 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:40641. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,289 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,291 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:35283. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,291 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:39097. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,291 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:42935. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,291 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:36687. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,289 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,292 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:43571. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,292 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:33483. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,292 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:32805. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,292 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:37877. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,292 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:35763. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,292 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:40031. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,292 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:42463. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,292 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:46577. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,289 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,293 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:42483. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,293 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:44527. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,293 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:39527. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,293 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:43249. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,293 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:38055. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,293 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:43923. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,293 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:32981. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,293 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:38835. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,293 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:39445. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,294 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:41143. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,294 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:40355. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,294 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:44385. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,294 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:34263. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,295 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:43833. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,301 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:38413'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,304 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.63:39963, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.64:36105, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.41:46769, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.54:35221, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,308 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:37733'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,309 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:35595'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,309 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:41097'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,310 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 175, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,310 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1020, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,310 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:39181'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,310 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:44677'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,310 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.38:46587, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,311 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.39:44451, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,311 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.66:40799, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,311 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 804, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,311 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1563, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,311 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1129, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,311 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1481, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,311 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1388, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,311 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1134, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,312 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.38:36079, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,312 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.41:35821, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,312 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.65:41157, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,313 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,314 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,321 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:41011'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,322 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:42753'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,322 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:40073'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,322 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:33857'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,322 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2581, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,322 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:37435'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,323 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2587, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,323 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1393, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,323 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:46233'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,323 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 627, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,323 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:32985'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,323 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:41601'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,323 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1940, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,323 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 8030, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,323 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,323 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:36833'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,323 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,323 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 836, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,324 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2906, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,324 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:34175'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,324 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.56:38157, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,324 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 937, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,324 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2895, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,324 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:36035'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,324 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2237, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,324 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:41249'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,324 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2870, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,324 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:40841'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,324 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2640, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,325 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3039, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,325 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:41971'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,325 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,325 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2644, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,325 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2314, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,325 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:46467'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,325 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2079, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,325 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2339, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,325 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:37067'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,325 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1021, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,325 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2998, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,326 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,326 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:42495'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,326 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,326 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,325 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 8343, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,326 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,326 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1572, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,326 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 5441, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,326 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,326 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,326 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:36807'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,326 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,326 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,326 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1360, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,326 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1181, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,326 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 8604, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,326 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,326 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 2094, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,326 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:40299'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,327 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1872, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,327 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 2084, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,327 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:37101'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,327 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1215, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,327 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1044, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,327 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:42953'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,327 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1001, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,327 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1174, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,328 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1164, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,328 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:44203'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,328 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1158, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,328 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:37039'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,328 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1877, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,328 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:39925'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,328 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1168, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,328 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1888, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,328 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:40405'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,329 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1500, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,328 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1557, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,329 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:39059'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,329 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2238, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,329 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:43973'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,329 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1574, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,330 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:33441'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,330 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1142, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,330 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 2036, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,330 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:36609'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,330 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 2033, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,330 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:38827'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,330 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2261, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,331 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,331 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:34721'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,331 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,331 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,331 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,331 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3404, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,331 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,331 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 562, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,331 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:35419'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,331 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,331 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,331 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,331 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2670, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,331 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,331 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.38:42705, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,331 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,331 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1532, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,331 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,332 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,332 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,332 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,332 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 948, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,332 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1366, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,332 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1127, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,332 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1552, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,332 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,332 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1125, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,332 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,332 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,332 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1130, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,332 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1141, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,332 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1128, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,333 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1172, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,333 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1192, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,334 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1188, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,334 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,334 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,335 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,336 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,336 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,336 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,338 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,343 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,343 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,344 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,344 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,350 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:11,508 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:11,920 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,923 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:41651. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,925 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:11,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:11,939 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.41:58218 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:11,950 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:34117'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,951 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,951 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,951 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,951 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,951 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,960 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,061 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:12,064 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:33401. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:12,062 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:12,065 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:37319. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:12,078 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.41:58136 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:12,080 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.41:58476 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:12,087 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:35303'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:12,088 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:33861'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:12,088 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:12,088 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:12,088 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:12,088 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:12,088 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:12,088 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:12,088 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:12,089 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:12,089 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:12,089 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:12,092 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1551e4188650>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:12,098 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,100 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,421 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,431 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,432 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,526 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,527 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,559 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,586 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,587 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,592 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:13,217 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:13,220 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:35821. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:13,231 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:13,237 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.41:58314 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:13,242 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:44627'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:13,243 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:13,243 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:13,243 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:13,243 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:13,243 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:13,252 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:13,328 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:13,338 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:13,355 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:13,393 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:13,513 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:13,880 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:38413'. Reason: nanny-close-gracefully
2025-09-05 09:20:13,884 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:38413' closed.
2025-09-05 09:20:13,908 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:44677'. Reason: nanny-close-gracefully
2025-09-05 09:20:13,909 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:44203'. Reason: nanny-close-gracefully
2025-09-05 09:20:13,909 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:42753'. Reason: nanny-close-gracefully
2025-09-05 09:20:13,910 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:44677' closed.
2025-09-05 09:20:13,910 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:44203' closed.
2025-09-05 09:20:13,910 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:42753' closed.
2025-09-05 09:20:13,929 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:13,964 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,032 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:14,102 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,103 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,315 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:41097'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,316 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:41097' closed.
2025-09-05 09:20:14,356 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:34117'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,357 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:34117' closed.
2025-09-05 09:20:14,425 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,434 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,436 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,487 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:35303'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,497 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:35303' closed.
2025-09-05 09:20:14,512 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:33861'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,513 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:33861' closed.
2025-09-05 09:20:14,530 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,531 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,563 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,591 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,592 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,596 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,768 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:14,810 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:39181'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,811 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:39181' closed.
2025-09-05 09:20:14,850 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:46467'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,851 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:46467' closed.
2025-09-05 09:20:14,902 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:36807'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,903 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:36807' closed.
2025-09-05 09:20:14,991 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:34721'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,992 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:34721' closed.
2025-09-05 09:20:14,993 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:14,996 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:46137. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,000 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:15,013 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.41:58138 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:15,017 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:35707'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,018 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2236, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:15,018 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:15,019 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:15,019 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:15,019 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:15,019 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:15,045 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:38827'. Reason: nanny-close-gracefully
2025-09-05 09:20:15,046 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:38827' closed.
2025-09-05 09:20:15,063 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:37101'. Reason: nanny-close-gracefully
2025-09-05 09:20:15,064 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:37101' closed.
2025-09-05 09:20:15,123 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:36609'. Reason: nanny-close-gracefully
2025-09-05 09:20:15,124 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:36609' closed.
2025-09-05 09:20:15,146 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:35419'. Reason: nanny-close-gracefully
2025-09-05 09:20:15,147 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:35419' closed.
2025-09-05 09:20:15,169 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:39925'. Reason: nanny-close-gracefully
2025-09-05 09:20:15,171 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:39925' closed.
2025-09-05 09:20:15,256 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:15,361 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:15,364 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:35887. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:15,380 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.41:58602 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:15,392 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:38605'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:15,392 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:15,392 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:15,393 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:15,393 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:15,393 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:15,397 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:15,396 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14844b323810>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:15,404 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,625 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,652 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:44627'. Reason: nanny-close-gracefully
2025-09-05 09:20:15,653 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:44627' closed.
2025-09-05 09:20:15,789 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:33857'. Reason: nanny-close-gracefully
2025-09-05 09:20:15,791 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:33857' closed.
2025-09-05 09:20:16,036 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:16,150 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,293 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:16,295 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:16,297 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:34671. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:16,310 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.41:58496 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:16,314 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:36075'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:16,315 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:16,315 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:16,315 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:16,315 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:16,315 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:16,318 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1548bc2e53d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:16,325 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,405 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:16,406 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:46769. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:16,416 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.101.58:32819
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.101.41:56324 remote=tcp://10.6.101.58:32819>: Stream is closed
2025-09-05 09:20:16,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:16,427 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.41:58550 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:16,430 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:39915'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:16,431 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:16,431 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:16,431 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:16,431 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:16,432 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:16,435 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14915fb7ae90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:16,440 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,442 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:39059'. Reason: nanny-close-gracefully
2025-09-05 09:20:16,443 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:39059' closed.
2025-09-05 09:20:16,772 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,004 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,153 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:36833'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,154 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:36833' closed.
2025-09-05 09:20:17,405 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:40299'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,406 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:40299' closed.
2025-09-05 09:20:17,407 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,629 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,827 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:38605'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,829 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:38605' closed.
2025-09-05 09:20:18,021 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:35707'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,022 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:35707' closed.
2025-09-05 09:20:18,154 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:18,329 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,328 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:18,330 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:36685. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,332 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,348 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.41:58260 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:18,352 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:42527'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,353 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3055, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:18,353 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:18,353 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:18,353 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:18,353 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:18,354 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:18,355 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:18,442 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,585 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:36035'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,586 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:36035' closed.
2025-09-05 09:20:18,639 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,702 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:18,705 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:33917. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,719 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:18,724 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.41:58442 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:18,728 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:40891'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,729 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:18,729 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:18,729 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:18,729 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:18,729 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:18,731 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x154921136110>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:18,737 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,773 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:36075'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,774 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:36075' closed.
2025-09-05 09:20:18,798 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:39915'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,799 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:39915' closed.
2025-09-05 09:20:18,851 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,891 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:18,894 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:43325. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,902 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.101.62:44707
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.101.41:43258 remote=tcp://10.6.101.62:44707>: Stream is closed
2025-09-05 09:20:18,907 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:18,912 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.41:58536 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:18,915 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:41429'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:18,916 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:18,917 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:18,917 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:18,917 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:18,917 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:18,919 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14eb945c7550>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:18,924 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,936 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,006 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,007 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,006 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:19,009 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:46231. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,019 - distributed.worker - ERROR - failed during get data with tcp://10.6.101.41:46231 -> tcp://10.6.101.46:34821
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.101.41:46231 remote=tcp://10.6.101.46:42246>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-05 09:20:19,025 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:19,031 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.41:58164 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:19,034 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:40367'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,035 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:19,035 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:19,035 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:19,035 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:19,035 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:19,037 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x147852dd8150>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:19,043 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,173 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:19,174 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:19,176 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:36871. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,176 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.41:43297. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,188 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:19,186 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.101.58:32819
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.101.41:56312 remote=tcp://10.6.101.58:32819>: Stream is closed
2025-09-05 09:20:19,192 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:19,194 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.41:58246 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:19,197 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.41:58234 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:19,199 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:38279'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,200 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:19,200 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:19,200 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:19,200 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:19,200 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:19,200 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.41:42807'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,201 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:19,201 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:19,201 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:19,201 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:19,201 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:19,202 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x146b7b67b610>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:19,203 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x150f964536d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:19,208 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,209 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,248 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,502 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,505 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,625 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,779 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,783 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,783 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,895 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,951 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,952 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,953 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,954 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,197 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,221 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,336 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,643 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,729 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:40841'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,730 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:40841' closed.
2025-09-05 09:20:20,740 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,855 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,927 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,940 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,011 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,011 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,019 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:41601'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,020 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:41601' closed.
2025-09-05 09:20:21,046 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,148 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:40891'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,149 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:40891' closed.
2025-09-05 09:20:21,211 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,212 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,251 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,270 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:37039'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,271 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:37039' closed.
2025-09-05 09:20:21,348 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:41429'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,349 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:41429' closed.
2025-09-05 09:20:21,350 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:42953'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,351 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:42953' closed.
2025-09-05 09:20:21,422 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:42527'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,424 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:42527' closed.
2025-09-05 09:20:21,437 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:40367'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,438 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:41011'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,439 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:40367' closed.
2025-09-05 09:20:21,440 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:41011' closed.
2025-09-05 09:20:21,507 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,509 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,616 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:35595'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,617 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:35595' closed.
2025-09-05 09:20:21,628 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:38279'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,629 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:38279' closed.
2025-09-05 09:20:21,629 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,633 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:42807'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,634 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:42807' closed.
2025-09-05 09:20:21,783 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,787 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,787 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,900 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,903 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:32985'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,905 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:40073'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,906 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:32985' closed.
2025-09-05 09:20:21,907 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:40073' closed.
2025-09-05 09:20:21,955 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,957 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,957 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,959 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,021 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:37733'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,021 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:37733' closed.
2025-09-05 09:20:22,185 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:37067'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,186 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:37067' closed.
2025-09-05 09:20:22,203 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,204 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:40405'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,205 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:40405' closed.
2025-09-05 09:20:22,236 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:43973'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,237 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:43973' closed.
2025-09-05 09:20:22,357 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:37435'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,358 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:37435' closed.
2025-09-05 09:20:22,366 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:46233'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,367 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:46233' closed.
2025-09-05 09:20:22,445 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:41971'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,446 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:41971' closed.
2025-09-05 09:20:22,469 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:41249'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,471 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:41249' closed.
2025-09-05 09:20:22,484 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:42495'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,485 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:42495' closed.
2025-09-05 09:20:22,595 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:34175'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,596 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:34175' closed.
2025-09-05 09:20:22,646 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.41:33441'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,647 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.41:33441' closed.
2025-09-05 09:20:22,649 - distributed.dask_worker - INFO - End worker
