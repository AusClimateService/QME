Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-03 10:31:43,536 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:43249'
2025-09-03 10:31:43,547 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:33155'
2025-09-03 10:31:43,552 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:46347'
2025-09-03 10:31:43,558 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:43901'
2025-09-03 10:31:43,565 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:38527'
2025-09-03 10:31:43,614 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:42663'
2025-09-03 10:31:43,618 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:43625'
2025-09-03 10:31:43,622 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:33049'
2025-09-03 10:31:43,628 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:41693'
2025-09-03 10:31:43,634 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:35895'
2025-09-03 10:31:43,638 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:41227'
2025-09-03 10:31:43,642 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:39929'
2025-09-03 10:31:43,650 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:38409'
2025-09-03 10:31:43,655 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:36423'
2025-09-03 10:31:43,660 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:42459'
2025-09-03 10:31:43,664 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:39831'
2025-09-03 10:31:43,668 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:46649'
2025-09-03 10:31:43,673 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:35307'
2025-09-03 10:31:43,678 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:45399'
2025-09-03 10:31:43,682 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:35973'
2025-09-03 10:31:43,687 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:41455'
2025-09-03 10:31:43,692 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:37727'
2025-09-03 10:31:43,697 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:45417'
2025-09-03 10:31:43,702 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:46079'
2025-09-03 10:31:43,706 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:42457'
2025-09-03 10:31:43,710 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:37909'
2025-09-03 10:31:43,714 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:35627'
2025-09-03 10:31:43,720 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:40911'
2025-09-03 10:31:43,724 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:45393'
2025-09-03 10:31:43,728 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:36641'
2025-09-03 10:31:43,733 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:44631'
2025-09-03 10:31:43,737 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:36597'
2025-09-03 10:31:43,740 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:41047'
2025-09-03 10:31:43,743 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:40659'
2025-09-03 10:31:43,748 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:46615'
2025-09-03 10:31:43,754 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:42137'
2025-09-03 10:31:43,758 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:36231'
2025-09-03 10:31:43,763 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:42731'
2025-09-03 10:31:43,767 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:33747'
2025-09-03 10:31:43,771 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:37429'
2025-09-03 10:31:43,775 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:35477'
2025-09-03 10:31:43,779 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:42067'
2025-09-03 10:31:43,854 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:44399'
2025-09-03 10:31:43,859 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:36853'
2025-09-03 10:31:43,865 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:33239'
2025-09-03 10:31:43,870 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:41139'
2025-09-03 10:31:43,875 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:39181'
2025-09-03 10:31:43,880 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:42027'
2025-09-03 10:31:43,884 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:33815'
2025-09-03 10:31:43,889 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:36949'
2025-09-03 10:31:43,894 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:37787'
2025-09-03 10:31:43,897 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:46201'
2025-09-03 10:31:43,902 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:33805'
2025-09-03 10:31:43,906 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:35629'
2025-09-03 10:31:43,911 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:33645'
2025-09-03 10:31:43,915 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:34563'
2025-09-03 10:31:43,920 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:34253'
2025-09-03 10:31:43,926 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:43453'
2025-09-03 10:31:43,931 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:40147'
2025-09-03 10:31:43,935 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:36465'
2025-09-03 10:31:43,941 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:45515'
2025-09-03 10:31:43,946 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:33039'
2025-09-03 10:31:43,950 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:36401'
2025-09-03 10:31:43,954 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:37985'
2025-09-03 10:31:43,957 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:40539'
2025-09-03 10:31:43,961 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:39701'
2025-09-03 10:31:43,966 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:38805'
2025-09-03 10:31:43,970 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:44577'
2025-09-03 10:31:43,975 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:39395'
2025-09-03 10:31:43,980 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:43957'
2025-09-03 10:31:43,985 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:38137'
2025-09-03 10:31:43,990 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:40785'
2025-09-03 10:31:43,995 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:41711'
2025-09-03 10:31:44,001 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:36677'
2025-09-03 10:31:44,004 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:37471'
2025-09-03 10:31:44,008 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:40775'
2025-09-03 10:31:44,012 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:41141'
2025-09-03 10:31:44,031 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:38267'
2025-09-03 10:31:44,037 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:42767'
2025-09-03 10:31:44,043 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:36247'
2025-09-03 10:31:44,046 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:41383'
2025-09-03 10:31:44,051 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:45653'
2025-09-03 10:31:44,056 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:38791'
2025-09-03 10:31:44,059 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:46149'
2025-09-03 10:31:44,062 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:36797'
2025-09-03 10:31:44,067 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:45013'
2025-09-03 10:31:44,073 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:44081'
2025-09-03 10:31:44,078 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:37245'
2025-09-03 10:31:44,083 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:45273'
2025-09-03 10:31:44,199 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:33851'
2025-09-03 10:31:45,671 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:45721
2025-09-03 10:31:45,672 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:38177
2025-09-03 10:31:45,672 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:39735
2025-09-03 10:31:45,672 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:44937
2025-09-03 10:31:45,672 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:42535
2025-09-03 10:31:45,672 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:45099
2025-09-03 10:31:45,672 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:46257
2025-09-03 10:31:45,672 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:39399
2025-09-03 10:31:45,672 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:45721
2025-09-03 10:31:45,672 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:46059
2025-09-03 10:31:45,672 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:38177
2025-09-03 10:31:45,672 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:36833
2025-09-03 10:31:45,672 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:44295
2025-09-03 10:31:45,672 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:40449
2025-09-03 10:31:45,672 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:39735
2025-09-03 10:31:45,672 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:44937
2025-09-03 10:31:45,672 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:40251
2025-09-03 10:31:45,672 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:41645
2025-09-03 10:31:45,672 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:42535
2025-09-03 10:31:45,672 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:45099
2025-09-03 10:31:45,672 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:46257
2025-09-03 10:31:45,672 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:39399
2025-09-03 10:31:45,672 - distributed.worker - INFO -          dashboard at:          10.6.102.24:41451
2025-09-03 10:31:45,672 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:45741
2025-09-03 10:31:45,672 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:46059
2025-09-03 10:31:45,672 - distributed.worker - INFO -          dashboard at:          10.6.102.24:44301
2025-09-03 10:31:45,672 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:36833
2025-09-03 10:31:45,672 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:36583
2025-09-03 10:31:45,672 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:44295
2025-09-03 10:31:45,672 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:40449
2025-09-03 10:31:45,672 - distributed.worker - INFO -          dashboard at:          10.6.102.24:43681
2025-09-03 10:31:45,672 - distributed.worker - INFO -          dashboard at:          10.6.102.24:40981
2025-09-03 10:31:45,672 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:40251
2025-09-03 10:31:45,672 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:41645
2025-09-03 10:31:45,672 - distributed.worker - INFO -          dashboard at:          10.6.102.24:45993
2025-09-03 10:31:45,672 - distributed.worker - INFO -          dashboard at:          10.6.102.24:44017
2025-09-03 10:31:45,672 - distributed.worker - INFO -          dashboard at:          10.6.102.24:33065
2025-09-03 10:31:45,672 - distributed.worker - INFO -          dashboard at:          10.6.102.24:37381
2025-09-03 10:31:45,672 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,672 - distributed.worker - INFO -          dashboard at:          10.6.102.24:38191
2025-09-03 10:31:45,672 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:45741
2025-09-03 10:31:45,672 - distributed.worker - INFO -          dashboard at:          10.6.102.24:42823
2025-09-03 10:31:45,672 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,672 - distributed.worker - INFO -          dashboard at:          10.6.102.24:43417
2025-09-03 10:31:45,672 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:36583
2025-09-03 10:31:45,672 - distributed.worker - INFO -          dashboard at:          10.6.102.24:45671
2025-09-03 10:31:45,672 - distributed.worker - INFO -          dashboard at:          10.6.102.24:35661
2025-09-03 10:31:45,672 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,672 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,672 - distributed.worker - INFO -          dashboard at:          10.6.102.24:40967
2025-09-03 10:31:45,672 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,672 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,672 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,672 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,672 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,672 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,672 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,672 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,672 - distributed.worker - INFO -          dashboard at:          10.6.102.24:36559
2025-09-03 10:31:45,672 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,672 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,672 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,672 - distributed.worker - INFO -          dashboard at:          10.6.102.24:35031
2025-09-03 10:31:45,672 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,672 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,672 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,672 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,672 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,672 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,672 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,672 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,672 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,672 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,672 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,672 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,672 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,672 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,672 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,672 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,672 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,672 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,672 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,672 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,672 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,672 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,672 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,672 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,672 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,673 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,673 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,673 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,673 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,673 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,673 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,673 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,673 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,673 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,673 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,673 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,673 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,673 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-i_76dlyk
2025-09-03 10:31:45,673 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,673 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,673 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-d0jh4tzs
2025-09-03 10:31:45,673 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,673 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,673 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,673 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,673 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-fiv87ngh
2025-09-03 10:31:45,673 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,673 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-g6u8tdc_
2025-09-03 10:31:45,673 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-_2d_aota
2025-09-03 10:31:45,673 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,673 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-yqg6x3f8
2025-09-03 10:31:45,673 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-y5jxcvgd
2025-09-03 10:31:45,673 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-63tyy4qt
2025-09-03 10:31:45,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,673 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-h7ebp4sj
2025-09-03 10:31:45,673 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ziuo46k0
2025-09-03 10:31:45,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,673 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,673 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-pm9r4v1z
2025-09-03 10:31:45,673 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-z7ap25ia
2025-09-03 10:31:45,673 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-d1jt3luj
2025-09-03 10:31:45,673 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:33761
2025-09-03 10:31:45,673 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,673 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:46595
2025-09-03 10:31:45,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,673 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-dtfcusux
2025-09-03 10:31:45,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,673 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-_ufv3sbu
2025-09-03 10:31:45,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,673 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-las5m6ik
2025-09-03 10:31:45,673 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:33761
2025-09-03 10:31:45,673 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:46595
2025-09-03 10:31:45,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,673 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:35323
2025-09-03 10:31:45,673 - distributed.worker - INFO -          dashboard at:          10.6.102.24:33241
2025-09-03 10:31:45,673 - distributed.worker - INFO -          dashboard at:          10.6.102.24:35685
2025-09-03 10:31:45,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,673 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,673 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:35323
2025-09-03 10:31:45,673 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,673 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,673 - distributed.worker - INFO -          dashboard at:          10.6.102.24:37553
2025-09-03 10:31:45,673 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,673 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,673 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,673 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,673 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:42237
2025-09-03 10:31:45,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,673 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-vd8fkk7t
2025-09-03 10:31:45,673 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-012gc45a
2025-09-03 10:31:45,673 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,673 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:42237
2025-09-03 10:31:45,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,673 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,674 - distributed.worker - INFO -          dashboard at:          10.6.102.24:35537
2025-09-03 10:31:45,674 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-i2bl725d
2025-09-03 10:31:45,674 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,674 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,674 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,674 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,674 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,674 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-idh1fcsk
2025-09-03 10:31:45,674 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,675 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:37243
2025-09-03 10:31:45,675 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:37243
2025-09-03 10:31:45,675 - distributed.worker - INFO -          dashboard at:          10.6.102.24:39349
2025-09-03 10:31:45,675 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,675 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,675 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,675 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,676 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-4v8m15k7
2025-09-03 10:31:45,676 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,677 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:43813
2025-09-03 10:31:45,677 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:43813
2025-09-03 10:31:45,678 - distributed.worker - INFO -          dashboard at:          10.6.102.24:41475
2025-09-03 10:31:45,678 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,678 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,678 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,678 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,678 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-4c5qyxcg
2025-09-03 10:31:45,678 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,679 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:46871
2025-09-03 10:31:45,679 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:46871
2025-09-03 10:31:45,679 - distributed.worker - INFO -          dashboard at:          10.6.102.24:35557
2025-09-03 10:31:45,679 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,679 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,679 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,680 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,680 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-6g4a5rq7
2025-09-03 10:31:45,680 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,680 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:38503
2025-09-03 10:31:45,680 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:45039
2025-09-03 10:31:45,680 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:38503
2025-09-03 10:31:45,680 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:45039
2025-09-03 10:31:45,680 - distributed.worker - INFO -          dashboard at:          10.6.102.24:43341
2025-09-03 10:31:45,680 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:42299
2025-09-03 10:31:45,680 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:44339
2025-09-03 10:31:45,681 - distributed.worker - INFO -          dashboard at:          10.6.102.24:44169
2025-09-03 10:31:45,681 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,680 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:46729
2025-09-03 10:31:45,681 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:42299
2025-09-03 10:31:45,681 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:33449
2025-09-03 10:31:45,681 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,681 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:44339
2025-09-03 10:31:45,681 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,681 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:35883
2025-09-03 10:31:45,681 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:46729
2025-09-03 10:31:45,681 - distributed.worker - INFO -          dashboard at:          10.6.102.24:37601
2025-09-03 10:31:45,681 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:40601
2025-09-03 10:31:45,681 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,681 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:33449
2025-09-03 10:31:45,681 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,681 - distributed.worker - INFO -          dashboard at:          10.6.102.24:45015
2025-09-03 10:31:45,681 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:35883
2025-09-03 10:31:45,681 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,681 - distributed.worker - INFO -          dashboard at:          10.6.102.24:44153
2025-09-03 10:31:45,681 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,681 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:40601
2025-09-03 10:31:45,681 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:37021
2025-09-03 10:31:45,681 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,681 - distributed.worker - INFO -          dashboard at:          10.6.102.24:36325
2025-09-03 10:31:45,681 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-zl6e8ems
2025-09-03 10:31:45,681 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,681 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,681 - distributed.worker - INFO -          dashboard at:          10.6.102.24:32813
2025-09-03 10:31:45,681 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:37021
2025-09-03 10:31:45,681 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,681 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,681 - distributed.worker - INFO -          dashboard at:          10.6.102.24:46449
2025-09-03 10:31:45,681 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,681 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,681 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,681 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,681 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,681 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,681 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,681 - distributed.worker - INFO -          dashboard at:          10.6.102.24:34171
2025-09-03 10:31:45,682 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,682 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,682 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,682 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,682 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-4wbazcgp
2025-09-03 10:31:45,682 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,682 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,682 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,682 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,682 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,682 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,682 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,682 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-y8w9g_sg
2025-09-03 10:31:45,682 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,682 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,682 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,682 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,682 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,682 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-6kzfx6xy
2025-09-03 10:31:45,682 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,682 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-4py_zlkj
2025-09-03 10:31:45,682 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,682 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,682 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-522je50w
2025-09-03 10:31:45,682 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-1hsmvz45
2025-09-03 10:31:45,682 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,682 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-a5a56euw
2025-09-03 10:31:45,682 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,682 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,682 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,682 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,682 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,682 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-v8qo9ccr
2025-09-03 10:31:45,682 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,682 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:41629
2025-09-03 10:31:45,682 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:41629
2025-09-03 10:31:45,683 - distributed.worker - INFO -          dashboard at:          10.6.102.24:38475
2025-09-03 10:31:45,683 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,683 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,683 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,683 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,683 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-b2sbm8t3
2025-09-03 10:31:45,683 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,710 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:41809
2025-09-03 10:31:45,710 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:41809
2025-09-03 10:31:45,710 - distributed.worker - INFO -          dashboard at:          10.6.102.24:37837
2025-09-03 10:31:45,710 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:45,710 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:45,710 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:45,710 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:45,710 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-omjfyxns
2025-09-03 10:31:45,710 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,092 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:46597
2025-09-03 10:31:46,092 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:46597
2025-09-03 10:31:46,092 - distributed.worker - INFO -          dashboard at:          10.6.102.24:45737
2025-09-03 10:31:46,092 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,092 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,092 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,092 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,092 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-mcjzqmum
2025-09-03 10:31:46,092 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,095 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:42935
2025-09-03 10:31:46,095 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:42935
2025-09-03 10:31:46,096 - distributed.worker - INFO -          dashboard at:          10.6.102.24:40229
2025-09-03 10:31:46,096 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,096 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,096 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,096 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,096 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-u95shtj2
2025-09-03 10:31:46,096 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,129 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:40599
2025-09-03 10:31:46,129 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:40599
2025-09-03 10:31:46,130 - distributed.worker - INFO -          dashboard at:          10.6.102.24:42955
2025-09-03 10:31:46,130 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,130 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,130 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,130 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,130 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ldg9q5rc
2025-09-03 10:31:46,130 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,153 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:39455
2025-09-03 10:31:46,153 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:39455
2025-09-03 10:31:46,153 - distributed.worker - INFO -          dashboard at:          10.6.102.24:36095
2025-09-03 10:31:46,153 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,153 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,153 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,153 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,153 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-10hviq4n
2025-09-03 10:31:46,153 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,160 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:41631
2025-09-03 10:31:46,160 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:41631
2025-09-03 10:31:46,160 - distributed.worker - INFO -          dashboard at:          10.6.102.24:32825
2025-09-03 10:31:46,160 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,161 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,161 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,161 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,161 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ogzl30mb
2025-09-03 10:31:46,161 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,172 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:33933
2025-09-03 10:31:46,172 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:33933
2025-09-03 10:31:46,172 - distributed.worker - INFO -          dashboard at:          10.6.102.24:40419
2025-09-03 10:31:46,172 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,172 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,172 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,172 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,172 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-sly3jpc1
2025-09-03 10:31:46,172 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,179 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:34943
2025-09-03 10:31:46,179 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:34943
2025-09-03 10:31:46,179 - distributed.worker - INFO -          dashboard at:          10.6.102.24:35915
2025-09-03 10:31:46,179 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,179 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,179 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,179 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,179 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-m3900qzj
2025-09-03 10:31:46,180 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,180 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:35369
2025-09-03 10:31:46,180 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:35369
2025-09-03 10:31:46,180 - distributed.worker - INFO -          dashboard at:          10.6.102.24:42861
2025-09-03 10:31:46,180 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,180 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,180 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,180 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,180 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-mpsywt3c
2025-09-03 10:31:46,180 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,184 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:45167
2025-09-03 10:31:46,185 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:45167
2025-09-03 10:31:46,185 - distributed.worker - INFO -          dashboard at:          10.6.102.24:39637
2025-09-03 10:31:46,185 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,185 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,185 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,185 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,185 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-io9oyltr
2025-09-03 10:31:46,185 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,185 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:33859
2025-09-03 10:31:46,185 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:33859
2025-09-03 10:31:46,185 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:39771
2025-09-03 10:31:46,185 - distributed.worker - INFO -          dashboard at:          10.6.102.24:36461
2025-09-03 10:31:46,186 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:39771
2025-09-03 10:31:46,186 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,186 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,186 - distributed.worker - INFO -          dashboard at:          10.6.102.24:43475
2025-09-03 10:31:46,186 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,186 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,186 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,186 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,186 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,186 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-nhnvgq0z
2025-09-03 10:31:46,186 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,186 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,186 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-1_8omvuk
2025-09-03 10:31:46,186 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,186 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:39195
2025-09-03 10:31:46,186 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:39195
2025-09-03 10:31:46,186 - distributed.worker - INFO -          dashboard at:          10.6.102.24:40871
2025-09-03 10:31:46,186 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,186 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,186 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,186 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,186 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-8iua1e16
2025-09-03 10:31:46,186 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,200 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:37103
2025-09-03 10:31:46,200 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:37103
2025-09-03 10:31:46,200 - distributed.worker - INFO -          dashboard at:          10.6.102.24:40773
2025-09-03 10:31:46,200 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,200 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,200 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,200 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,200 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-a13s_mj6
2025-09-03 10:31:46,200 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,205 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:41337
2025-09-03 10:31:46,205 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:41337
2025-09-03 10:31:46,205 - distributed.worker - INFO -          dashboard at:          10.6.102.24:41911
2025-09-03 10:31:46,205 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,205 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,205 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,205 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,205 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ryjrgsvu
2025-09-03 10:31:46,205 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,212 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:40309
2025-09-03 10:31:46,212 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:40309
2025-09-03 10:31:46,212 - distributed.worker - INFO -          dashboard at:          10.6.102.24:40191
2025-09-03 10:31:46,212 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,212 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,212 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,212 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,212 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-4mpf20g_
2025-09-03 10:31:46,212 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,213 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:39035
2025-09-03 10:31:46,213 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:39035
2025-09-03 10:31:46,213 - distributed.worker - INFO -          dashboard at:          10.6.102.24:40937
2025-09-03 10:31:46,213 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,213 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,213 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,213 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,213 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-y1a4mlk3
2025-09-03 10:31:46,213 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,218 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:42141
2025-09-03 10:31:46,218 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:42141
2025-09-03 10:31:46,218 - distributed.worker - INFO -          dashboard at:          10.6.102.24:40689
2025-09-03 10:31:46,218 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:39377
2025-09-03 10:31:46,218 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,218 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,218 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:39377
2025-09-03 10:31:46,218 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,218 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,218 - distributed.worker - INFO -          dashboard at:          10.6.102.24:40947
2025-09-03 10:31:46,218 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,218 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-6117bkfo
2025-09-03 10:31:46,218 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,218 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,218 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,219 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,219 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-j8gapapc
2025-09-03 10:31:46,219 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,219 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:41343
2025-09-03 10:31:46,219 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:41343
2025-09-03 10:31:46,219 - distributed.worker - INFO -          dashboard at:          10.6.102.24:38323
2025-09-03 10:31:46,219 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,219 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,219 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,219 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,219 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-txjyknqt
2025-09-03 10:31:46,219 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,222 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:33969
2025-09-03 10:31:46,222 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:33969
2025-09-03 10:31:46,222 - distributed.worker - INFO -          dashboard at:          10.6.102.24:36441
2025-09-03 10:31:46,222 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,223 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,223 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,223 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,223 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ragr6vts
2025-09-03 10:31:46,223 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,225 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:37105
2025-09-03 10:31:46,225 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:37105
2025-09-03 10:31:46,225 - distributed.worker - INFO -          dashboard at:          10.6.102.24:46499
2025-09-03 10:31:46,225 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,225 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,225 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,225 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,225 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-chvd73pb
2025-09-03 10:31:46,225 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,227 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:45765
2025-09-03 10:31:46,227 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:45765
2025-09-03 10:31:46,227 - distributed.worker - INFO -          dashboard at:          10.6.102.24:38519
2025-09-03 10:31:46,227 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,227 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,227 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,227 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,227 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-kb1j0f54
2025-09-03 10:31:46,227 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,229 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:37549
2025-09-03 10:31:46,229 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:37549
2025-09-03 10:31:46,229 - distributed.worker - INFO -          dashboard at:          10.6.102.24:37101
2025-09-03 10:31:46,229 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,229 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,229 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,229 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,229 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-p9ej73b0
2025-09-03 10:31:46,229 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,330 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:34297
2025-09-03 10:31:46,330 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:34297
2025-09-03 10:31:46,330 - distributed.worker - INFO -          dashboard at:          10.6.102.24:33561
2025-09-03 10:31:46,330 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,330 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,330 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,330 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,330 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-2rqrqp32
2025-09-03 10:31:46,330 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,342 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:40443
2025-09-03 10:31:46,342 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:40443
2025-09-03 10:31:46,342 - distributed.worker - INFO -          dashboard at:          10.6.102.24:34481
2025-09-03 10:31:46,342 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,342 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,342 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,342 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,342 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-i0jhrwi7
2025-09-03 10:31:46,342 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,389 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:41055
2025-09-03 10:31:46,390 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:41055
2025-09-03 10:31:46,390 - distributed.worker - INFO -          dashboard at:          10.6.102.24:35449
2025-09-03 10:31:46,390 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,390 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,390 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,390 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,390 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-sicty4g1
2025-09-03 10:31:46,390 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,421 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:34295
2025-09-03 10:31:46,421 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:34295
2025-09-03 10:31:46,421 - distributed.worker - INFO -          dashboard at:          10.6.102.24:39153
2025-09-03 10:31:46,422 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,422 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,422 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,422 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,422 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-lwc9qcq6
2025-09-03 10:31:46,422 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,471 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:42965
2025-09-03 10:31:46,471 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:42965
2025-09-03 10:31:46,471 - distributed.worker - INFO -          dashboard at:          10.6.102.24:46721
2025-09-03 10:31:46,471 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,471 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,471 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,471 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,471 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-nap9nw0y
2025-09-03 10:31:46,471 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,512 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:35245
2025-09-03 10:31:46,512 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:35245
2025-09-03 10:31:46,512 - distributed.worker - INFO -          dashboard at:          10.6.102.24:41899
2025-09-03 10:31:46,512 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,512 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,512 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,512 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,512 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-otgjzphz
2025-09-03 10:31:46,512 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,513 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:43701
2025-09-03 10:31:46,513 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:43701
2025-09-03 10:31:46,513 - distributed.worker - INFO -          dashboard at:          10.6.102.24:42529
2025-09-03 10:31:46,513 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,513 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,513 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,513 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,513 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-8ojuguba
2025-09-03 10:31:46,513 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,519 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:33767
2025-09-03 10:31:46,519 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:33767
2025-09-03 10:31:46,519 - distributed.worker - INFO -          dashboard at:          10.6.102.24:43215
2025-09-03 10:31:46,519 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,519 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,519 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,519 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,519 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-s2_ls704
2025-09-03 10:31:46,519 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,581 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:33081
2025-09-03 10:31:46,581 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:33081
2025-09-03 10:31:46,581 - distributed.worker - INFO -          dashboard at:          10.6.102.24:33457
2025-09-03 10:31:46,581 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,581 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,581 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,581 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,581 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-nybvsp0y
2025-09-03 10:31:46,581 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,603 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:37533
2025-09-03 10:31:46,603 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:37533
2025-09-03 10:31:46,603 - distributed.worker - INFO -          dashboard at:          10.6.102.24:33819
2025-09-03 10:31:46,603 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,603 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,603 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,603 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,603 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-m_qwo4a9
2025-09-03 10:31:46,603 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,649 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:33353
2025-09-03 10:31:46,649 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:33353
2025-09-03 10:31:46,649 - distributed.worker - INFO -          dashboard at:          10.6.102.24:36051
2025-09-03 10:31:46,649 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,649 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,649 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,649 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,649 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-9vkjn6jq
2025-09-03 10:31:46,649 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,807 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:32781
2025-09-03 10:31:46,807 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:32781
2025-09-03 10:31:46,807 - distributed.worker - INFO -          dashboard at:          10.6.102.24:37255
2025-09-03 10:31:46,807 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,807 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,807 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,807 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,807 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-2rxd09qq
2025-09-03 10:31:46,807 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,811 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:41371
2025-09-03 10:31:46,811 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:41371
2025-09-03 10:31:46,811 - distributed.worker - INFO -          dashboard at:          10.6.102.24:35253
2025-09-03 10:31:46,811 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,811 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,811 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,811 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,811 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-g0vhxq8t
2025-09-03 10:31:46,811 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,812 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:36157
2025-09-03 10:31:46,812 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:36157
2025-09-03 10:31:46,812 - distributed.worker - INFO -          dashboard at:          10.6.102.24:40421
2025-09-03 10:31:46,812 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,812 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,812 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,813 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,813 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-qa4l9scq
2025-09-03 10:31:46,813 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,817 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:41363
2025-09-03 10:31:46,817 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:41363
2025-09-03 10:31:46,817 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:37881
2025-09-03 10:31:46,817 - distributed.worker - INFO -          dashboard at:          10.6.102.24:45309
2025-09-03 10:31:46,817 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:37881
2025-09-03 10:31:46,817 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,817 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,817 - distributed.worker - INFO -          dashboard at:          10.6.102.24:46645
2025-09-03 10:31:46,817 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,817 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,817 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,817 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,817 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,817 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-dtkpa_sm
2025-09-03 10:31:46,817 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,817 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,817 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-oos5h_xk
2025-09-03 10:31:46,817 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,817 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:39659
2025-09-03 10:31:46,817 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:39659
2025-09-03 10:31:46,817 - distributed.worker - INFO -          dashboard at:          10.6.102.24:43595
2025-09-03 10:31:46,817 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,817 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,817 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,817 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,818 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ibnb7de_
2025-09-03 10:31:46,817 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:45751
2025-09-03 10:31:46,818 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,818 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:45751
2025-09-03 10:31:46,818 - distributed.worker - INFO -          dashboard at:          10.6.102.24:41919
2025-09-03 10:31:46,818 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,818 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,818 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,818 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,818 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-m3oc7m52
2025-09-03 10:31:46,818 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,821 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:44257
2025-09-03 10:31:46,821 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:44257
2025-09-03 10:31:46,821 - distributed.worker - INFO -          dashboard at:          10.6.102.24:35309
2025-09-03 10:31:46,821 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,821 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,821 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,821 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,821 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-s3rdp_xp
2025-09-03 10:31:46,821 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,821 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:46477
2025-09-03 10:31:46,821 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:46477
2025-09-03 10:31:46,821 - distributed.worker - INFO -          dashboard at:          10.6.102.24:38047
2025-09-03 10:31:46,821 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,821 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,821 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,821 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,821 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-6f2iu31c
2025-09-03 10:31:46,822 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,828 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:44769
2025-09-03 10:31:46,828 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:44769
2025-09-03 10:31:46,828 - distributed.worker - INFO -          dashboard at:          10.6.102.24:33729
2025-09-03 10:31:46,828 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,829 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,829 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,829 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,829 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-opttu9v4
2025-09-03 10:31:46,829 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,830 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:32909
2025-09-03 10:31:46,830 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:32909
2025-09-03 10:31:46,830 - distributed.worker - INFO -          dashboard at:          10.6.102.24:38145
2025-09-03 10:31:46,830 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,830 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,830 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,830 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,830 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-b5cp9jig
2025-09-03 10:31:46,830 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,830 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:44957
2025-09-03 10:31:46,831 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:44957
2025-09-03 10:31:46,831 - distributed.worker - INFO -          dashboard at:          10.6.102.24:40869
2025-09-03 10:31:46,831 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,831 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,831 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,831 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,831 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-wvv_bj_q
2025-09-03 10:31:46,831 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,832 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:33653
2025-09-03 10:31:46,832 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:33653
2025-09-03 10:31:46,832 - distributed.worker - INFO -          dashboard at:          10.6.102.24:36163
2025-09-03 10:31:46,832 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,832 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,832 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,832 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,832 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-f018kd8k
2025-09-03 10:31:46,832 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,833 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:39757
2025-09-03 10:31:46,833 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:39757
2025-09-03 10:31:46,833 - distributed.worker - INFO -          dashboard at:          10.6.102.24:38345
2025-09-03 10:31:46,833 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,833 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,833 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,833 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,833 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-p6stekfv
2025-09-03 10:31:46,833 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,834 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:38319
2025-09-03 10:31:46,835 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:38319
2025-09-03 10:31:46,835 - distributed.worker - INFO -          dashboard at:          10.6.102.24:41413
2025-09-03 10:31:46,835 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,835 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,835 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,835 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,835 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-bdzk8n1a
2025-09-03 10:31:46,835 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,836 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:35743
2025-09-03 10:31:46,836 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:35743
2025-09-03 10:31:46,836 - distributed.worker - INFO -          dashboard at:          10.6.102.24:38459
2025-09-03 10:31:46,836 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,836 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,836 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,836 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,836 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:34075
2025-09-03 10:31:46,836 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-u9947u1a
2025-09-03 10:31:46,836 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:34075
2025-09-03 10:31:46,836 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,836 - distributed.worker - INFO -          dashboard at:          10.6.102.24:43593
2025-09-03 10:31:46,836 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,836 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,836 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,836 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,836 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-sm7qhfh0
2025-09-03 10:31:46,836 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,837 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:45931
2025-09-03 10:31:46,837 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:45931
2025-09-03 10:31:46,837 - distributed.worker - INFO -          dashboard at:          10.6.102.24:37319
2025-09-03 10:31:46,837 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,837 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,837 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,837 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,837 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-utubtw31
2025-09-03 10:31:46,837 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,838 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:42273
2025-09-03 10:31:46,838 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:42273
2025-09-03 10:31:46,838 - distributed.worker - INFO -          dashboard at:          10.6.102.24:41989
2025-09-03 10:31:46,838 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,838 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,838 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,838 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,838 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-y9wew4t2
2025-09-03 10:31:46,838 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,842 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:46715
2025-09-03 10:31:46,842 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:46715
2025-09-03 10:31:46,842 - distributed.worker - INFO -          dashboard at:          10.6.102.24:40247
2025-09-03 10:31:46,842 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,842 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,842 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,842 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,842 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-n1prsegq
2025-09-03 10:31:46,842 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,843 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:33459
2025-09-03 10:31:46,844 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:33459
2025-09-03 10:31:46,844 - distributed.worker - INFO -          dashboard at:          10.6.102.24:46869
2025-09-03 10:31:46,844 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,844 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,844 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,844 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,844 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-854xmn4s
2025-09-03 10:31:46,844 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,858 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:39957
2025-09-03 10:31:46,858 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:39957
2025-09-03 10:31:46,858 - distributed.worker - INFO -          dashboard at:          10.6.102.24:43217
2025-09-03 10:31:46,858 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:46,858 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:46,858 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:46,858 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:46,858 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-t6tx33oj
2025-09-03 10:31:46,858 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,385 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:47,386 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,386 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,388 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:47,437 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:47,438 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,438 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,440 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:47,462 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:47,463 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,463 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,465 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:47,475 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:47,476 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,476 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,478 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:47,534 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:47,535 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,535 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,537 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:47,862 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:47,863 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:47,863 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:47,865 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,042 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,043 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,043 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,045 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,054 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,055 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,056 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,058 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,067 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,068 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,068 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,070 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,081 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,083 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,083 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,087 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,092 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,093 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,094 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,095 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,105 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,106 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,106 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,108 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,118 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,119 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,119 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,121 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,130 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,131 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,132 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,133 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,147 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,152 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,152 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,156 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,157 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,157 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,158 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,159 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,169 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,171 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,171 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,175 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,181 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,182 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,182 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,184 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,193 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,195 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,197 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,206 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,207 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,207 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,209 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,219 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,222 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,222 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,226 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,231 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,232 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,232 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,234 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,243 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,245 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,245 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,247 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,257 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,258 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,258 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,260 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,269 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,270 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,270 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,272 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,282 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,283 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,283 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,285 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,294 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,296 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,296 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,298 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,307 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,309 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,309 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,311 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,320 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,321 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,321 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,323 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,333 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,334 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,336 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,345 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,347 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,347 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,349 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,358 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,359 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,359 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,361 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,370 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,371 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,372 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,374 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,383 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,384 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,384 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,386 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,423 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,424 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,424 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,426 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,489 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,490 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,491 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,493 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,515 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,516 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,516 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,518 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,518 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,524 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,524 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,527 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,528 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,529 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,531 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,531 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,541 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,542 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,542 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,544 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,554 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,555 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,555 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,557 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,566 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,567 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,568 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,570 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,579 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,581 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,581 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,583 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,684 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,685 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,685 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,687 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,697 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,698 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,698 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,700 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,709 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,711 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,711 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,713 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,722 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,723 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,724 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,726 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,735 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,736 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,736 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,738 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,748 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,749 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,750 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,751 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,760 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,762 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,762 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,764 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,773 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,775 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,775 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,777 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,786 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,787 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,787 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,789 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,799 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,800 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,801 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,803 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,812 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,813 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,813 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,815 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,824 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,825 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,826 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,827 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,838 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,839 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,839 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,841 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,850 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,851 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,852 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,854 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,863 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,864 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,864 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,866 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:48,876 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:48,877 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:48,877 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:48,879 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:49,735 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:36177'
2025-09-03 10:31:49,738 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:32811'
2025-09-03 10:31:49,741 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:44133'
2025-09-03 10:31:49,746 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:37789'
2025-09-03 10:31:49,749 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:40153'
2025-09-03 10:31:49,882 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:40303'
2025-09-03 10:31:49,886 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:42435'
2025-09-03 10:31:49,889 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:40699'
2025-09-03 10:31:49,894 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:33539'
2025-09-03 10:31:49,897 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:40155'
2025-09-03 10:31:49,903 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:44735'
2025-09-03 10:31:49,909 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:43255'
2025-09-03 10:31:49,913 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:33625'
2025-09-03 10:31:49,919 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.24:44493'
2025-09-03 10:31:50,567 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:38363
2025-09-03 10:31:50,567 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:38363
2025-09-03 10:31:50,567 - distributed.worker - INFO -          dashboard at:          10.6.102.24:45519
2025-09-03 10:31:50,567 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,567 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,567 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:50,567 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:50,567 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-a3d9h5yb
2025-09-03 10:31:50,567 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,570 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:34047
2025-09-03 10:31:50,570 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:34047
2025-09-03 10:31:50,570 - distributed.worker - INFO -          dashboard at:          10.6.102.24:38325
2025-09-03 10:31:50,570 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,570 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,570 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:50,570 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:50,570 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ghedojb8
2025-09-03 10:31:50,570 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,586 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:35945
2025-09-03 10:31:50,586 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:35945
2025-09-03 10:31:50,586 - distributed.worker - INFO -          dashboard at:          10.6.102.24:46077
2025-09-03 10:31:50,586 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,586 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,586 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:50,586 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:50,586 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-pa28c7om
2025-09-03 10:31:50,586 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,593 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:43847
2025-09-03 10:31:50,593 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:43847
2025-09-03 10:31:50,593 - distributed.worker - INFO -          dashboard at:          10.6.102.24:39741
2025-09-03 10:31:50,593 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,593 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,593 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:50,593 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:50,593 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-fsxywid2
2025-09-03 10:31:50,593 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,619 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:39229
2025-09-03 10:31:50,619 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:39229
2025-09-03 10:31:50,619 - distributed.worker - INFO -          dashboard at:          10.6.102.24:37331
2025-09-03 10:31:50,619 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,619 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,619 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:50,619 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:50,619 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-swkk4393
2025-09-03 10:31:50,619 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,686 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:43101
2025-09-03 10:31:50,686 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:43101
2025-09-03 10:31:50,686 - distributed.worker - INFO -          dashboard at:          10.6.102.24:34591
2025-09-03 10:31:50,686 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,686 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,686 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:50,686 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:50,686 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-k8uyg8bc
2025-09-03 10:31:50,686 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,711 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:38265
2025-09-03 10:31:50,711 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:38265
2025-09-03 10:31:50,711 - distributed.worker - INFO -          dashboard at:          10.6.102.24:41247
2025-09-03 10:31:50,711 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,711 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,711 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:50,711 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:50,711 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-f0yk35ht
2025-09-03 10:31:50,711 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,720 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:42189
2025-09-03 10:31:50,721 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:42189
2025-09-03 10:31:50,721 - distributed.worker - INFO -          dashboard at:          10.6.102.24:35869
2025-09-03 10:31:50,721 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,721 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,721 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:50,721 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:50,721 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-4leogxgp
2025-09-03 10:31:50,721 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,733 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:40733
2025-09-03 10:31:50,733 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:40733
2025-09-03 10:31:50,733 - distributed.worker - INFO -          dashboard at:          10.6.102.24:42817
2025-09-03 10:31:50,734 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,734 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,734 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:50,734 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:50,734 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-9yplxeun
2025-09-03 10:31:50,734 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,751 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:34935
2025-09-03 10:31:50,754 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:34935
2025-09-03 10:31:50,752 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:40717
2025-09-03 10:31:50,754 - distributed.worker - INFO -          dashboard at:          10.6.102.24:43025
2025-09-03 10:31:50,754 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,754 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,754 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:40717
2025-09-03 10:31:50,754 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:50,754 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:50,754 - distributed.worker - INFO -          dashboard at:          10.6.102.24:38573
2025-09-03 10:31:50,754 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-gfrsf9vr
2025-09-03 10:31:50,754 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,754 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,754 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,754 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:50,754 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:50,754 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ivsqa0r9
2025-09-03 10:31:50,754 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,766 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:37439
2025-09-03 10:31:50,767 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:37439
2025-09-03 10:31:50,767 - distributed.worker - INFO -          dashboard at:          10.6.102.24:38001
2025-09-03 10:31:50,767 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,767 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,767 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:50,767 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:50,767 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-_pforuhc
2025-09-03 10:31:50,767 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,771 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:45523
2025-09-03 10:31:50,771 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:45523
2025-09-03 10:31:50,771 - distributed.worker - INFO -          dashboard at:          10.6.102.24:34909
2025-09-03 10:31:50,771 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,771 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,771 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:50,771 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:50,771 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-olsxhijt
2025-09-03 10:31:50,771 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,795 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.24:43153
2025-09-03 10:31:50,795 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.24:43153
2025-09-03 10:31:50,796 - distributed.worker - INFO -          dashboard at:          10.6.102.24:42769
2025-09-03 10:31:50,796 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,796 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,796 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:50,796 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:50,796 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-c1u0hghx
2025-09-03 10:31:50,796 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,019 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:51,020 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,020 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,022 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:51,032 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:51,033 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,033 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,036 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:51,058 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:51,059 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,059 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,062 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:51,097 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:51,099 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,099 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,101 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:51,256 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:51,258 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,258 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,260 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,367 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,369 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,369 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,370 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,381 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,382 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,383 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,384 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,394 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,396 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,396 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,398 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,408 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,409 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,409 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,411 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,097 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,098 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,098 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,100 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,111 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,113 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,113 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,115 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,306 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,307 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,309 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,321 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,322 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,322 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,324 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:57,138 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:57,139 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,139 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,141 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:57,166 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:57,167 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,167 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,169 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:57,493 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:57,495 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:57,495 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:57,497 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,236 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,238 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,238 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,240 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,324 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,325 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,325 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,327 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,352 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,354 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,354 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,355 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,917 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,918 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,918 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,920 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,932 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,933 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,933 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,935 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,947 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,949 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,949 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,951 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:16,582 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:16,808 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:16,813 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:16,814 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:16,818 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:16,819 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:16,819 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:16,819 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:16,822 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:16,822 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:16,830 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:16,831 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:16,832 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:16,833 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:16,834 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:16,836 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:16,838 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:16,838 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:16,838 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:16,839 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:16,844 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:16,845 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:16,859 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:18,868 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:19,353 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:19,355 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:19,355 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:19,357 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:19,829 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:19,848 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:19,855 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:19,869 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:19,882 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:22,065 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:22,105 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:27,102 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:27,311 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:27,327 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:29,244 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:29,329 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:29,357 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:46,909 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:46,915 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:46,916 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:46,920 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:46,920 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:46,920 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:46,921 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:46,924 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:46,924 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:46,932 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:46,932 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:46,933 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:46,935 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:46,936 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:46,938 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:46,939 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:46,940 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:46,940 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:46,941 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:46,946 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:46,947 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:46,960 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:49,868 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:50,615 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:50,617 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:50,617 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:50,618 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:50,635 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:50,636 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:50,637 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:50,638 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:50,652 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:50,653 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:50,654 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:50,655 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:50,869 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:50,882 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:50,962 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:50,964 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:50,964 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:50,965 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:50,979 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:50,980 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:50,981 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:50,982 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:50,997 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:50,998 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:50,998 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,000 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,014 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,015 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,015 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,017 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,031 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,032 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,033 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,034 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,049 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,050 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,050 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,052 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,066 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,067 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,067 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,069 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,083 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,085 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,085 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,086 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,100 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,101 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,101 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,103 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,117 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,118 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,118 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,120 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,134 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,136 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,136 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,138 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,152 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,153 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,153 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,155 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,168 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,169 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,169 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,171 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,186 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,187 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,187 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,189 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,203 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,205 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,205 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,207 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,221 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,222 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,222 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,224 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,238 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,239 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,240 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,241 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,255 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,256 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,257 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,258 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,272 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:51,273 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,274 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:51,275 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:53,063 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:53,105 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:35:51,471 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,477 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,492 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,499 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,502 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,503 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,615 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,619 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,722 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,724 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,921 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,922 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,943 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,945 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,946 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,950 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,062 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,065 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,101 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,103 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,754 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,756 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,100 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,105 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,355 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,361 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,367 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,368 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,387 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,389 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,414 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,416 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,417 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,417 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,439 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,445 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,448 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,454 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,531 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,533 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,605 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,613 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,957 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,959 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,146 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,148 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,282 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,284 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,293 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,294 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,297 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,304 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,312 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,318 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,324 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,325 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,372 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,373 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,381 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,384 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,467 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,473 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,874 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,877 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,884 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,882 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,905 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,907 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,009 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,014 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,057 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,062 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,087 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,093 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,159 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,165 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,225 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,230 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,230 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,232 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,253 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,259 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,272 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,274 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,277 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,282 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,757 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,760 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,803 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,808 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,891 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,895 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,013 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,014 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,128 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,130 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,377 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,380 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,566 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,571 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,632 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,634 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,634 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,635 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,669 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,681 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,704 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,706 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,832 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,833 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,056 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,059 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,060 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,061 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,395 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,400 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,443 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,444 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,575 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,577 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,580 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,581 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,633 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,635 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,654 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,656 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,703 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,705 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,706 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,708 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,753 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,754 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,914 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,915 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,941 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,947 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,958 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,964 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,084 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,086 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,142 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,147 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,388 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,394 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,494 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,500 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,628 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,631 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,633 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,634 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,667 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,669 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,832 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,835 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,044 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,045 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,084 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,086 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,173 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,178 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,288 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,289 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,328 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,335 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,406 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,412 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,713 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,716 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,803 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,805 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,950 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,955 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,956 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,959 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,961 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,963 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,562 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,564 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,232 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,233 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,713 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,719 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,004 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,005 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,235 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,236 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,330 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,336 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,413 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,419 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,774 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,780 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:03,230 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:03,232 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:03,439 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:03,441 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:04,525 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:04,533 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:04,766 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:04,769 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:07,404 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:07,409 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:08,084 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:08,086 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:10,321 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:10,322 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:11,612 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:11,614 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:22,886 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,887 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,889 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,889 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,892 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,894 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,898 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,900 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,903 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,905 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,904 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,908 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,910 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,912 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,915 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,917 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,918 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,920 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,921 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,922 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,922 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,924 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,927 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,931 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,932 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,933 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,934 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,935 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,936 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,938 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,956 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,958 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,400 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,402 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,500 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,502 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,529 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,529 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,533 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,534 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,534 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,535 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,536 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,537 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,540 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,541 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,543 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,543 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,565 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,568 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,787 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,789 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,790 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,791 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,791 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,793 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,833 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,835 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,850 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,852 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,852 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,854 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,857 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,858 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,859 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,859 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,861 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,866 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,868 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,869 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,870 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,871 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,877 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,878 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,879 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,880 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,881 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,883 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,881 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,886 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,900 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,905 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,911 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,913 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,030 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,032 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,036 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,038 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,073 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,074 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,076 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,076 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,079 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,082 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,093 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,096 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,098 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,102 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,104 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,106 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,115 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,118 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,119 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,121 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,129 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,134 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,226 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,228 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,241 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,243 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,337 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,339 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,348 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,351 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,353 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,354 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,395 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,397 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,403 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,404 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,405 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,406 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,428 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,430 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,430 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,432 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,434 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,434 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,436 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,436 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,437 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,438 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,439 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,440 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,441 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,443 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,449 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,451 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,453 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,454 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,457 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,458 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,504 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,509 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,583 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,585 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,596 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,598 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,628 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,630 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,640 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,642 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,668 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,669 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,678 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,681 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,754 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,756 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,849 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,851 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,853 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,855 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,856 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,856 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,889 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,900 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,902 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,905 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,926 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,928 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,930 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,932 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,963 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,965 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,971 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,973 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,994 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,996 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,027 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,027 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,029 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,029 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,042 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,044 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,057 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,057 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,059 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,059 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,302 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,304 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,335 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,338 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,478 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,480 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,717 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,719 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,784 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,787 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,899 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,901 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,962 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,967 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,227 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,229 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,464 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,466 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:27,627 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:27,630 - distributed.utils - INFO - Reload module qme_vars from .py file
