Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-10-02 09:24:31,508 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:34655'
2025-10-02 09:24:31,518 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:37669'
2025-10-02 09:24:31,522 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:44289'
2025-10-02 09:24:31,526 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:46553'
2025-10-02 09:24:31,530 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:44329'
2025-10-02 09:24:31,534 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:33001'
2025-10-02 09:24:31,539 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:35111'
2025-10-02 09:24:31,543 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:35953'
2025-10-02 09:24:31,548 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:35827'
2025-10-02 09:24:31,553 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:33561'
2025-10-02 09:24:31,557 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:44291'
2025-10-02 09:24:31,560 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:37089'
2025-10-02 09:24:31,565 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:41169'
2025-10-02 09:24:31,569 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:42337'
2025-10-02 09:24:31,573 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:37691'
2025-10-02 09:24:31,576 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:35191'
2025-10-02 09:24:31,580 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:43643'
2025-10-02 09:24:31,585 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:43811'
2025-10-02 09:24:31,590 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:42155'
2025-10-02 09:24:31,677 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:36673'
2025-10-02 09:24:31,681 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:36627'
2025-10-02 09:24:31,686 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:41345'
2025-10-02 09:24:31,690 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:34991'
2025-10-02 09:24:31,694 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:38679'
2025-10-02 09:24:31,699 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:36013'
2025-10-02 09:24:31,704 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:45179'
2025-10-02 09:24:31,709 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:45065'
2025-10-02 09:24:31,713 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:43741'
2025-10-02 09:24:31,717 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:44085'
2025-10-02 09:24:31,721 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:37511'
2025-10-02 09:24:31,725 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:39459'
2025-10-02 09:24:31,729 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:46371'
2025-10-02 09:24:31,733 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:35409'
2025-10-02 09:24:31,741 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:33129'
2025-10-02 09:24:31,745 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:40911'
2025-10-02 09:24:31,750 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:46533'
2025-10-02 09:24:31,754 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:35575'
2025-10-02 09:24:31,759 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:33427'
2025-10-02 09:24:31,764 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:38407'
2025-10-02 09:24:31,768 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:36701'
2025-10-02 09:24:31,773 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:43989'
2025-10-02 09:24:31,777 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:33409'
2025-10-02 09:24:31,781 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:43841'
2025-10-02 09:24:31,786 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:44217'
2025-10-02 09:24:31,790 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:43427'
2025-10-02 09:24:31,795 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:35209'
2025-10-02 09:24:31,799 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:45147'
2025-10-02 09:24:31,804 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:44197'
2025-10-02 09:24:31,808 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:42031'
2025-10-02 09:24:31,812 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:37465'
2025-10-02 09:24:31,816 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:44943'
2025-10-02 09:24:31,823 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.17:45527'
2025-10-02 09:24:32,752 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:39161
2025-10-02 09:24:32,752 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:37931
2025-10-02 09:24:32,752 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:39161
2025-10-02 09:24:32,752 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:46729
2025-10-02 09:24:32,752 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:44669
2025-10-02 09:24:32,752 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:37931
2025-10-02 09:24:32,752 - distributed.worker - INFO -          dashboard at:           10.6.83.17:44929
2025-10-02 09:24:32,752 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:40909
2025-10-02 09:24:32,752 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:46221
2025-10-02 09:24:32,752 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:46729
2025-10-02 09:24:32,752 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:44669
2025-10-02 09:24:32,752 - distributed.worker - INFO -          dashboard at:           10.6.83.17:36919
2025-10-02 09:24:32,752 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,752 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:40909
2025-10-02 09:24:32,752 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:46221
2025-10-02 09:24:32,752 - distributed.worker - INFO -          dashboard at:           10.6.83.17:43361
2025-10-02 09:24:32,752 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:46445
2025-10-02 09:24:32,752 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,752 - distributed.worker - INFO -          dashboard at:           10.6.83.17:35641
2025-10-02 09:24:32,752 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,752 - distributed.worker - INFO -          dashboard at:           10.6.83.17:33949
2025-10-02 09:24:32,752 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,752 - distributed.worker - INFO -          dashboard at:           10.6.83.17:33627
2025-10-02 09:24:32,752 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:32,752 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:46445
2025-10-02 09:24:32,752 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,752 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,752 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,752 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,752 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,752 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:32,752 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:32,752 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,752 - distributed.worker - INFO -          dashboard at:           10.6.83.17:43373
2025-10-02 09:24:32,752 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:32,752 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,752 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,752 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:32,752 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-yt4wpyvw
2025-10-02 09:24:32,752 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:32,752 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:32,752 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,752 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:32,752 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:32,752 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-7wvlfaui
2025-10-02 09:24:32,752 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:32,752 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,752 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,752 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:32,752 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-p8bw90sj
2025-10-02 09:24:32,752 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:32,752 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,752 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-eo65theq
2025-10-02 09:24:32,752 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:32,752 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-ghruhr9b
2025-10-02 09:24:32,752 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,752 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-_iuv6qmr
2025-10-02 09:24:32,753 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:32,753 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,753 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,753 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,753 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-wodtvpue
2025-10-02 09:24:32,753 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,775 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:32,775 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,775 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,776 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:32,777 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:41255
2025-10-02 09:24:32,777 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:41255
2025-10-02 09:24:32,777 - distributed.worker - INFO -          dashboard at:           10.6.83.17:41447
2025-10-02 09:24:32,777 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,777 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,777 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:32,777 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:32,777 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-wz4vsaum
2025-10-02 09:24:32,777 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,780 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:43607
2025-10-02 09:24:32,780 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:43607
2025-10-02 09:24:32,780 - distributed.worker - INFO -          dashboard at:           10.6.83.17:34899
2025-10-02 09:24:32,780 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,780 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,780 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:32,780 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:32,780 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-bpi5obp_
2025-10-02 09:24:32,780 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,782 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:32,782 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,782 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,783 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:32,788 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:32,789 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,790 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:35125
2025-10-02 09:24:32,790 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:35125
2025-10-02 09:24:32,790 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,790 - distributed.worker - INFO -          dashboard at:           10.6.83.17:36467
2025-10-02 09:24:32,790 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,790 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,790 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:32,790 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:32,790 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-ha6plg2g
2025-10-02 09:24:32,790 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,791 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:32,792 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:38147
2025-10-02 09:24:32,792 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:38147
2025-10-02 09:24:32,792 - distributed.worker - INFO -          dashboard at:           10.6.83.17:39693
2025-10-02 09:24:32,792 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,792 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,792 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:32,792 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:32,792 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-_4wcfeha
2025-10-02 09:24:32,792 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,793 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:32,794 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,794 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,795 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:32,796 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:32,797 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:41817
2025-10-02 09:24:32,797 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:41817
2025-10-02 09:24:32,797 - distributed.worker - INFO -          dashboard at:           10.6.83.17:36937
2025-10-02 09:24:32,797 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,797 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,797 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:32,797 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:32,797 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-dx_im31a
2025-10-02 09:24:32,797 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,798 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,798 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,800 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:32,800 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:32,801 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,801 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,803 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:34159
2025-10-02 09:24:32,803 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:34159
2025-10-02 09:24:32,803 - distributed.worker - INFO -          dashboard at:           10.6.83.17:35777
2025-10-02 09:24:32,803 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,803 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,803 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:32,803 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:32,803 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:32,803 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-7i9g2j4m
2025-10-02 09:24:32,803 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,803 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:32,804 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,804 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,805 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:32,806 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:36045
2025-10-02 09:24:32,806 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:36045
2025-10-02 09:24:32,806 - distributed.worker - INFO -          dashboard at:           10.6.83.17:37565
2025-10-02 09:24:32,806 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,806 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,807 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:32,807 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:32,807 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-cfkmgk31
2025-10-02 09:24:32,807 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,813 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:33757
2025-10-02 09:24:32,813 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:33757
2025-10-02 09:24:32,813 - distributed.worker - INFO -          dashboard at:           10.6.83.17:32931
2025-10-02 09:24:32,813 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,813 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,813 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:32,813 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:32,813 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-vwghrygb
2025-10-02 09:24:32,813 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,819 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:33045
2025-10-02 09:24:32,820 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:33045
2025-10-02 09:24:32,820 - distributed.worker - INFO -          dashboard at:           10.6.83.17:34629
2025-10-02 09:24:32,820 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,820 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,820 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:32,820 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:32,820 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-4vc7_oe_
2025-10-02 09:24:32,820 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,822 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:40225
2025-10-02 09:24:32,822 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:40225
2025-10-02 09:24:32,822 - distributed.worker - INFO -          dashboard at:           10.6.83.17:33955
2025-10-02 09:24:32,822 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,822 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,822 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:32,822 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:32,822 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-49wifm5i
2025-10-02 09:24:32,822 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,824 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:32,824 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:34125
2025-10-02 09:24:32,824 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:34125
2025-10-02 09:24:32,824 - distributed.worker - INFO -          dashboard at:           10.6.83.17:34817
2025-10-02 09:24:32,824 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,824 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,824 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:32,824 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:32,824 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-d5zs3ygx
2025-10-02 09:24:32,824 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,824 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,825 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,826 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:32,827 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:32,828 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,828 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,829 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:32,830 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:32,832 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,832 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,832 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:32,833 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,833 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:32,833 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,835 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:32,835 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:32,837 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,837 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,838 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:32,838 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:32,839 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,839 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,840 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:32,841 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:32,841 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,841 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,842 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:32,843 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:32,844 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,844 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:32,844 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,844 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,844 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,845 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:32,845 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:32,848 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:32,849 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,849 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,850 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:32,859 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:32,860 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,860 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,861 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:32,889 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:34597
2025-10-02 09:24:32,889 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:34597
2025-10-02 09:24:32,890 - distributed.worker - INFO -          dashboard at:           10.6.83.17:34627
2025-10-02 09:24:32,890 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,890 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,890 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:32,890 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:32,890 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-luyutmwx
2025-10-02 09:24:32,890 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,898 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:46597
2025-10-02 09:24:32,899 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:46597
2025-10-02 09:24:32,899 - distributed.worker - INFO -          dashboard at:           10.6.83.17:36155
2025-10-02 09:24:32,899 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,899 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,899 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:32,899 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:32,899 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-txidcfzu
2025-10-02 09:24:32,899 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,914 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:32,915 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,916 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,917 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:32,921 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:32,922 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,922 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,923 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:32,935 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:40961
2025-10-02 09:24:32,935 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:40961
2025-10-02 09:24:32,935 - distributed.worker - INFO -          dashboard at:           10.6.83.17:44125
2025-10-02 09:24:32,935 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,935 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,935 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:32,935 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:32,935 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-txhc5l9e
2025-10-02 09:24:32,935 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,958 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:32,959 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,959 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,959 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:38237
2025-10-02 09:24:32,959 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:38237
2025-10-02 09:24:32,960 - distributed.worker - INFO -          dashboard at:           10.6.83.17:40947
2025-10-02 09:24:32,960 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,960 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,960 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:32,960 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:32,960 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-be8odw2c
2025-10-02 09:24:32,960 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,961 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:32,974 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:32,974 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:32,974 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:32,975 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:33,076 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:35999
2025-10-02 09:24:33,077 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:35999
2025-10-02 09:24:33,077 - distributed.worker - INFO -          dashboard at:           10.6.83.17:34527
2025-10-02 09:24:33,077 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,077 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,077 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:33,077 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:33,077 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-17bkmogk
2025-10-02 09:24:33,077 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,101 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:33,102 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,102 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,103 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:33,135 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:35829
2025-10-02 09:24:33,135 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:35829
2025-10-02 09:24:33,135 - distributed.worker - INFO -          dashboard at:           10.6.83.17:36645
2025-10-02 09:24:33,135 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,135 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,135 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:33,135 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:33,135 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-09utcekq
2025-10-02 09:24:33,135 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,149 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:36611
2025-10-02 09:24:33,149 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:36611
2025-10-02 09:24:33,149 - distributed.worker - INFO -          dashboard at:           10.6.83.17:41561
2025-10-02 09:24:33,149 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,149 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,149 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:33,149 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:33,149 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-axlcnsu5
2025-10-02 09:24:33,149 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,149 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:45919
2025-10-02 09:24:33,149 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:45919
2025-10-02 09:24:33,149 - distributed.worker - INFO -          dashboard at:           10.6.83.17:36791
2025-10-02 09:24:33,149 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,149 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,149 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:33,149 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:33,149 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-6n25t1i4
2025-10-02 09:24:33,149 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,160 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:33,161 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,161 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,163 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:33,173 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:33,174 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,174 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,175 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:37967
2025-10-02 09:24:33,175 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:37967
2025-10-02 09:24:33,175 - distributed.worker - INFO -          dashboard at:           10.6.83.17:36955
2025-10-02 09:24:33,175 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,175 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,175 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:33,175 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:33,175 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-7gphbm5v
2025-10-02 09:24:33,175 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,175 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:33,179 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:33,180 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,181 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,182 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:33,185 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:43697
2025-10-02 09:24:33,185 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:43697
2025-10-02 09:24:33,185 - distributed.worker - INFO -          dashboard at:           10.6.83.17:45153
2025-10-02 09:24:33,185 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,185 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,186 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:33,186 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:33,186 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-wov1di7y
2025-10-02 09:24:33,186 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,188 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:41853
2025-10-02 09:24:33,188 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:41853
2025-10-02 09:24:33,188 - distributed.worker - INFO -          dashboard at:           10.6.83.17:33363
2025-10-02 09:24:33,188 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,188 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,188 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:33,188 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:33,188 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-ygdebgk0
2025-10-02 09:24:33,188 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,191 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:35105
2025-10-02 09:24:33,191 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:35105
2025-10-02 09:24:33,191 - distributed.worker - INFO -          dashboard at:           10.6.83.17:33791
2025-10-02 09:24:33,191 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,191 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,191 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:33,191 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:33,191 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-l71z9hkm
2025-10-02 09:24:33,191 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,193 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:45089
2025-10-02 09:24:33,193 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:45089
2025-10-02 09:24:33,193 - distributed.worker - INFO -          dashboard at:           10.6.83.17:40209
2025-10-02 09:24:33,193 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,193 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,193 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:33,193 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:33,193 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-75gb9k2v
2025-10-02 09:24:33,193 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,194 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:46279
2025-10-02 09:24:33,195 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:46279
2025-10-02 09:24:33,195 - distributed.worker - INFO -          dashboard at:           10.6.83.17:44019
2025-10-02 09:24:33,195 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,195 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,195 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:33,195 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:33,195 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-rw4vta5h
2025-10-02 09:24:33,195 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,196 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:45945
2025-10-02 09:24:33,196 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:45945
2025-10-02 09:24:33,196 - distributed.worker - INFO -          dashboard at:           10.6.83.17:37965
2025-10-02 09:24:33,196 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,196 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,196 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:33,196 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:33,196 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:43991
2025-10-02 09:24:33,196 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-rixupt61
2025-10-02 09:24:33,196 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:43991
2025-10-02 09:24:33,196 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:35647
2025-10-02 09:24:33,196 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:42189
2025-10-02 09:24:33,196 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,196 - distributed.worker - INFO -          dashboard at:           10.6.83.17:40609
2025-10-02 09:24:33,196 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:35647
2025-10-02 09:24:33,196 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:42189
2025-10-02 09:24:33,196 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,196 - distributed.worker - INFO -          dashboard at:           10.6.83.17:33327
2025-10-02 09:24:33,196 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,196 - distributed.worker - INFO -          dashboard at:           10.6.83.17:43009
2025-10-02 09:24:33,196 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,196 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:33,196 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,196 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,196 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:33,196 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,196 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:33,196 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:33,196 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:33,196 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-ql_va201
2025-10-02 09:24:33,196 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:33,196 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-ty4kuuf4
2025-10-02 09:24:33,196 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,196 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-x7z41mgg
2025-10-02 09:24:33,197 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,197 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,197 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:41675
2025-10-02 09:24:33,197 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:41675
2025-10-02 09:24:33,197 - distributed.worker - INFO -          dashboard at:           10.6.83.17:40045
2025-10-02 09:24:33,197 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,197 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,197 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:33,197 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:33,197 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-gz1seszs
2025-10-02 09:24:33,197 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,201 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:41579
2025-10-02 09:24:33,201 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:41579
2025-10-02 09:24:33,201 - distributed.worker - INFO -          dashboard at:           10.6.83.17:41263
2025-10-02 09:24:33,201 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,201 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,201 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:33,201 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:33,201 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-dtxwzcn_
2025-10-02 09:24:33,201 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,203 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:43983
2025-10-02 09:24:33,203 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:46475
2025-10-02 09:24:33,203 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:43983
2025-10-02 09:24:33,203 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:46475
2025-10-02 09:24:33,203 - distributed.worker - INFO -          dashboard at:           10.6.83.17:44673
2025-10-02 09:24:33,203 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,203 - distributed.worker - INFO -          dashboard at:           10.6.83.17:45427
2025-10-02 09:24:33,203 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,203 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,203 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:33,203 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,203 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:33,203 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:33,203 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:33,203 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-kpl9xauh
2025-10-02 09:24:33,203 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-8xm01xns
2025-10-02 09:24:33,203 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,203 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,205 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:39421
2025-10-02 09:24:33,205 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:39421
2025-10-02 09:24:33,205 - distributed.worker - INFO -          dashboard at:           10.6.83.17:46635
2025-10-02 09:24:33,205 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,205 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,205 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:33,205 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:33,205 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-90nj4g3y
2025-10-02 09:24:33,205 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,205 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:34471
2025-10-02 09:24:33,205 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:34471
2025-10-02 09:24:33,205 - distributed.worker - INFO -          dashboard at:           10.6.83.17:41515
2025-10-02 09:24:33,205 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,205 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,205 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:33,206 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:33,206 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-ybykeu51
2025-10-02 09:24:33,206 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,206 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:33,207 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,207 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,209 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:33,209 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:42443
2025-10-02 09:24:33,209 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:42443
2025-10-02 09:24:33,210 - distributed.worker - INFO -          dashboard at:           10.6.83.17:36681
2025-10-02 09:24:33,210 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,210 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,210 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:33,210 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:33,210 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-zrx3bkir
2025-10-02 09:24:33,210 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,211 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:46263
2025-10-02 09:24:33,211 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:46263
2025-10-02 09:24:33,212 - distributed.worker - INFO -          dashboard at:           10.6.83.17:34355
2025-10-02 09:24:33,212 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,212 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,212 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:33,212 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:33,212 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-i6ocpqoy
2025-10-02 09:24:33,212 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,212 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:34895
2025-10-02 09:24:33,212 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:34895
2025-10-02 09:24:33,212 - distributed.worker - INFO -          dashboard at:           10.6.83.17:34027
2025-10-02 09:24:33,212 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,212 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,212 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:33,212 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:33,212 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-0l1k9x4k
2025-10-02 09:24:33,212 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,212 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:38217
2025-10-02 09:24:33,213 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:38217
2025-10-02 09:24:33,213 - distributed.worker - INFO -          dashboard at:           10.6.83.17:45155
2025-10-02 09:24:33,213 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,213 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,213 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:33,213 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:39721
2025-10-02 09:24:33,213 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:33,213 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:39721
2025-10-02 09:24:33,213 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-evn6byhx
2025-10-02 09:24:33,213 - distributed.worker - INFO -          dashboard at:           10.6.83.17:38055
2025-10-02 09:24:33,213 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,213 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,213 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,213 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:33,213 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:33,213 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-7eet_ke9
2025-10-02 09:24:33,213 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,213 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:33,213 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:42667
2025-10-02 09:24:33,214 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:42667
2025-10-02 09:24:33,214 - distributed.worker - INFO -          dashboard at:           10.6.83.17:41453
2025-10-02 09:24:33,214 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,214 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,214 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:33,214 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:33,214 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-wqbjebhn
2025-10-02 09:24:33,214 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,214 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,214 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,215 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:33,215 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:36567
2025-10-02 09:24:33,215 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:36567
2025-10-02 09:24:33,215 - distributed.worker - INFO -          dashboard at:           10.6.83.17:46423
2025-10-02 09:24:33,215 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,215 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,215 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:33,215 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:33,215 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-r3fiq_gh
2025-10-02 09:24:33,215 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,218 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:33,219 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,219 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,220 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:45447
2025-10-02 09:24:33,220 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:45447
2025-10-02 09:24:33,220 - distributed.worker - INFO -          dashboard at:           10.6.83.17:37437
2025-10-02 09:24:33,220 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,220 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,220 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:33,220 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:33,220 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-0h0jbfga
2025-10-02 09:24:33,220 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,221 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:33,222 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:33,222 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:37227
2025-10-02 09:24:33,222 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:37227
2025-10-02 09:24:33,222 - distributed.worker - INFO -          dashboard at:           10.6.83.17:42355
2025-10-02 09:24:33,222 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,223 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,223 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:33,223 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:33,223 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-7t3bsfmh
2025-10-02 09:24:33,223 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,223 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,223 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,225 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:33,227 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:33,228 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,228 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,229 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:33,229 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:33,230 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,231 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,232 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:33,233 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.17:38627
2025-10-02 09:24:33,233 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.17:38627
2025-10-02 09:24:33,233 - distributed.worker - INFO -          dashboard at:           10.6.83.17:45767
2025-10-02 09:24:33,233 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,233 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,233 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:33,233 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:33,233 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-wp7n64uj
2025-10-02 09:24:33,233 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,234 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:33,235 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,235 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,236 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:33,237 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:33,237 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,237 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,239 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:33,239 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:33,240 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,240 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,241 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:33,242 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:33,242 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,242 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,243 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:33,244 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,244 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:33,244 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,245 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:33,245 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,245 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,246 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:33,246 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:33,247 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:33,248 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,248 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,248 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:33,249 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,250 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,250 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:33,251 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:33,252 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:33,253 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,253 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,254 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:33,258 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:33,259 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,260 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,260 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:33,261 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:33,261 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,261 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,263 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:33,265 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:33,266 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,266 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,266 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:33,267 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,268 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,268 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:33,268 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:33,269 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,269 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:33,269 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:33,269 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,270 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,270 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,271 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:33,271 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:33,272 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:33,272 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,272 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,272 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:33,274 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,274 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,274 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:33,274 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:33,275 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,275 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,275 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:33,277 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:33,279 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:33,280 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,281 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,281 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:33,282 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:33,282 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:33,282 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:33,283 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:56,304 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,304 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,305 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,305 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,305 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,305 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,305 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,306 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,306 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,306 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,306 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,306 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,306 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,306 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,306 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,306 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,307 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,307 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,307 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,307 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,307 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,308 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,308 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,308 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,308 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,308 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,308 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,308 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,308 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,308 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,308 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,308 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,308 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,308 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,308 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,308 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,309 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,309 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,309 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,309 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,309 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,309 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,309 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,309 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,309 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,309 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,309 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,310 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,310 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,310 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,308 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,310 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,310 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,308 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,310 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,310 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,308 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,311 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,311 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,311 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,311 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,309 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,309 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,312 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,312 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,312 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,312 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,312 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,312 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,312 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,312 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,310 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,310 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,311 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,311 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,312 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,312 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,312 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,312 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,313 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,314 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,314 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,314 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,319 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,319 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,320 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,317 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,320 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,321 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,321 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,322 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,322 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,323 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,324 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,324 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,325 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,325 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,325 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,326 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,326 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,327 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,326 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,330 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,334 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:59,059 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,060 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,060 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,060 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,060 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,060 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,060 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,061 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,060 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,060 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,061 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,061 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,061 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,061 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,061 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,061 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,061 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,061 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,060 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,062 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,062 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,062 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,062 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,062 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,062 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,062 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,063 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,063 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,063 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,063 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,063 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,063 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,063 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,064 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,064 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,064 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,064 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,063 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,063 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,064 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,064 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,064 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,064 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,063 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,063 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,064 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,064 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,064 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,064 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,064 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,064 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,064 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,064 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,064 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,064 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,065 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,064 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,065 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,065 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,065 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,065 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,064 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,065 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,065 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,065 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,065 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,065 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,065 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,065 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,066 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,066 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,065 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,066 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,065 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,065 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,066 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,066 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,066 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,065 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,066 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,063 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,066 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,067 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,067 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,067 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,067 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,067 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,067 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,067 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,064 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,067 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,067 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,068 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,068 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,068 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,068 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,068 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,069 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,066 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,069 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,069 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,070 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,070 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,071 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,459 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,460 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,460 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,460 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,460 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,460 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,460 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,460 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,460 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,461 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,461 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,461 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,461 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,461 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,461 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,461 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,461 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,462 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,462 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,462 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,462 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,462 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,462 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,462 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,463 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,463 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,463 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,463 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,463 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,463 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,463 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,464 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,464 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,464 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,464 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,464 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,464 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,464 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,464 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,464 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,464 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,464 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,464 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,465 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,465 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,465 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,465 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,465 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,465 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,465 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,465 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,465 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,465 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,465 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,465 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,465 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,465 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,465 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,465 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,466 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,466 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,466 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,466 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,466 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,466 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,466 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,466 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,466 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,466 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,466 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,466 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,466 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,466 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,467 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,467 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,468 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,468 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,468 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,468 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,468 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,468 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,468 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,468 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,468 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,469 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,469 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,469 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,469 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,469 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,469 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,469 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,469 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,469 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,469 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,470 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,470 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,470 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,470 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,470 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,469 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,471 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,472 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,479 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,485 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,894 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,894 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,895 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,895 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,895 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,895 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,895 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,895 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,895 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,895 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,895 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,896 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,896 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,896 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,896 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,896 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,896 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,896 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,896 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,896 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,897 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,897 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,897 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,897 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,897 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,897 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,897 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,897 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,897 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,897 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,897 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,897 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,898 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,898 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,898 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,898 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,898 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,898 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,898 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,898 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,898 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,898 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,898 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,898 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,898 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,898 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,898 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,898 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,899 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,899 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,899 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,899 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,899 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,899 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,899 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,899 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,899 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,899 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,899 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,899 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,899 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,899 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,900 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,899 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,899 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,899 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,899 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,899 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,899 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,900 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,900 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,900 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,900 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,900 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,900 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,900 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,900 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,900 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,900 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,900 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,901 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,901 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,901 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,901 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,901 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,901 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,901 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,901 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,901 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,902 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,902 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,902 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,902 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,902 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,902 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,902 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,902 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,902 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,903 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,903 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,903 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,903 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,903 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,905 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:24,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,079 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,080 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,081 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,081 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,081 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,091 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,097 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:24,102 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 19:25:43,473 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,473 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,473 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,474 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,473 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,767 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:46279. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,473 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,489 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,768 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:46221. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,473 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,768 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:46729. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,768 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:34895. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,768 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:43983. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,473 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,474 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,474 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,473 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,769 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:42443. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,769 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:35647. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,474 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,474 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,769 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:41579. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,473 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,769 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:34597. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,769 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:39421. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,769 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:35999. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,474 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,770 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:40961. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,473 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,770 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:43607. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,770 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:37227. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,473 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,473 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,770 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:36045. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,474 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,771 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:42667. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,473 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,771 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:43991. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,474 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,771 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:45089. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,473 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,473 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,772 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:37967. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,474 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,772 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:33045. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,772 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:39161. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,772 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:38147. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,473 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,473 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,772 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:46445. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,473 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,773 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:34471. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,773 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:46263. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,473 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,501 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,773 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:41255. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,516 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,527 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,774 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:38627. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,607 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,774 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:40909. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,619 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,634 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,641 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,670 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,692 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,697 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,739 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,742 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,754 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,776 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:41675. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,779 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:41817. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,779 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:44669. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,779 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:33757. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,780 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:37931. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,780 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:42189. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,780 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:38217. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,780 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:40225. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,781 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:41853. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,781 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:34159. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,781 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:35125. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,781 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:36611. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,781 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:38237. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,781 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:34125. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,789 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,798 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:45919. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,798 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:45147'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,799 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:39459'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,799 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:44329'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,799 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,800 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:44291'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,800 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:35409'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,800 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,800 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,800 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:44197'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,800 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,800 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:37465'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,800 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,800 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,800 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,800 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,800 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:43811'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,800 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,800 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,800 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,800 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,801 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,801 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:40911'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,800 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,801 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,801 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,801 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:37089'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,801 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,801 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,801 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:34991'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,801 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,801 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,801 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,801 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:36013'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,801 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,801 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,801 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,801 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:33561'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,801 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,801 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,801 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,801 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,801 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,801 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,801 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:43841'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,801 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,802 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,802 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,802 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,802 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,802 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,802 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:36627'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,802 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,802 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,802 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,802 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,802 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:44085'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,802 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,802 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:41345'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,802 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,802 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,802 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,802 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,802 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,802 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,802 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,802 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,802 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:45065'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,802 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,802 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,802 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,802 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,802 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,802 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,803 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,803 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,803 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,803 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,803 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:45527'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,803 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,803 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,803 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,803 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,803 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,803 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,803 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,803 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:33001'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,803 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,803 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,803 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,803 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,803 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,803 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:46553'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,803 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,803 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,803 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,803 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,803 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,803 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:35191'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,803 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,803 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,804 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,804 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,804 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,804 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,804 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:43427'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,804 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,804 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,804 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,804 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,804 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:43989'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,804 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,804 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,804 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,804 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,804 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,804 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:43643'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,804 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,804 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,804 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,804 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,804 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:33129'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,804 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,804 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,804 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:44289'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,804 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,805 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,805 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,805 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:41169'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,805 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,805 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,805 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,805 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,805 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,805 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,805 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,805 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,805 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,805 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,805 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,805 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,805 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,805 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,805 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,805 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,805 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,805 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,805 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,805 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,805 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,805 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,805 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,805 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,805 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,806 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,806 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,806 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,806 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,806 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,806 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,806 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,806 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,806 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,806 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,807 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,807 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,807 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,807 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,807 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,807 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,807 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,808 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,809 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,810 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,810 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,810 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,811 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,812 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,813 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,814 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,815 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,815 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,816 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,816 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,816 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,816 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,817 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,817 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,817 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,817 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,817 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,817 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,817 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,818 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,818 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,811 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:43,821 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,821 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,820 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,830 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:46597. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,835 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:45179'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,840 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:42155'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,839 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,846 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:45945. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,847 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 13, 0, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:43,849 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 1, 15, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:43,852 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,850 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,854 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,858 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,858 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:35829. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,860 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,863 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,865 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,862 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,868 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,870 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,871 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:35105. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,873 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,870 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,876 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,880 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:46475. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,905 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:35111'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,915 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,910 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,924 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:39721. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,927 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:43697. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,928 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:35953'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,947 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,955 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:36567. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,963 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:36673'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,972 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,981 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.17:45447. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,059 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:46533'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,083 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:42031'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,101 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 14, 2, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,103 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 4, 14, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,107 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,112 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,115 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 12, 4, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,115 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:35575'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,116 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:35209'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,117 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:37511'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,118 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,120 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,123 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,123 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:44943'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,126 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,126 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 11, 11, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,128 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,131 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,136 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,141 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,142 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,144 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:37669'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,148 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,148 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:42337'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,153 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,153 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 16, 11, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,154 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:38679'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,158 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,159 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,160 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 15, 12, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,164 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,165 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,165 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:44217'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,166 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,167 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 16, 3, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,169 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:33409'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,170 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,170 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 5, 18, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,171 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,172 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,175 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,176 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,177 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,180 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,182 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,182 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,185 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 6, 20, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,187 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,187 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,191 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 15, 7, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,192 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,193 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,199 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,204 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,209 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,215 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,218 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,222 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 10, 17, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,222 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 12, 19, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,223 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 15, 10, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,224 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,228 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,228 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,229 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,231 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 5, 1, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,232 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:33427'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,233 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,234 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,234 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,235 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:43741'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,239 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,239 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,239 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,244 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,244 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,245 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,245 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:35827'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,249 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,249 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:46371'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,250 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,250 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,255 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,255 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:37691'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,258 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,260 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,264 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,266 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,267 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 8, 7, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,267 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 4, 16, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,268 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,268 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,269 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,269 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,274 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,274 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,279 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,280 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,280 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:34655'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,281 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:36701'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,285 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.17:38407'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,289 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,294 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,299 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,305 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,310 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,320 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 5, 2, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,331 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 4, 6, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,333 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,334 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,334 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,334 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,340 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,420 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 0, 14, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,443 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,448 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,452 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,453 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 2, 9, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,454 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,454 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,454 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,454 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,455 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,455 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,458 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,459 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,463 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,464 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,468 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,474 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,504 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 11, 17, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,508 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,514 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,519 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,525 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,530 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,531 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,536 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,541 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,547 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,552 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,573 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,579 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,584 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,589 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,594 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,350 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14ed910d9690>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:44,750 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 8, 21, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,890 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,896 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,901 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,906 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,912 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,613 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:44,696 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1477c8fc1fd0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:44,915 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:44,697 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x15222961d810>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:44,742 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x148bbf541810>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:45,162 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:45,387 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x146644388f10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:45,810 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,811 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,812 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,813 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,814 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,814 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,815 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,815 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,816 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,817 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,818 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,818 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,819 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,820 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,820 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,821 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,821 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,821 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,821 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,821 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,822 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,826 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,827 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:46,124 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.17:43841'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,126 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.17:45147'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,128 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.17:43841' closed.
2025-10-02 19:25:46,129 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.17:45147' closed.
2025-10-02 19:25:46,170 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.17:41169'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,171 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.17:41169' closed.
2025-10-02 19:25:46,187 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.17:44085'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,188 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.17:44085' closed.
2025-10-02 19:25:46,223 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.17:35191'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,224 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.17:35191' closed.
2025-10-02 19:25:46,247 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.17:35409'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,249 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.17:35409' closed.
2025-10-02 19:25:46,336 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.17:44291'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,337 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.17:44291' closed.
2025-10-02 19:25:46,338 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.17:43643'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,339 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.17:43643' closed.
2025-10-02 19:25:46,363 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.17:44329'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,364 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.17:44329' closed.
2025-10-02 19:25:46,375 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.17:45065'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,376 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.17:45065' closed.
2025-10-02 19:25:46,399 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.17:44197'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,400 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.17:44197' closed.
2025-10-02 19:25:46,406 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.17:33129'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,407 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.17:33129' closed.
2025-10-02 19:25:46,449 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.17:33561'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,450 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.17:33561' closed.
2025-10-02 19:25:46,453 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.17:37089'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,454 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.17:37089' closed.
2025-10-02 19:25:46,462 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.17:43427'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,463 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.17:43427' closed.
2025-10-02 19:25:46,471 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.17:36627'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,473 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.17:43989'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,476 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.17:36627' closed.
2025-10-02 19:25:46,476 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.17:46553'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,477 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.17:43989' closed.
2025-10-02 19:25:46,477 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.17:46553' closed.
2025-10-02 19:25:46,483 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.17:33001'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,484 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.17:33001' closed.
2025-10-02 19:25:46,486 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.17:45527'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,487 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.17:45527' closed.
2025-10-02 19:25:46,492 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.17:41345'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,493 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.17:41345' closed.
2025-10-02 19:25:46,498 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.17:37465'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,499 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.17:36013'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,499 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.17:37465' closed.
2025-10-02 19:25:46,500 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.17:36013' closed.
2025-10-02 19:25:46,514 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.17:39459'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,515 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.17:39459' closed.
2025-10-02 19:25:46,524 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.17:40911'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,525 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.17:40911' closed.
2025-10-02 19:25:46,576 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.17:44289'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,577 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.17:44289' closed.
2025-10-02 19:25:46,586 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.17:43811'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,587 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.17:43811' closed.
2025-10-02 19:25:46,623 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.17:34991'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,624 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.17:34991' closed.
2025-10-02 19:25:53,013 - distributed.nanny - INFO - Worker closed
