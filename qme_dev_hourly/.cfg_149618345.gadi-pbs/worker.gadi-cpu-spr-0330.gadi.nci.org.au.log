Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-13 18:55:04,124 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:43171'
2025-09-13 18:55:04,135 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:46623'
2025-09-13 18:55:04,142 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:40729'
2025-09-13 18:55:04,146 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:41263'
2025-09-13 18:55:04,150 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:44295'
2025-09-13 18:55:04,154 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:39099'
2025-09-13 18:55:04,157 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:35773'
2025-09-13 18:55:04,162 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:44187'
2025-09-13 18:55:04,167 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:34431'
2025-09-13 18:55:04,173 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:38469'
2025-09-13 18:55:04,177 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:46271'
2025-09-13 18:55:04,182 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:36783'
2025-09-13 18:55:04,188 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:39675'
2025-09-13 18:55:04,191 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:34757'
2025-09-13 18:55:04,196 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:36841'
2025-09-13 18:55:04,200 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:36735'
2025-09-13 18:55:04,205 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:37031'
2025-09-13 18:55:04,209 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:46583'
2025-09-13 18:55:04,215 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:36409'
2025-09-13 18:55:04,219 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:36095'
2025-09-13 18:55:04,305 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:32841'
2025-09-13 18:55:04,309 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:45185'
2025-09-13 18:55:04,313 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:39539'
2025-09-13 18:55:04,318 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:36043'
2025-09-13 18:55:04,323 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:41503'
2025-09-13 18:55:04,327 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:40741'
2025-09-13 18:55:04,332 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:41409'
2025-09-13 18:55:04,337 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:38805'
2025-09-13 18:55:04,341 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:37519'
2025-09-13 18:55:04,346 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:40121'
2025-09-13 18:55:04,350 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:44137'
2025-09-13 18:55:04,354 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:45077'
2025-09-13 18:55:04,359 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:39183'
2025-09-13 18:55:04,364 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:42555'
2025-09-13 18:55:04,368 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:45943'
2025-09-13 18:55:04,371 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:45147'
2025-09-13 18:55:04,376 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:40949'
2025-09-13 18:55:04,381 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:33881'
2025-09-13 18:55:04,385 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:42671'
2025-09-13 18:55:04,389 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:33635'
2025-09-13 18:55:04,393 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:43221'
2025-09-13 18:55:04,397 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:33107'
2025-09-13 18:55:04,402 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:45745'
2025-09-13 18:55:04,407 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:34261'
2025-09-13 18:55:04,412 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:42807'
2025-09-13 18:55:04,417 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:38741'
2025-09-13 18:55:04,421 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:45291'
2025-09-13 18:55:04,426 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:38715'
2025-09-13 18:55:04,430 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:41053'
2025-09-13 18:55:04,434 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:34497'
2025-09-13 18:55:04,438 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:37947'
2025-09-13 18:55:04,442 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.42:43005'
2025-09-13 18:55:05,506 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:43173
2025-09-13 18:55:05,506 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:42635
2025-09-13 18:55:05,506 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:33273
2025-09-13 18:55:05,506 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:44547
2025-09-13 18:55:05,506 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:38933
2025-09-13 18:55:05,506 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:35759
2025-09-13 18:55:05,506 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:38631
2025-09-13 18:55:05,506 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:43173
2025-09-13 18:55:05,506 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:41595
2025-09-13 18:55:05,506 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:42635
2025-09-13 18:55:05,506 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:33555
2025-09-13 18:55:05,506 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:39619
2025-09-13 18:55:05,506 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:33273
2025-09-13 18:55:05,506 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:32963
2025-09-13 18:55:05,506 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:44547
2025-09-13 18:55:05,506 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:33169
2025-09-13 18:55:05,506 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:39171
2025-09-13 18:55:05,506 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:33489
2025-09-13 18:55:05,506 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:38933
2025-09-13 18:55:05,506 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:35759
2025-09-13 18:55:05,506 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:41667
2025-09-13 18:55:05,506 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:46759
2025-09-13 18:55:05,506 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:38631
2025-09-13 18:55:05,506 - distributed.worker - INFO -          dashboard at:           10.6.85.42:45963
2025-09-13 18:55:05,506 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:41595
2025-09-13 18:55:05,506 - distributed.worker - INFO -          dashboard at:           10.6.85.42:37269
2025-09-13 18:55:05,506 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:33555
2025-09-13 18:55:05,506 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:38335
2025-09-13 18:55:05,506 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:39619
2025-09-13 18:55:05,506 - distributed.worker - INFO -          dashboard at:           10.6.85.42:44943
2025-09-13 18:55:05,506 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:32963
2025-09-13 18:55:05,506 - distributed.worker - INFO -          dashboard at:           10.6.85.42:37387
2025-09-13 18:55:05,506 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:33169
2025-09-13 18:55:05,506 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:39171
2025-09-13 18:55:05,506 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:33489
2025-09-13 18:55:05,506 - distributed.worker - INFO -          dashboard at:           10.6.85.42:40351
2025-09-13 18:55:05,506 - distributed.worker - INFO -          dashboard at:           10.6.85.42:41815
2025-09-13 18:55:05,506 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:36097
2025-09-13 18:55:05,506 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:41667
2025-09-13 18:55:05,506 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:46759
2025-09-13 18:55:05,506 - distributed.worker - INFO -          dashboard at:           10.6.85.42:41015
2025-09-13 18:55:05,506 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,506 - distributed.worker - INFO -          dashboard at:           10.6.85.42:36285
2025-09-13 18:55:05,506 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,506 - distributed.worker - INFO -          dashboard at:           10.6.85.42:46171
2025-09-13 18:55:05,506 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:38335
2025-09-13 18:55:05,506 - distributed.worker - INFO -          dashboard at:           10.6.85.42:42535
2025-09-13 18:55:05,506 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,506 - distributed.worker - INFO -          dashboard at:           10.6.85.42:35683
2025-09-13 18:55:05,506 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,506 - distributed.worker - INFO -          dashboard at:           10.6.85.42:43697
2025-09-13 18:55:05,506 - distributed.worker - INFO -          dashboard at:           10.6.85.42:33567
2025-09-13 18:55:05,506 - distributed.worker - INFO -          dashboard at:           10.6.85.42:40819
2025-09-13 18:55:05,506 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,506 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,506 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:36097
2025-09-13 18:55:05,506 - distributed.worker - INFO -          dashboard at:           10.6.85.42:37701
2025-09-13 18:55:05,506 - distributed.worker - INFO -          dashboard at:           10.6.85.42:43951
2025-09-13 18:55:05,506 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,506 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,506 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:43891
2025-09-13 18:55:05,506 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,506 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,506 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,506 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,506 - distributed.worker - INFO -          dashboard at:           10.6.85.42:40183
2025-09-13 18:55:05,506 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,506 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,506 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,506 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,506 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,506 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,506 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,506 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,506 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,506 - distributed.worker - INFO -          dashboard at:           10.6.85.42:46869
2025-09-13 18:55:05,507 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,507 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,507 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:43891
2025-09-13 18:55:05,507 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,507 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,507 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,507 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,507 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,507 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,507 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,507 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,507 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,507 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,507 - distributed.worker - INFO -          dashboard at:           10.6.85.42:38607
2025-09-13 18:55:05,507 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,507 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,507 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-4nouylc_
2025-09-13 18:55:05,507 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,507 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,507 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,507 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,507 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,507 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,507 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,507 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,507 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,507 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,507 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,507 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,507 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,507 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,507 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,507 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-nmkdw1t2
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,507 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-8334gzm3
2025-09-13 18:55:05,507 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,507 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-k1tzpopy
2025-09-13 18:55:05,507 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,507 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,507 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-iwthcog0
2025-09-13 18:55:05,507 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,507 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,507 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,507 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-0742pl1s
2025-09-13 18:55:05,507 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,507 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,507 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-9ipd_hwk
2025-09-13 18:55:05,507 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,507 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-8phs0czo
2025-09-13 18:55:05,507 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-6l0cwluo
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,507 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,507 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-nt1wsun6
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,507 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-ssfcv0md
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,507 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-er_x3q6q
2025-09-13 18:55:05,507 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-z0l6m5c9
2025-09-13 18:55:05,507 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-vcdolswx
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,507 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,507 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-nmpn54dr
2025-09-13 18:55:05,507 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,507 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-89fc2xlx
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,507 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-acegb0g5
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,507 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,507 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-_9hh_jw9
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,507 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-fsha_i3x
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,514 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:43553
2025-09-13 18:55:05,514 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:43553
2025-09-13 18:55:05,514 - distributed.worker - INFO -          dashboard at:           10.6.85.42:46125
2025-09-13 18:55:05,515 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,515 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,515 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,515 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,515 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-scruj350
2025-09-13 18:55:05,515 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,516 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:45903
2025-09-13 18:55:05,517 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:45903
2025-09-13 18:55:05,517 - distributed.worker - INFO -          dashboard at:           10.6.85.42:34023
2025-09-13 18:55:05,517 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,517 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,517 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,517 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,517 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-9huvvqfl
2025-09-13 18:55:05,517 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,525 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:32853
2025-09-13 18:55:05,525 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:32853
2025-09-13 18:55:05,526 - distributed.worker - INFO -          dashboard at:           10.6.85.42:35631
2025-09-13 18:55:05,526 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,526 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,526 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,526 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,526 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-3mo2qiho
2025-09-13 18:55:05,526 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,535 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,535 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,535 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,536 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,536 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,536 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:34997
2025-09-13 18:55:05,537 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,537 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:34997
2025-09-13 18:55:05,537 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,537 - distributed.worker - INFO -          dashboard at:           10.6.85.42:38019
2025-09-13 18:55:05,537 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,537 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,537 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,537 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,537 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-q9e2iy8b
2025-09-13 18:55:05,537 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,537 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,537 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,537 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,537 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,538 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,538 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,539 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,539 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,539 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,539 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,539 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,539 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,540 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,540 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,540 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,540 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,541 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,541 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,542 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,542 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,542 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,542 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,542 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,542 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,543 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,543 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,543 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,543 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,544 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,544 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,544 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,545 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,545 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,545 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,545 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,545 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,546 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,546 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,546 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,546 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,547 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,547 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,547 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,547 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,547 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,547 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,548 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,548 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,549 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,549 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,549 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,550 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,550 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,550 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,550 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,550 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,551 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,551 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,552 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,552 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,552 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,552 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,552 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,552 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,552 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,553 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,553 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,553 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,553 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,553 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,553 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,554 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,554 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,555 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,555 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,555 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,557 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,560 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,561 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,561 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,563 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,564 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,565 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,565 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,566 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,576 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:36203
2025-09-13 18:55:05,576 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:36203
2025-09-13 18:55:05,577 - distributed.worker - INFO -          dashboard at:           10.6.85.42:37867
2025-09-13 18:55:05,577 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,577 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,577 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,577 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,577 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-komocujf
2025-09-13 18:55:05,577 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,592 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:45607
2025-09-13 18:55:05,592 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:45607
2025-09-13 18:55:05,592 - distributed.worker - INFO -          dashboard at:           10.6.85.42:33229
2025-09-13 18:55:05,592 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,592 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,592 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,592 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,592 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-004z3v29
2025-09-13 18:55:05,593 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,596 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,597 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,597 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,598 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,613 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,614 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,615 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,616 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,634 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:41671
2025-09-13 18:55:05,635 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:41671
2025-09-13 18:55:05,635 - distributed.worker - INFO -          dashboard at:           10.6.85.42:37357
2025-09-13 18:55:05,635 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,635 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,635 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,635 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,635 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-han61j9g
2025-09-13 18:55:05,635 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,635 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:40843
2025-09-13 18:55:05,636 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:40843
2025-09-13 18:55:05,636 - distributed.worker - INFO -          dashboard at:           10.6.85.42:46327
2025-09-13 18:55:05,636 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,636 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,636 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,636 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,636 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-enppik8w
2025-09-13 18:55:05,636 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,656 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,657 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,657 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,658 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,659 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,659 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,659 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,660 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:39131
2025-09-13 18:55:05,660 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:39131
2025-09-13 18:55:05,660 - distributed.worker - INFO -          dashboard at:           10.6.85.42:41695
2025-09-13 18:55:05,660 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,660 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,660 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,660 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,661 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-0z0xv0yf
2025-09-13 18:55:05,661 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,661 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,662 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:43499
2025-09-13 18:55:05,662 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:43499
2025-09-13 18:55:05,663 - distributed.worker - INFO -          dashboard at:           10.6.85.42:40485
2025-09-13 18:55:05,663 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,663 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,663 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,663 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-ig8ehx_t
2025-09-13 18:55:05,663 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,682 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,683 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,683 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,684 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,684 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,685 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,685 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,686 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,734 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:40801
2025-09-13 18:55:05,734 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:40801
2025-09-13 18:55:05,734 - distributed.worker - INFO -          dashboard at:           10.6.85.42:43027
2025-09-13 18:55:05,734 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,734 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,734 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,734 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,734 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-pi7dfi44
2025-09-13 18:55:05,734 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,734 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:44091
2025-09-13 18:55:05,735 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:44091
2025-09-13 18:55:05,735 - distributed.worker - INFO -          dashboard at:           10.6.85.42:41035
2025-09-13 18:55:05,735 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,735 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,735 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,735 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,735 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-eeg1i8p7
2025-09-13 18:55:05,735 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,743 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:38761
2025-09-13 18:55:05,743 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:38761
2025-09-13 18:55:05,743 - distributed.worker - INFO -          dashboard at:           10.6.85.42:37689
2025-09-13 18:55:05,743 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,743 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,743 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,743 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,743 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-ifazffhw
2025-09-13 18:55:05,743 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,750 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:40255
2025-09-13 18:55:05,750 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:40255
2025-09-13 18:55:05,750 - distributed.worker - INFO -          dashboard at:           10.6.85.42:39419
2025-09-13 18:55:05,750 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,750 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,750 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,750 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,750 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-k2147m5o
2025-09-13 18:55:05,750 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,752 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:39713
2025-09-13 18:55:05,752 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:39713
2025-09-13 18:55:05,752 - distributed.worker - INFO -          dashboard at:           10.6.85.42:33185
2025-09-13 18:55:05,752 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,752 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,752 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,752 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,752 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-l4a3vqt3
2025-09-13 18:55:05,752 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,754 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,755 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,755 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,757 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,757 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,758 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,758 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,760 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,764 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,765 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,765 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,766 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,772 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,772 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,773 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,774 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,774 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,775 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,775 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,776 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,817 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:32961
2025-09-13 18:55:05,817 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:32961
2025-09-13 18:55:05,817 - distributed.worker - INFO -          dashboard at:           10.6.85.42:38205
2025-09-13 18:55:05,817 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,817 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,817 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,817 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,817 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-zah27v87
2025-09-13 18:55:05,817 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,828 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:33341
2025-09-13 18:55:05,828 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:33341
2025-09-13 18:55:05,828 - distributed.worker - INFO -          dashboard at:           10.6.85.42:42967
2025-09-13 18:55:05,828 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,828 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,828 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,828 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,829 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-_155z87s
2025-09-13 18:55:05,829 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,839 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,840 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,840 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,841 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,851 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,852 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,852 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,853 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,917 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:34823
2025-09-13 18:55:05,917 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:34823
2025-09-13 18:55:05,917 - distributed.worker - INFO -          dashboard at:           10.6.85.42:36131
2025-09-13 18:55:05,917 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,917 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,917 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,917 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,917 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-v69ffj_f
2025-09-13 18:55:05,917 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,923 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:36207
2025-09-13 18:55:05,923 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:36207
2025-09-13 18:55:05,923 - distributed.worker - INFO -          dashboard at:           10.6.85.42:39001
2025-09-13 18:55:05,923 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,923 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,923 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,923 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,923 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-pu7b6p7n
2025-09-13 18:55:05,924 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,924 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:35299
2025-09-13 18:55:05,924 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:35299
2025-09-13 18:55:05,924 - distributed.worker - INFO -          dashboard at:           10.6.85.42:41967
2025-09-13 18:55:05,924 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,924 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,924 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,924 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,924 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-nj9qm0gu
2025-09-13 18:55:05,924 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,925 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:35735
2025-09-13 18:55:05,925 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:35735
2025-09-13 18:55:05,925 - distributed.worker - INFO -          dashboard at:           10.6.85.42:36397
2025-09-13 18:55:05,925 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,925 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,925 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,925 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,925 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-npr8wx31
2025-09-13 18:55:05,925 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,926 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:46439
2025-09-13 18:55:05,926 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:46439
2025-09-13 18:55:05,926 - distributed.worker - INFO -          dashboard at:           10.6.85.42:33651
2025-09-13 18:55:05,926 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,926 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,926 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,926 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,926 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-39krqxao
2025-09-13 18:55:05,927 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,929 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:38985
2025-09-13 18:55:05,930 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:38985
2025-09-13 18:55:05,930 - distributed.worker - INFO -          dashboard at:           10.6.85.42:38051
2025-09-13 18:55:05,930 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,930 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,930 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,930 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,930 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-x7bfn5ii
2025-09-13 18:55:05,930 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,936 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:36945
2025-09-13 18:55:05,936 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:36945
2025-09-13 18:55:05,936 - distributed.worker - INFO -          dashboard at:           10.6.85.42:38505
2025-09-13 18:55:05,936 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,936 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,936 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,936 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,936 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-6egpu6xm
2025-09-13 18:55:05,936 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,937 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:36161
2025-09-13 18:55:05,937 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:36161
2025-09-13 18:55:05,937 - distributed.worker - INFO -          dashboard at:           10.6.85.42:41339
2025-09-13 18:55:05,937 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,937 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,937 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,937 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,937 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-zo73_vyq
2025-09-13 18:55:05,937 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,941 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:39519
2025-09-13 18:55:05,941 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:39519
2025-09-13 18:55:05,941 - distributed.worker - INFO -          dashboard at:           10.6.85.42:41825
2025-09-13 18:55:05,941 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,941 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,941 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,941 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,941 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,941 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-n_h5p39o
2025-09-13 18:55:05,941 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,942 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,942 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,944 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,944 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,945 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,945 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,946 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,947 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:34527
2025-09-13 18:55:05,947 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:34527
2025-09-13 18:55:05,947 - distributed.worker - INFO -          dashboard at:           10.6.85.42:37253
2025-09-13 18:55:05,947 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,947 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,947 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,947 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,947 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-4102xbfa
2025-09-13 18:55:05,947 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,948 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,949 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,949 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,949 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:46617
2025-09-13 18:55:05,949 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:46617
2025-09-13 18:55:05,950 - distributed.worker - INFO -          dashboard at:           10.6.85.42:43139
2025-09-13 18:55:05,950 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,950 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,950 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,950 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,950 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-zcr2qhlk
2025-09-13 18:55:05,950 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,950 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,951 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,952 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,952 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,952 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,953 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,954 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,954 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,954 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,955 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:39177
2025-09-13 18:55:05,955 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:39177
2025-09-13 18:55:05,955 - distributed.worker - INFO -          dashboard at:           10.6.85.42:37239
2025-09-13 18:55:05,955 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,955 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,955 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,955 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,955 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-_z73m0wm
2025-09-13 18:55:05,955 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,955 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,955 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,955 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,956 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,957 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,957 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,958 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,959 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,960 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,961 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,961 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,962 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,964 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,965 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,965 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,966 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:40539
2025-09-13 18:55:05,966 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:40539
2025-09-13 18:55:05,966 - distributed.worker - INFO -          dashboard at:           10.6.85.42:34487
2025-09-13 18:55:05,966 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,966 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,966 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,966 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,966 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-3r47rn3z
2025-09-13 18:55:05,966 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,967 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,972 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:45587
2025-09-13 18:55:05,972 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:45587
2025-09-13 18:55:05,972 - distributed.worker - INFO -          dashboard at:           10.6.85.42:41165
2025-09-13 18:55:05,972 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,972 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,972 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,972 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,972 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-n78g5dvc
2025-09-13 18:55:05,972 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,973 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,974 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,974 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,975 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,975 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,975 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,976 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,976 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:35289
2025-09-13 18:55:05,976 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:35289
2025-09-13 18:55:05,976 - distributed.worker - INFO -          dashboard at:           10.6.85.42:44481
2025-09-13 18:55:05,976 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,976 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,976 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,976 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,976 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-nq4v2b0g
2025-09-13 18:55:05,976 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,977 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,977 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,978 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,978 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,980 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,984 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.42:35615
2025-09-13 18:55:05,984 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.42:35615
2025-09-13 18:55:05,984 - distributed.worker - INFO -          dashboard at:           10.6.85.42:36361
2025-09-13 18:55:05,984 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,984 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,984 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,984 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,984 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-onle6ae0
2025-09-13 18:55:05,984 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,985 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,985 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,986 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,986 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,986 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,986 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,986 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,987 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,990 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,991 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,991 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,993 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,994 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,995 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,995 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,995 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:11,297 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,297 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,297 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,297 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,297 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,297 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,297 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,298 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,298 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,298 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,298 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,298 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,298 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,298 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,298 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,298 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,298 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,299 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,299 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,299 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,299 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,299 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,299 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,299 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,300 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,300 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,300 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,300 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,300 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,300 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,300 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,300 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,300 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,300 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,300 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,300 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,300 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,301 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,301 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,301 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,301 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,301 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,301 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,301 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,301 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,301 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,301 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,301 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,301 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,301 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,301 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,302 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,302 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,302 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,302 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,302 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,302 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,302 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,302 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,302 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,302 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,302 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,302 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,302 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,302 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,302 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,302 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,302 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,302 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,302 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,302 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,303 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,303 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,303 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,303 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,301 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,303 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,303 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,303 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,303 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,302 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,304 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,304 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,304 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,304 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,305 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,303 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,305 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,305 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,305 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,304 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,305 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,305 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,304 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,305 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,306 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,310 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,311 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,308 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,311 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,312 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,312 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,314 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,315 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:13,874 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,874 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,874 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,874 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,875 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,875 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,875 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,875 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,875 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,875 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,875 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,875 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,876 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,876 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,876 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,876 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,876 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,876 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,876 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,876 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,877 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,876 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,877 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,877 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,876 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,877 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,877 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,877 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,877 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,877 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,876 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,877 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,877 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,878 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,877 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,878 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,878 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,878 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,878 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,877 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,878 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,878 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,878 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,878 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,878 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,878 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,878 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,878 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,878 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,878 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,879 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,879 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,879 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,879 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,879 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,878 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,878 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,878 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,879 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,879 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,879 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,878 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,879 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,879 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,880 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,880 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,879 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,880 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,879 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,880 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,880 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,880 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,879 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,879 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,880 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,878 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,880 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,880 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,879 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,880 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,880 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,880 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,881 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,880 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,881 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,881 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,881 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,881 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,881 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,879 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,881 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,881 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,882 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,882 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,882 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,882 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,882 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,880 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,882 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,883 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,883 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,883 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,883 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,884 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,943 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,943 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,943 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,943 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,943 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,943 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,943 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,943 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,944 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,944 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,944 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,944 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,944 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,944 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,944 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,944 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,945 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,945 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,945 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,945 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,945 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,945 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,945 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,945 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,945 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,946 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,946 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,946 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,946 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,946 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,946 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,946 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,946 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,946 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,946 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,946 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,947 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,947 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,947 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,947 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,947 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,947 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,947 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,947 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,947 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,947 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,947 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,947 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,947 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,947 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,947 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,947 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,947 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,948 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,948 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,948 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,948 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,948 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,948 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,948 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,948 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,948 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,948 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,948 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,948 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,948 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,948 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,948 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,948 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,949 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,949 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,949 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,949 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,949 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,949 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,949 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,948 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,949 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,949 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,950 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,950 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,950 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,950 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,950 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,951 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,951 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,951 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,951 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,951 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,951 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,951 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,951 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,951 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,951 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,951 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,951 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,952 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,952 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,952 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,952 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,952 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,952 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,953 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,958 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:14,041 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,042 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,042 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,042 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,042 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,042 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,042 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,042 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,042 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,042 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,042 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,042 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,042 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,043 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,043 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,043 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,043 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,043 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,043 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,043 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,043 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,043 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,043 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,043 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,044 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,044 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,044 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,044 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,044 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,044 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,044 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,044 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,044 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,044 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,044 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,044 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,044 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,044 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,044 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,044 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,044 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,044 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,044 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,044 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,044 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,045 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,045 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,045 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,045 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,045 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,045 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,045 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,045 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,045 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,045 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,045 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,045 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,045 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,045 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,045 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,045 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,045 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,046 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,046 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,046 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,046 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,046 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,046 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,046 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,046 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,046 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,046 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,046 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,046 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,046 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,046 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,047 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,047 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,047 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,047 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,047 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,047 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,047 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,047 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,047 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,048 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,048 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,048 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,048 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,048 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,048 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,048 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,048 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,048 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,048 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,048 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,049 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,049 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,049 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,049 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,049 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,049 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,049 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,049 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:56:11,874 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,881 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,881 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,883 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,887 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:56:11,905 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,228 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:38335. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,228 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,228 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:32853. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,228 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:33555. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,228 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:42635. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,228 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:41667. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,229 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:44547. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,229 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:32963. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,228 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,229 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:36097. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,229 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,229 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,229 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:46759. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,229 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,229 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:33169. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,229 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,229 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,229 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,229 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,229 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:35289. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,229 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,229 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:33489. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,229 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:46617. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,230 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:38761. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,229 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,229 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,229 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,230 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:44091. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,230 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:45903. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,229 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,230 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:39713. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,230 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:41595. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,228 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,230 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:43891. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,230 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:43499. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,229 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,228 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,230 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:36207. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,227 - distributed.semaphore - ERROR - Release failed for id=5ab14cb4e9a74f4fa85e50b15102571d, lease_id=ff560bf57bb4432bae3df9710e25934e, name=/g/data/xv83/users/at2708/bias_adjustment_acs_qme/qme_dev_copy_hourly/outputs6/BOM/ACCESS-CM2/historical/r4i1p1f1/BARPA-R/v1-r1-ACS-QME-BARRAR2-1980-2022/1hr/tasAdjust/v20250311/tasAdjust_AUST-05i_ACCESS-CM2_historical_r4i1p1f1_BOM_BARPA-R_v1-r1-ACS-QME-BARRAR2-1980-2022_1hr_19600101-19601231.nc. Cluster network might be unstable?
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/semaphore.py", line 486, in _release
    await retry_operation(
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.semaphore_release local=tcp://10.6.85.42:56288 remote=tcp://10.6.85.37:8784>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-13 18:57:39,230 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:39519. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,228 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,231 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:35735. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,229 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,229 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,231 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:36161. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,229 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,231 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:38631. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,231 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:34997. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,231 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:33341. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,231 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:34527. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,229 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,229 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,229 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,231 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:45607. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,231 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:39177. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,229 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,232 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:46439. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,232 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:35615. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,232 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:39619. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,232 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:38985. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,232 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:35299. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,232 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:32961. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,232 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,232 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:41671. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,233 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:40539. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,233 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:36203. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,233 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:38933. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,233 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:40843. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,241 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:45185'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,243 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:43171'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,244 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,244 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,244 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:37031'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,244 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,244 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,244 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,245 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,245 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,245 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,245 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,245 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,245 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,245 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,245 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,245 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,245 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,246 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,246 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,246 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,249 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,250 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,251 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,252 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:40729'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,252 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:46623'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,253 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:39675'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,253 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,253 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:44295'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,253 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,253 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,253 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:36095'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,253 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,253 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,253 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,253 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,253 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:36841'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,254 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,254 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,254 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,254 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 482, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,254 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,254 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:36409'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,254 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,254 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 633, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,254 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,254 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,254 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,254 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:45291'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,254 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,254 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,254 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,254 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:45077'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,254 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 124, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,254 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,255 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 384, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,255 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:44187'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,255 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 693, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,255 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,255 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,255 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 53, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,255 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 441, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,255 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:33881'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,255 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,255 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,255 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,255 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,255 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 562, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,255 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,255 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,255 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:37947'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,255 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,255 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,256 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,256 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,256 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,256 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,256 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,256 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,256 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 220, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,256 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,256 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,256 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,256 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,256 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,256 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,256 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,256 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 341, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,256 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,256 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,256 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,256 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,256 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,256 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,256 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 325, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,256 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,256 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,256 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,256 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 260, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,257 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,257 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,257 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,257 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,257 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,257 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,257 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,257 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,257 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,258 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,258 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,258 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,258 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,259 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,259 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,259 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,259 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,260 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,261 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,261 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,263 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,264 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:45147'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,264 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:32841'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,264 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:41263'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,264 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:40121'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,264 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:39099'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,264 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 453, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,265 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:38741'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,265 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 521, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,265 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:34261'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,265 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,265 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,265 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 157, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,265 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:33107'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,265 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,265 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,265 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 682, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,265 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,265 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:40741'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,265 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,265 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,265 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,266 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,266 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:46271'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,266 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,266 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,266 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,266 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,266 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,266 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,266 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,266 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 72, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,266 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:38469'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,266 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,266 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,266 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,266 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,266 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,266 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 717, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,266 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,266 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:41503'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,266 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 650, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,266 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,266 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,266 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,266 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,266 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:43005'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,266 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,266 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,266 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,266 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,266 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,266 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,266 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,267 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:36043'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,267 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,267 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,267 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,267 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,267 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,267 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,267 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,267 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:38715'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,267 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,267 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,267 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,267 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 589, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,267 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,267 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,267 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,267 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,267 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:44137'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,267 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,267 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 498, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,267 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,267 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,267 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:34757'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,267 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,267 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 405, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,267 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,267 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:38805'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,267 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,267 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,267 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 613, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,267 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,268 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:45943'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,268 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,268 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,268 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,268 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:45745'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,268 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,268 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,268 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,268 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,268 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,268 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,268 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:40949'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,268 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,268 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,268 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,268 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,268 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,268 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,268 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:39539'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,268 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,268 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,268 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,268 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,268 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,268 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,268 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,269 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:43221'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,269 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,269 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:34497'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,269 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,269 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,269 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:42555'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,269 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,269 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 288, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,269 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 686, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,270 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 312, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,270 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 666, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,270 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 534, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,270 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 277, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,270 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 141, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,270 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 382, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,270 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 532, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,271 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 510, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,271 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:33635'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,271 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,272 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,272 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,272 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 56, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,272 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 313, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,272 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,272 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,273 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,273 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,273 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,273 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,273 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,273 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,274 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,275 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,275 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,276 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,276 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,276 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,277 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,277 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,278 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,869 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,869 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:40255. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,872 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:42807'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,872 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,872 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,872 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,872 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,872 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,873 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,873 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14b01a2ef990>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-13 18:57:39,877 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,888 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:40,133 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:40,133 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:36945. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:40,140 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:39183'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:40,141 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:40,141 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:40,141 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:40,141 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:40,141 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:40,141 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:40,141 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:40,142 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:35759. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:40,141 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14fd3454b2d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-13 18:57:40,144 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:36735'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:40,145 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:40,145 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:40,145 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:40,145 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:40,145 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:40,145 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:40,145 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:40,145 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1502ccefebd0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-13 18:57:40,149 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:40,230 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:40,230 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:43553. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:40,233 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:36783'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:40,234 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:40,234 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:40,234 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:40,234 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:40,234 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:40,234 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:40,234 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x148585a2f290>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-13 18:57:40,237 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:40,335 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:40,335 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:40,335 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:45587. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:40,335 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:39131. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:40,338 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:41053'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:40,339 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:40,339 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:40,339 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:40,339 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:40,339 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:40,339 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:40,340 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:42671'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:40,340 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:40,341 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:40,341 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:40,341 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:40,341 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:40,341 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:40,339 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x145b81bcf4d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-13 18:57:40,343 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:40,341 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14f4cafb2fd0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-13 18:57:40,343 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:40,343 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:34823. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:40,344 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:40,346 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:41409'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:40,346 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:40,346 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:40,346 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:40,346 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:40,347 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:40,347 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:40,347 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1533b5337390>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-13 18:57:40,350 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:40,354 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:40,355 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:43173. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:40,357 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:34431'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:40,357 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:40,358 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:40,358 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:40,358 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:40,358 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:40,358 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:40,359 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:40,359 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:33273. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:40,358 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1471b2766ed0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-13 18:57:40,361 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:40,362 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:35773'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:40,362 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:40,363 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:40,363 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:40,363 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:40,363 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:40,363 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:40,363 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x146ed82834d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-13 18:57:40,366 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:40,369 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:40,370 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:40801. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:40,372 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:37519'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:40,373 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:40,373 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:40,373 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:40,373 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:40,373 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:40,373 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:40,374 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1499a7e6ee50>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-13 18:57:40,377 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:40,402 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:40,403 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.42:39171. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:40,406 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.42:46583'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:40,407 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:40,407 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:40,407 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:40,407 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:40,407 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:40,407 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:40,407 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x145969503190>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-13 18:57:40,411 - distributed.nanny - INFO - Worker closed
