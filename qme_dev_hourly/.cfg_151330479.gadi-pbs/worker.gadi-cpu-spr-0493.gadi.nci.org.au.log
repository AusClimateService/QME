Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-10-01 11:43:39,338 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:35567'
2025-10-01 11:43:39,348 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:43535'
2025-10-01 11:43:39,351 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:39045'
2025-10-01 11:43:39,357 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:37729'
2025-10-01 11:43:39,361 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:46481'
2025-10-01 11:43:39,365 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:43205'
2025-10-01 11:43:39,370 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:33947'
2025-10-01 11:43:39,374 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:46761'
2025-10-01 11:43:39,378 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:40721'
2025-10-01 11:43:39,384 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:41347'
2025-10-01 11:43:39,388 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:43807'
2025-10-01 11:43:39,392 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:45389'
2025-10-01 11:43:39,396 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:36749'
2025-10-01 11:43:39,400 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:46377'
2025-10-01 11:43:39,403 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:41549'
2025-10-01 11:43:39,408 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:38919'
2025-10-01 11:43:39,412 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:35865'
2025-10-01 11:43:39,417 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:38983'
2025-10-01 11:43:39,421 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:42509'
2025-10-01 11:43:39,425 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:33709'
2025-10-01 11:43:39,499 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:41111'
2025-10-01 11:43:39,504 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:43753'
2025-10-01 11:43:39,509 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:44323'
2025-10-01 11:43:39,513 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:38409'
2025-10-01 11:43:39,518 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:45315'
2025-10-01 11:43:39,522 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:39127'
2025-10-01 11:43:39,527 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:39879'
2025-10-01 11:43:39,532 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:33673'
2025-10-01 11:43:39,537 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:40583'
2025-10-01 11:43:39,542 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:46345'
2025-10-01 11:43:39,546 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:41215'
2025-10-01 11:43:39,551 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:33583'
2025-10-01 11:43:39,556 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:36403'
2025-10-01 11:43:39,560 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:34227'
2025-10-01 11:43:39,564 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:42833'
2025-10-01 11:43:39,570 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:41159'
2025-10-01 11:43:39,573 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:45451'
2025-10-01 11:43:39,578 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:43211'
2025-10-01 11:43:39,583 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:37685'
2025-10-01 11:43:39,587 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:33811'
2025-10-01 11:43:39,591 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:34759'
2025-10-01 11:43:39,595 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:44171'
2025-10-01 11:43:39,600 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:45605'
2025-10-01 11:43:39,603 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:36915'
2025-10-01 11:43:39,608 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:37849'
2025-10-01 11:43:39,614 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:38987'
2025-10-01 11:43:39,619 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:42019'
2025-10-01 11:43:39,624 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:37471'
2025-10-01 11:43:39,629 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:39833'
2025-10-01 11:43:39,633 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:39771'
2025-10-01 11:43:39,638 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:41513'
2025-10-01 11:43:39,642 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.61:45731'
2025-10-01 11:43:40,747 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:40113
2025-10-01 11:43:40,747 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:41699
2025-10-01 11:43:40,747 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:38349
2025-10-01 11:43:40,747 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:38027
2025-10-01 11:43:40,747 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:33605
2025-10-01 11:43:40,747 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:36211
2025-10-01 11:43:40,747 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:43489
2025-10-01 11:43:40,747 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:41455
2025-10-01 11:43:40,747 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:40113
2025-10-01 11:43:40,747 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:41071
2025-10-01 11:43:40,747 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:36227
2025-10-01 11:43:40,747 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:41699
2025-10-01 11:43:40,747 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:46675
2025-10-01 11:43:40,747 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:38349
2025-10-01 11:43:40,747 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:42921
2025-10-01 11:43:40,747 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:33087
2025-10-01 11:43:40,747 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:38027
2025-10-01 11:43:40,747 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:40065
2025-10-01 11:43:40,747 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:44133
2025-10-01 11:43:40,747 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:33529
2025-10-01 11:43:40,748 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:33605
2025-10-01 11:43:40,747 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:33155
2025-10-01 11:43:40,747 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:40105
2025-10-01 11:43:40,748 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:36211
2025-10-01 11:43:40,747 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:37715
2025-10-01 11:43:40,748 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:43489
2025-10-01 11:43:40,748 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:41455
2025-10-01 11:43:40,748 - distributed.worker - INFO -          dashboard at:          10.6.102.61:38881
2025-10-01 11:43:40,748 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:40821
2025-10-01 11:43:40,748 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:41071
2025-10-01 11:43:40,748 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:36227
2025-10-01 11:43:40,748 - distributed.worker - INFO -          dashboard at:          10.6.102.61:38135
2025-10-01 11:43:40,748 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:46675
2025-10-01 11:43:40,748 - distributed.worker - INFO -          dashboard at:          10.6.102.61:44719
2025-10-01 11:43:40,748 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:42921
2025-10-01 11:43:40,748 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:33087
2025-10-01 11:43:40,748 - distributed.worker - INFO -          dashboard at:          10.6.102.61:34667
2025-10-01 11:43:40,748 - distributed.worker - INFO -          dashboard at:          10.6.102.61:37057
2025-10-01 11:43:40,748 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:40065
2025-10-01 11:43:40,748 - distributed.worker - INFO -          dashboard at:          10.6.102.61:39757
2025-10-01 11:43:40,748 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:44133
2025-10-01 11:43:40,748 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:33529
2025-10-01 11:43:40,748 - distributed.worker - INFO -          dashboard at:          10.6.102.61:40805
2025-10-01 11:43:40,748 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:33155
2025-10-01 11:43:40,748 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:40105
2025-10-01 11:43:40,748 - distributed.worker - INFO -          dashboard at:          10.6.102.61:46799
2025-10-01 11:43:40,748 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:37715
2025-10-01 11:43:40,748 - distributed.worker - INFO -          dashboard at:          10.6.102.61:37055
2025-10-01 11:43:40,748 - distributed.worker - INFO -          dashboard at:          10.6.102.61:33809
2025-10-01 11:43:40,748 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,748 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:40821
2025-10-01 11:43:40,748 - distributed.worker - INFO -          dashboard at:          10.6.102.61:46043
2025-10-01 11:43:40,748 - distributed.worker - INFO -          dashboard at:          10.6.102.61:32893
2025-10-01 11:43:40,748 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,748 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,748 - distributed.worker - INFO -          dashboard at:          10.6.102.61:46067
2025-10-01 11:43:40,748 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,748 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,748 - distributed.worker - INFO -          dashboard at:          10.6.102.61:43769
2025-10-01 11:43:40,748 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,748 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,748 - distributed.worker - INFO -          dashboard at:          10.6.102.61:41727
2025-10-01 11:43:40,748 - distributed.worker - INFO -          dashboard at:          10.6.102.61:37209
2025-10-01 11:43:40,748 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,748 - distributed.worker - INFO -          dashboard at:          10.6.102.61:33393
2025-10-01 11:43:40,748 - distributed.worker - INFO -          dashboard at:          10.6.102.61:45087
2025-10-01 11:43:40,748 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,748 - distributed.worker - INFO -          dashboard at:          10.6.102.61:35577
2025-10-01 11:43:40,748 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,748 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,748 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,748 - distributed.worker - INFO -          dashboard at:          10.6.102.61:33789
2025-10-01 11:43:40,748 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,748 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,748 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,748 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,748 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,748 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,748 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,748 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,748 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,748 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,748 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,748 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,748 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,748 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,748 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,748 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,748 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,748 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,748 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,748 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,748 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,748 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,748 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,748 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,749 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,748 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,748 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,748 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:35869
2025-10-01 11:43:40,748 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,748 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,748 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,748 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,748 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,748 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,748 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,748 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,749 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,749 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,749 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,749 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,749 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,749 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,749 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,749 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,749 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,749 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,749 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,749 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,749 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,749 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,749 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:35869
2025-10-01 11:43:40,749 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,749 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,749 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,749 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,749 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,749 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,749 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,749 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,749 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,749 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,749 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,749 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-mlmjlfsj
2025-10-01 11:43:40,749 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,749 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,749 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-nsukkp4n
2025-10-01 11:43:40,749 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,749 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-9qn56ggh
2025-10-01 11:43:40,749 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-cjnhmwg6
2025-10-01 11:43:40,749 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,749 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-izvk4ngy
2025-10-01 11:43:40,749 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-vdctxw3j
2025-10-01 11:43:40,749 - distributed.worker - INFO -          dashboard at:          10.6.102.61:41995
2025-10-01 11:43:40,749 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,749 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-bfcwq2t5
2025-10-01 11:43:40,749 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-df3l1m30
2025-10-01 11:43:40,749 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,749 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,749 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-xxcw9wq2
2025-10-01 11:43:40,749 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-7roqzz2v
2025-10-01 11:43:40,749 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,749 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-bz9tb98s
2025-10-01 11:43:40,749 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-s8shwj7c
2025-10-01 11:43:40,749 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,749 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-9yisbhqd
2025-10-01 11:43:40,749 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-698e1fx_
2025-10-01 11:43:40,749 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,749 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-ezan62l3
2025-10-01 11:43:40,749 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,749 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,749 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-z6tinipt
2025-10-01 11:43:40,749 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,749 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,749 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,749 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-za182_38
2025-10-01 11:43:40,749 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,749 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,749 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-fktykcv5
2025-10-01 11:43:40,749 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-_z62gdw5
2025-10-01 11:43:40,749 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,749 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,749 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-724ipz04
2025-10-01 11:43:40,749 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,749 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,749 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,749 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,749 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,749 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,749 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,749 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,749 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,749 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,749 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,749 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,749 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,749 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-78s7qsz9
2025-10-01 11:43:40,749 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,755 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:35327
2025-10-01 11:43:40,755 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:35327
2025-10-01 11:43:40,755 - distributed.worker - INFO -          dashboard at:          10.6.102.61:34953
2025-10-01 11:43:40,755 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,755 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,755 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,755 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,755 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-y0flm1bd
2025-10-01 11:43:40,756 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,757 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:44047
2025-10-01 11:43:40,757 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:44047
2025-10-01 11:43:40,757 - distributed.worker - INFO -          dashboard at:          10.6.102.61:37257
2025-10-01 11:43:40,757 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,757 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,757 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,757 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,757 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-chp1iyyp
2025-10-01 11:43:40,757 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,758 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:34105
2025-10-01 11:43:40,758 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:34105
2025-10-01 11:43:40,758 - distributed.worker - INFO -          dashboard at:          10.6.102.61:37883
2025-10-01 11:43:40,758 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,758 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,758 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,759 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,759 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-25_rblmz
2025-10-01 11:43:40,759 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,778 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:40,778 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,778 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,779 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:40,786 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:40,786 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,786 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,787 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:40,792 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:40,793 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,794 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,795 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:40,798 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:40,799 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,800 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,801 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:40,803 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:40,804 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,804 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,806 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:40,807 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:40,808 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,808 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,810 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:40,810 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:40,812 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,812 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,813 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:40,815 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:40,816 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,816 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,818 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:40,818 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:40,818 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,819 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,820 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:40,821 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:40,822 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,822 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,823 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:40,824 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:40,824 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,825 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,826 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:40,826 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:40,827 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,827 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,828 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:40,829 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:40,829 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,830 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,831 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:40,831 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:40,832 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,832 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,833 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:40,834 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:40,834 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,835 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,836 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:40,836 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:40,837 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,837 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,839 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:40,839 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:40,840 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,840 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,841 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:40,842 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:40,842 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,842 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,843 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:40,844 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:40,845 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,845 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,846 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:40,846 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:40,847 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,847 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,848 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:40,848 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:40,849 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,849 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,850 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:40459
2025-10-01 11:43:40,850 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:40459
2025-10-01 11:43:40,850 - distributed.worker - INFO -          dashboard at:          10.6.102.61:44597
2025-10-01 11:43:40,850 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,850 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,850 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,850 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,850 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-jqh6p_ae
2025-10-01 11:43:40,850 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,850 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:40,851 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:40,851 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,852 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,853 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:40,853 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:40,854 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,854 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,855 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:40,855 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:40,856 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,857 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,858 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:40,866 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:40,866 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,866 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,867 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:40,899 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:33475
2025-10-01 11:43:40,899 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:33475
2025-10-01 11:43:40,899 - distributed.worker - INFO -          dashboard at:          10.6.102.61:44057
2025-10-01 11:43:40,899 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,899 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,899 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,899 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,899 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-2jghjz27
2025-10-01 11:43:40,899 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,923 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:40,924 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,924 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,925 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:40,947 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:44337
2025-10-01 11:43:40,947 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:37473
2025-10-01 11:43:40,947 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:44337
2025-10-01 11:43:40,947 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:37473
2025-10-01 11:43:40,947 - distributed.worker - INFO -          dashboard at:          10.6.102.61:36711
2025-10-01 11:43:40,947 - distributed.worker - INFO -          dashboard at:          10.6.102.61:46727
2025-10-01 11:43:40,947 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,947 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,947 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,947 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,947 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,947 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,947 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,947 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,947 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-kkw3czlb
2025-10-01 11:43:40,947 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-dk8l2byv
2025-10-01 11:43:40,947 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,947 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,954 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:40541
2025-10-01 11:43:40,954 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:40541
2025-10-01 11:43:40,954 - distributed.worker - INFO -          dashboard at:          10.6.102.61:37339
2025-10-01 11:43:40,954 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,954 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,954 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,954 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,954 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-irpbmofj
2025-10-01 11:43:40,954 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,955 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:41357
2025-10-01 11:43:40,955 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:41357
2025-10-01 11:43:40,955 - distributed.worker - INFO -          dashboard at:          10.6.102.61:40285
2025-10-01 11:43:40,955 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,955 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,955 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,955 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,955 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-2xabpjc_
2025-10-01 11:43:40,956 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,975 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:40,976 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,976 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,977 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:40,981 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:43129
2025-10-01 11:43:40,981 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:43129
2025-10-01 11:43:40,982 - distributed.worker - INFO -          dashboard at:          10.6.102.61:42627
2025-10-01 11:43:40,982 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,982 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,982 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:40,982 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:40,982 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-e8cx6mn5
2025-10-01 11:43:40,982 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,982 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:40,983 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,984 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,985 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:40,988 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:40,989 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,990 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,991 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:40,993 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:40,994 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:40,994 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:40,996 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:41,016 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:44243
2025-10-01 11:43:41,016 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:44243
2025-10-01 11:43:41,016 - distributed.worker - INFO -          dashboard at:          10.6.102.61:44141
2025-10-01 11:43:41,016 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,016 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,016 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:41,016 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:41,016 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-0fsie7on
2025-10-01 11:43:41,016 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,017 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:41,017 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:35969
2025-10-01 11:43:41,017 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:35969
2025-10-01 11:43:41,017 - distributed.worker - INFO -          dashboard at:          10.6.102.61:44135
2025-10-01 11:43:41,017 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,017 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,017 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:41,017 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:41,017 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-vg9op3gh
2025-10-01 11:43:41,017 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,018 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,018 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,019 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:41,026 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:44249
2025-10-01 11:43:41,026 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:44249
2025-10-01 11:43:41,026 - distributed.worker - INFO -          dashboard at:          10.6.102.61:34819
2025-10-01 11:43:41,026 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,026 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,026 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:41,026 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:41,026 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-x3tdnqvb
2025-10-01 11:43:41,026 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,041 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:41,042 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,043 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,044 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:41,049 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:41,050 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,050 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,052 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:41,056 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:41,057 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,057 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,059 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:41,148 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:44877
2025-10-01 11:43:41,148 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:44877
2025-10-01 11:43:41,148 - distributed.worker - INFO -          dashboard at:          10.6.102.61:43153
2025-10-01 11:43:41,148 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,148 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,148 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:41,148 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:41,148 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-q230o0x8
2025-10-01 11:43:41,148 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,148 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:36017
2025-10-01 11:43:41,149 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:36017
2025-10-01 11:43:41,149 - distributed.worker - INFO -          dashboard at:          10.6.102.61:40613
2025-10-01 11:43:41,149 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,149 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,149 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:41,149 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:41,149 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-l5o6h7bm
2025-10-01 11:43:41,149 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,171 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:45709
2025-10-01 11:43:41,171 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:45709
2025-10-01 11:43:41,171 - distributed.worker - INFO -          dashboard at:          10.6.102.61:37999
2025-10-01 11:43:41,171 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,171 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,171 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:41,171 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:41,171 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-20ollssj
2025-10-01 11:43:41,171 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,172 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:38893
2025-10-01 11:43:41,172 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:38893
2025-10-01 11:43:41,172 - distributed.worker - INFO -          dashboard at:          10.6.102.61:44561
2025-10-01 11:43:41,172 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,172 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,172 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:41,172 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:41,172 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-uiy6o122
2025-10-01 11:43:41,172 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,176 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:38125
2025-10-01 11:43:41,176 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:38125
2025-10-01 11:43:41,176 - distributed.worker - INFO -          dashboard at:          10.6.102.61:45537
2025-10-01 11:43:41,176 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,176 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,176 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:41,176 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:41,176 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:41,176 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-68nnzrdz
2025-10-01 11:43:41,177 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,177 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,178 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,179 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:41,182 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:42481
2025-10-01 11:43:41,183 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:42481
2025-10-01 11:43:41,183 - distributed.worker - INFO -          dashboard at:          10.6.102.61:33125
2025-10-01 11:43:41,183 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,183 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,183 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:41,183 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:41,183 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-1voqj_bj
2025-10-01 11:43:41,183 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,183 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:41,184 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,184 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,186 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:41,186 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:41645
2025-10-01 11:43:41,186 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:41645
2025-10-01 11:43:41,186 - distributed.worker - INFO -          dashboard at:          10.6.102.61:42427
2025-10-01 11:43:41,186 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,187 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,187 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:41,187 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:41,187 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-e98c0nfv
2025-10-01 11:43:41,187 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,187 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:45507
2025-10-01 11:43:41,187 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:45507
2025-10-01 11:43:41,187 - distributed.worker - INFO -          dashboard at:          10.6.102.61:43927
2025-10-01 11:43:41,187 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,187 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,187 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:41,187 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:41,187 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-gs5u1oy0
2025-10-01 11:43:41,187 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,189 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:41651
2025-10-01 11:43:41,189 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:42837
2025-10-01 11:43:41,189 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:41651
2025-10-01 11:43:41,189 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:42837
2025-10-01 11:43:41,189 - distributed.worker - INFO -          dashboard at:          10.6.102.61:41019
2025-10-01 11:43:41,189 - distributed.worker - INFO -          dashboard at:          10.6.102.61:39037
2025-10-01 11:43:41,189 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,189 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,189 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,189 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:41,189 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,189 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:41,189 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:41,189 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:41,189 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-50xuinzp
2025-10-01 11:43:41,189 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-p3t48soq
2025-10-01 11:43:41,189 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,189 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,191 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:33067
2025-10-01 11:43:41,191 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:33067
2025-10-01 11:43:41,191 - distributed.worker - INFO -          dashboard at:          10.6.102.61:36625
2025-10-01 11:43:41,191 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,191 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,191 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:41,191 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:41,191 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-qr_sm8wf
2025-10-01 11:43:41,191 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,193 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:34437
2025-10-01 11:43:41,194 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:34437
2025-10-01 11:43:41,194 - distributed.worker - INFO -          dashboard at:          10.6.102.61:40705
2025-10-01 11:43:41,194 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,194 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,194 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:41,194 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:41,194 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-yahosfxx
2025-10-01 11:43:41,194 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,195 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:34787
2025-10-01 11:43:41,195 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:34787
2025-10-01 11:43:41,195 - distributed.worker - INFO -          dashboard at:          10.6.102.61:46317
2025-10-01 11:43:41,195 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,195 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,195 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:41,195 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:41,195 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-xdsbrb33
2025-10-01 11:43:41,195 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,198 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:42305
2025-10-01 11:43:41,199 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:42305
2025-10-01 11:43:41,199 - distributed.worker - INFO -          dashboard at:          10.6.102.61:41705
2025-10-01 11:43:41,199 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,199 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,199 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:41,199 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:41,199 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-0er9qj4h
2025-10-01 11:43:41,199 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,201 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:41,201 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:35681
2025-10-01 11:43:41,201 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:35681
2025-10-01 11:43:41,201 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,201 - distributed.worker - INFO -          dashboard at:          10.6.102.61:45509
2025-10-01 11:43:41,201 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,201 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,201 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,201 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:41,202 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:41,202 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-nyvq_azw
2025-10-01 11:43:41,202 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,202 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:41,205 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:46827
2025-10-01 11:43:41,205 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:46827
2025-10-01 11:43:41,205 - distributed.worker - INFO -          dashboard at:          10.6.102.61:40895
2025-10-01 11:43:41,205 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,205 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,205 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:41,205 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:41,205 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-gx6gc5va
2025-10-01 11:43:41,205 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,206 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:34407
2025-10-01 11:43:41,206 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:34407
2025-10-01 11:43:41,206 - distributed.worker - INFO -          dashboard at:          10.6.102.61:42445
2025-10-01 11:43:41,206 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,206 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,206 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:41,206 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:41,206 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-cgjcxgpi
2025-10-01 11:43:41,206 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,208 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:41,208 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,208 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,209 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:41,213 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:41,215 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,215 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,216 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:41,217 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:41,218 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,219 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,220 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:41,222 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:41,223 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,223 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,224 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:41,225 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:41,226 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,226 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,227 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:41,229 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:41,230 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,230 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,232 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:41,232 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:41,233 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,233 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,235 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:41,235 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:41,236 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,236 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,237 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:41,237 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:41,238 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,238 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,239 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:41,240 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:41,241 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.61:45985
2025-10-01 11:43:41,241 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,241 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.61:45985
2025-10-01 11:43:41,241 - distributed.worker - INFO -          dashboard at:          10.6.102.61:40743
2025-10-01 11:43:41,241 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,241 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,241 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,241 - distributed.worker - INFO -               Threads:                          2
2025-10-01 11:43:41,241 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-01 11:43:41,241 - distributed.worker - INFO -       Local Directory: /jobfs/151330479.gadi-pbs/dask-scratch-space/worker-h3ivckk_
2025-10-01 11:43:41,241 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,242 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:41,242 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:41,243 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,243 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,245 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:41,245 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:41,246 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,246 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,248 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:41,248 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:41,249 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,249 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,250 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:41,250 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:41,251 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,251 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,252 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 11:43:41,260 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-01 11:43:41,261 - distributed.worker - INFO -         Registered to:     tcp://10.6.102.37:8768
2025-10-01 11:43:41,261 - distributed.worker - INFO - -------------------------------------------------
2025-10-01 11:43:41,261 - distributed.core - INFO - Starting established connection to tcp://10.6.102.37:8768
2025-10-01 17:43:18,706 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,706 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,707 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,707 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:34407. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,707 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,707 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:40459. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,707 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,708 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:36227. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,708 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,708 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:41071. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,708 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:45985. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,709 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:40821. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,708 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,708 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,708 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,708 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,708 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,708 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,708 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,708 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,709 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,708 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,708 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,708 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,708 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,708 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,709 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,708 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,708 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,708 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,709 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,709 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,708 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,708 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,709 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,709 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,709 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,709 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,709 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,709 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,709 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,709 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,709 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,709 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,712 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:33067. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,709 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,709 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,709 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,709 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,709 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,712 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:38893. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,712 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:38125. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,712 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:34787. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,713 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:34437. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,713 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:45507. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,713 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:42305. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,713 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:33529. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,713 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:33087. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,713 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:37715. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,713 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:41455. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,713 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:44047. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,714 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:42837. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,714 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:44133. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,714 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:43489. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,714 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:46827. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,714 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:34105. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,714 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:35327. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,714 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:33155. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,715 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:45709. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,715 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:35681. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,715 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:35869. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,715 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:40113. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,715 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:40105. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,715 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:42481. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,715 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:46675. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,716 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:38027. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,716 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:41645. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,716 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:38349. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,716 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:36017. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,716 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:33605. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,716 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:40065. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,716 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:41651. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,717 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:36211. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,717 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:41699. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,717 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:44877. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,718 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:42921. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,709 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.61:35698 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,725 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,709 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.61:35640 remote=tcp://10.6.102.37:8768>: Stream is closed
2025-10-01 17:43:18,709 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.61:35686 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,709 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.61:35666 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,709 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.61:35656 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,710 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.61:35688 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,709 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.61:35680 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,709 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.61:35648 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,709 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.102.61:35672 remote=tcp://10.6.102.37:8768>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-01 17:43:18,726 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,727 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,727 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,727 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:44249. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,727 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,727 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:33475. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,727 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,727 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,727 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,727 - distributed.core - INFO - Connection to tcp://10.6.102.37:8768 has been closed.
2025-10-01 17:43:18,728 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:44243. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,728 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:40541. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,728 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:37473. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,728 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:35969. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,728 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:43129. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,728 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:41357. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,728 - distributed.worker - INFO - Stopping worker at tcp://10.6.102.61:44337. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,731 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:45731'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,733 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,738 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,738 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:39879'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,739 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:43205'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,739 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:35567'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,739 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,739 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:45451'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,739 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,739 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:38983'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,740 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,740 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,740 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,744 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,744 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,745 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,745 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,747 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,750 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:41159'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,750 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:37471'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,750 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:36403'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,750 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:37849'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,750 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,751 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:39833'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,751 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:35865'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,751 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,751 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:43535'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,751 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,751 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:36915'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,751 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,751 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:40583'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,751 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,751 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:46481'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,751 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,752 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:42833'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,752 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,752 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:36749'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,752 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,752 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:33583'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,752 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:38919'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,752 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,752 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,752 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:41347'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,752 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,753 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:45605'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,753 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:37729'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,753 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:46761'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,753 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,753 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,753 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:38409'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,753 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,753 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:33947'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,754 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:34227'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,754 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:37685'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,754 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,754 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:43807'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,754 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,754 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:40721'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,754 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,754 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:39771'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,754 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,754 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,754 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:45389'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,754 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,755 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,755 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:42019'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,755 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,755 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,755 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:42509'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,755 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,755 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:41549'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,755 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:34759'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,755 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,755 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:44323'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,756 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:41111'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,756 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,756 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:33709'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,756 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,756 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:39045'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,756 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,756 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:33811'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,756 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,756 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,756 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:46377'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,756 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,756 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:43753'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,756 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,757 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,757 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,757 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,757 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,758 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,761 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,761 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,762 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,763 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:41513'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,763 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:46345'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,763 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:33673'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,763 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:45315'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,763 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:38987'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,763 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,763 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,763 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,764 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:43211'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,764 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:44171'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,764 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,764 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:41215'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,764 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,764 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.102.61:39127'. Reason: worker-handle-scheduler-connection-broken
2025-10-01 17:43:18,764 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,764 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,764 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,764 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,764 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,764 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,765 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,765 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,765 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,765 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,765 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,765 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,765 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,766 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-01 17:43:18,766 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,766 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,767 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,767 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,767 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,767 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,768 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,768 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,768 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,768 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,768 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,769 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,769 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,769 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,769 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,769 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,769 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,769 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,770 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,770 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,770 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,771 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,771 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,772 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,774 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,775 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,775 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,775 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,775 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,776 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,776 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,776 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:18,777 - distributed.nanny - INFO - Worker closed
2025-10-01 17:43:20,748 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,773 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,774 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,775 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,777 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,777 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,778 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,779 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,779 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,779 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,783 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,786 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-01 17:43:20,964 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:45731'. Reason: nanny-close-gracefully
2025-10-01 17:43:20,969 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:35567'. Reason: nanny-close-gracefully
2025-10-01 17:43:20,973 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:45731' closed.
2025-10-01 17:43:20,975 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:45451'. Reason: nanny-close-gracefully
2025-10-01 17:43:20,976 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:35567' closed.
2025-10-01 17:43:20,977 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:45451' closed.
2025-10-01 17:43:20,986 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:39879'. Reason: nanny-close-gracefully
2025-10-01 17:43:20,987 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:39879' closed.
2025-10-01 17:43:21,089 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:43205'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,090 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:43205' closed.
2025-10-01 17:43:21,099 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:33947'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,099 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:33947' closed.
2025-10-01 17:43:21,133 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:46761'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,135 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:46761' closed.
2025-10-01 17:43:21,141 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:40721'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,142 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:44323'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,143 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:40721' closed.
2025-10-01 17:43:21,143 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:44323' closed.
2025-10-01 17:43:21,151 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:41347'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,152 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:41159'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,153 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:41347' closed.
2025-10-01 17:43:21,154 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:41159' closed.
2025-10-01 17:43:21,171 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:38919'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,172 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:38919' closed.
2025-10-01 17:43:21,178 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:38409'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,179 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:38409' closed.
2025-10-01 17:43:21,183 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:43535'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,184 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:43535' closed.
2025-10-01 17:43:21,185 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:38983'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,186 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:38983' closed.
2025-10-01 17:43:21,195 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:39833'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,197 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:35865'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,198 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:39833' closed.
2025-10-01 17:43:21,198 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:46481'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,200 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:35865' closed.
2025-10-01 17:43:21,201 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:45605'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,202 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:46481' closed.
2025-10-01 17:43:21,202 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:45605' closed.
2025-10-01 17:43:21,207 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:34759'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,207 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:34759' closed.
2025-10-01 17:43:21,211 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:41111'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,212 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:41111' closed.
2025-10-01 17:43:21,218 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:41215'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,219 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:41215' closed.
2025-10-01 17:43:21,226 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:37849'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,232 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:42019'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,233 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:34227'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,234 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:37849' closed.
2025-10-01 17:43:21,238 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:43807'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,241 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:33673'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,242 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:45315'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,243 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:37471'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,244 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:36403'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,245 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:42019' closed.
2025-10-01 17:43:21,246 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:36749'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,247 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:33583'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,247 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:34227' closed.
2025-10-01 17:43:21,247 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:45389'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,247 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:43211'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,248 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:43807' closed.
2025-10-01 17:43:21,254 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:33673' closed.
2025-10-01 17:43:21,254 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:45315' closed.
2025-10-01 17:43:21,254 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:37471' closed.
2025-10-01 17:43:21,255 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:36403' closed.
2025-10-01 17:43:21,258 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:36749' closed.
2025-10-01 17:43:21,258 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:33583' closed.
2025-10-01 17:43:21,258 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:45389' closed.
2025-10-01 17:43:21,258 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:43211' closed.
2025-10-01 17:43:21,258 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:39045'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,259 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:39045' closed.
2025-10-01 17:43:21,260 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:33709'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,261 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:33709' closed.
2025-10-01 17:43:21,264 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:37729'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,264 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:46345'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,265 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:38987'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,270 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:37729' closed.
2025-10-01 17:43:21,270 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:46377'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,272 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:42833'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,272 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:43753'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,272 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:46345' closed.
2025-10-01 17:43:21,274 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:41549'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,274 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:38987' closed.
2025-10-01 17:43:21,275 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:41513'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,275 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:40583'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,275 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:44171'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,277 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:33811'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,277 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:46377' closed.
2025-10-01 17:43:21,278 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:42833' closed.
2025-10-01 17:43:21,278 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:43753' closed.
2025-10-01 17:43:21,279 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:39127'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,280 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:41549' closed.
2025-10-01 17:43:21,281 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:41513' closed.
2025-10-01 17:43:21,281 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:37685'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,281 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:40583' closed.
2025-10-01 17:43:21,282 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:44171' closed.
2025-10-01 17:43:21,282 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:42509'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,282 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:33811' closed.
2025-10-01 17:43:21,282 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:36915'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,283 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:39127' closed.
2025-10-01 17:43:21,283 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:37685' closed.
2025-10-01 17:43:21,283 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:42509' closed.
2025-10-01 17:43:21,283 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:36915' closed.
2025-10-01 17:43:21,311 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.102.61:39771'. Reason: nanny-close-gracefully
2025-10-01 17:43:21,311 - distributed.nanny - INFO - Nanny at 'tcp://10.6.102.61:39771' closed.
2025-10-01 17:43:21,314 - distributed.dask_worker - INFO - End worker
