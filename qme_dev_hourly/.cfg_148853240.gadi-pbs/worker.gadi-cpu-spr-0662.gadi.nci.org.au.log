Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-05 09:25:23,764 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:38599'
2025-09-05 09:25:23,774 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:44185'
2025-09-05 09:25:23,777 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:38063'
2025-09-05 09:25:23,782 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:36285'
2025-09-05 09:25:23,786 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:36229'
2025-09-05 09:25:23,790 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:37873'
2025-09-05 09:25:23,793 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:36901'
2025-09-05 09:25:23,797 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:44029'
2025-09-05 09:25:23,801 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:37037'
2025-09-05 09:25:23,807 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:38623'
2025-09-05 09:25:23,811 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:42139'
2025-09-05 09:25:23,816 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:39961'
2025-09-05 09:25:23,820 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:38207'
2025-09-05 09:25:23,824 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:40775'
2025-09-05 09:25:23,828 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:37915'
2025-09-05 09:25:23,832 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:37417'
2025-09-05 09:25:23,837 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:40295'
2025-09-05 09:25:23,842 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:44241'
2025-09-05 09:25:23,847 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:37059'
2025-09-05 09:25:23,851 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:46839'
2025-09-05 09:25:23,924 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:38293'
2025-09-05 09:25:23,928 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:46141'
2025-09-05 09:25:23,933 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:37313'
2025-09-05 09:25:23,937 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:46305'
2025-09-05 09:25:23,941 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:45655'
2025-09-05 09:25:23,945 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:45293'
2025-09-05 09:25:23,951 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:34779'
2025-09-05 09:25:23,955 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:40199'
2025-09-05 09:25:23,960 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:40935'
2025-09-05 09:25:23,964 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:39221'
2025-09-05 09:25:23,969 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:46387'
2025-09-05 09:25:23,973 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:34271'
2025-09-05 09:25:23,977 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:45715'
2025-09-05 09:25:23,982 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:36279'
2025-09-05 09:25:23,987 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:44145'
2025-09-05 09:25:23,992 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:46315'
2025-09-05 09:25:23,996 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:36267'
2025-09-05 09:25:24,000 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:37557'
2025-09-05 09:25:24,005 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:34047'
2025-09-05 09:25:24,009 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:44247'
2025-09-05 09:25:24,013 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:43935'
2025-09-05 09:25:24,016 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:34813'
2025-09-05 09:25:24,020 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:34565'
2025-09-05 09:25:24,025 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:35987'
2025-09-05 09:25:24,029 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:33703'
2025-09-05 09:25:24,033 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:44299'
2025-09-05 09:25:24,037 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:38745'
2025-09-05 09:25:24,041 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:37843'
2025-09-05 09:25:24,046 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:44045'
2025-09-05 09:25:24,051 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:33817'
2025-09-05 09:25:24,055 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:38761'
2025-09-05 09:25:24,059 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.14:34059'
2025-09-05 09:25:25,307 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:45039
2025-09-05 09:25:25,307 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:33341
2025-09-05 09:25:25,307 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:44543
2025-09-05 09:25:25,307 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:41309
2025-09-05 09:25:25,307 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:37317
2025-09-05 09:25:25,307 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:40097
2025-09-05 09:25:25,308 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:37225
2025-09-05 09:25:25,307 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:41457
2025-09-05 09:25:25,307 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:36017
2025-09-05 09:25:25,307 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:43797
2025-09-05 09:25:25,307 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:43319
2025-09-05 09:25:25,307 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:36465
2025-09-05 09:25:25,308 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:43319
2025-09-05 09:25:25,307 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:46841
2025-09-05 09:25:25,307 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:42809
2025-09-05 09:25:25,307 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:45039
2025-09-05 09:25:25,308 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:33341
2025-09-05 09:25:25,308 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:44543
2025-09-05 09:25:25,307 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:41431
2025-09-05 09:25:25,308 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:41309
2025-09-05 09:25:25,308 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:37317
2025-09-05 09:25:25,308 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:36643
2025-09-05 09:25:25,308 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:40097
2025-09-05 09:25:25,308 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:37225
2025-09-05 09:25:25,308 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:41457
2025-09-05 09:25:25,308 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:36017
2025-09-05 09:25:25,308 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:36643
2025-09-05 09:25:25,308 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:43797
2025-09-05 09:25:25,308 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:36465
2025-09-05 09:25:25,308 - distributed.worker - INFO -          dashboard at:          10.6.105.14:42275
2025-09-05 09:25:25,308 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:46841
2025-09-05 09:25:25,308 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:42809
2025-09-05 09:25:25,308 - distributed.worker - INFO -          dashboard at:          10.6.105.14:46097
2025-09-05 09:25:25,308 - distributed.worker - INFO -          dashboard at:          10.6.105.14:37033
2025-09-05 09:25:25,308 - distributed.worker - INFO -          dashboard at:          10.6.105.14:33557
2025-09-05 09:25:25,308 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:41431
2025-09-05 09:25:25,308 - distributed.worker - INFO -          dashboard at:          10.6.105.14:39993
2025-09-05 09:25:25,308 - distributed.worker - INFO -          dashboard at:          10.6.105.14:36305
2025-09-05 09:25:25,308 - distributed.worker - INFO -          dashboard at:          10.6.105.14:38843
2025-09-05 09:25:25,308 - distributed.worker - INFO -          dashboard at:          10.6.105.14:38989
2025-09-05 09:25:25,308 - distributed.worker - INFO -          dashboard at:          10.6.105.14:33657
2025-09-05 09:25:25,308 - distributed.worker - INFO -          dashboard at:          10.6.105.14:45429
2025-09-05 09:25:25,308 - distributed.worker - INFO -          dashboard at:          10.6.105.14:41691
2025-09-05 09:25:25,308 - distributed.worker - INFO -          dashboard at:          10.6.105.14:42825
2025-09-05 09:25:25,308 - distributed.worker - INFO -          dashboard at:          10.6.105.14:33151
2025-09-05 09:25:25,308 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,308 - distributed.worker - INFO -          dashboard at:          10.6.105.14:33537
2025-09-05 09:25:25,308 - distributed.worker - INFO -          dashboard at:          10.6.105.14:38703
2025-09-05 09:25:25,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,308 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,308 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,308 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,308 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,308 - distributed.worker - INFO -          dashboard at:          10.6.105.14:42803
2025-09-05 09:25:25,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,308 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,308 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,308 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,308 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,308 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,308 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,308 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,308 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,308 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,308 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,308 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,308 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,308 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,308 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,308 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,308 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,308 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,308 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,308 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,308 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,308 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,308 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,308 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,308 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,308 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,308 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,308 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,308 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,308 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,308 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-ve2iv0wg
2025-09-05 09:25:25,309 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,309 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,309 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,309 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,309 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,309 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,309 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-a2l1z3ec
2025-09-05 09:25:25,309 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,309 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,309 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,309 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,309 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,309 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,309 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-82qsmczq
2025-09-05 09:25:25,309 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-791ldao2
2025-09-05 09:25:25,309 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,309 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,309 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-a0x82h4f
2025-09-05 09:25:25,309 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-kgudk78d
2025-09-05 09:25:25,309 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-bis46i3u
2025-09-05 09:25:25,309 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-5705z__9
2025-09-05 09:25:25,309 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-l3ra838e
2025-09-05 09:25:25,309 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,309 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,309 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-exnhhbx7
2025-09-05 09:25:25,309 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-3r_s7isa
2025-09-05 09:25:25,309 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-17drcuj1
2025-09-05 09:25:25,309 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-mm0zgne4
2025-09-05 09:25:25,309 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-5rds9jn7
2025-09-05 09:25:25,309 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,309 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,309 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-5darjy08
2025-09-05 09:25:25,309 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,309 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,309 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,309 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,309 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,309 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-t26d6bx7
2025-09-05 09:25:25,309 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,309 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,309 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,309 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,309 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,309 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,309 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,310 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:33567
2025-09-05 09:25:25,311 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:33567
2025-09-05 09:25:25,311 - distributed.worker - INFO -          dashboard at:          10.6.105.14:45215
2025-09-05 09:25:25,311 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,311 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,311 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,311 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,311 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-8cpvj6fy
2025-09-05 09:25:25,311 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,313 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:32843
2025-09-05 09:25:25,314 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:32843
2025-09-05 09:25:25,314 - distributed.worker - INFO -          dashboard at:          10.6.105.14:43487
2025-09-05 09:25:25,314 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,314 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,314 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,314 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,314 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-j2s0wjur
2025-09-05 09:25:25,314 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,314 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:40485
2025-09-05 09:25:25,314 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:40611
2025-09-05 09:25:25,314 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:40485
2025-09-05 09:25:25,314 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:32921
2025-09-05 09:25:25,315 - distributed.worker - INFO -          dashboard at:          10.6.105.14:35771
2025-09-05 09:25:25,315 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:40611
2025-09-05 09:25:25,315 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:32921
2025-09-05 09:25:25,315 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,315 - distributed.worker - INFO -          dashboard at:          10.6.105.14:35415
2025-09-05 09:25:25,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,315 - distributed.worker - INFO -          dashboard at:          10.6.105.14:42683
2025-09-05 09:25:25,315 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,315 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,315 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,315 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,315 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,315 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-la_p6hr0
2025-09-05 09:25:25,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,315 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,315 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,315 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-y96su2yv
2025-09-05 09:25:25,315 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,315 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,315 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-2iqz51_g
2025-09-05 09:25:25,316 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,316 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:45783
2025-09-05 09:25:25,317 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:39465
2025-09-05 09:25:25,317 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:45783
2025-09-05 09:25:25,317 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:39465
2025-09-05 09:25:25,317 - distributed.worker - INFO -          dashboard at:          10.6.105.14:37865
2025-09-05 09:25:25,317 - distributed.worker - INFO -          dashboard at:          10.6.105.14:40007
2025-09-05 09:25:25,317 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,317 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,317 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,317 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,317 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,317 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,317 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,317 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,317 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-icefkv_r
2025-09-05 09:25:25,317 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-byb29tyh
2025-09-05 09:25:25,317 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,317 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,341 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,342 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,342 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,343 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,345 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,346 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,346 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,347 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,350 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,351 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,352 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,353 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,354 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,355 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,356 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,357 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,357 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,357 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,357 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,358 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,360 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,361 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,362 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,363 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,363 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,364 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,364 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,365 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,366 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,366 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,366 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,367 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,368 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,368 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,368 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,370 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,370 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,371 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,371 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,372 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,373 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,373 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,373 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,374 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,374 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,375 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,375 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,376 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,376 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:42739
2025-09-05 09:25:25,376 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:42739
2025-09-05 09:25:25,376 - distributed.worker - INFO -          dashboard at:          10.6.105.14:35279
2025-09-05 09:25:25,376 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:40969
2025-09-05 09:25:25,376 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,376 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,376 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:40969
2025-09-05 09:25:25,377 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,377 - distributed.worker - INFO -          dashboard at:          10.6.105.14:44089
2025-09-05 09:25:25,377 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,377 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,377 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-rviy2mng
2025-09-05 09:25:25,377 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,377 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,377 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,377 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,377 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,377 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-9lt3x3n4
2025-09-05 09:25:25,377 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,377 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,377 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,378 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,379 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,379 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,379 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,379 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,381 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,381 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,381 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,381 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,382 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,382 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,382 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,383 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,383 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,383 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,384 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,384 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,384 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:39163
2025-09-05 09:25:25,384 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:39163
2025-09-05 09:25:25,384 - distributed.worker - INFO -          dashboard at:          10.6.105.14:36289
2025-09-05 09:25:25,384 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,384 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,384 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,384 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,384 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-t865s7sd
2025-09-05 09:25:25,384 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,384 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,385 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,385 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,386 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,386 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,387 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,387 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,387 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,388 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,388 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,388 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:42185
2025-09-05 09:25:25,388 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:42185
2025-09-05 09:25:25,388 - distributed.worker - INFO -          dashboard at:          10.6.105.14:42745
2025-09-05 09:25:25,388 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,388 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,388 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,388 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-h912tzf0
2025-09-05 09:25:25,388 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,389 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,389 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,389 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,389 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,389 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,390 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,390 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,390 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,390 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,391 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,391 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,392 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:45223
2025-09-05 09:25:25,392 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,392 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:45223
2025-09-05 09:25:25,392 - distributed.worker - INFO -          dashboard at:          10.6.105.14:45657
2025-09-05 09:25:25,392 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,392 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,392 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,392 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,392 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-1enw_3e0
2025-09-05 09:25:25,392 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,393 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,402 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:45539
2025-09-05 09:25:25,403 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:45539
2025-09-05 09:25:25,403 - distributed.worker - INFO -          dashboard at:          10.6.105.14:34109
2025-09-05 09:25:25,403 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,403 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,403 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,403 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,403 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-wjeqt8zp
2025-09-05 09:25:25,403 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,410 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:40315
2025-09-05 09:25:25,410 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:40315
2025-09-05 09:25:25,410 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,410 - distributed.worker - INFO -          dashboard at:          10.6.105.14:45383
2025-09-05 09:25:25,410 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,410 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,410 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,410 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,410 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-s4wwcysu
2025-09-05 09:25:25,410 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,411 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,411 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,411 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,412 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,412 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,413 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,413 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,414 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,414 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,414 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,414 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,416 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,416 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,416 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,416 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,417 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,417 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,417 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,419 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,422 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,423 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,423 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,424 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,435 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,436 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,436 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,437 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,438 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:45993
2025-09-05 09:25:25,438 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:45993
2025-09-05 09:25:25,438 - distributed.worker - INFO -          dashboard at:          10.6.105.14:44383
2025-09-05 09:25:25,438 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,438 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,438 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,438 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,438 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-0tvtoauo
2025-09-05 09:25:25,438 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,442 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:37769
2025-09-05 09:25:25,442 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:37769
2025-09-05 09:25:25,442 - distributed.worker - INFO -          dashboard at:          10.6.105.14:44551
2025-09-05 09:25:25,442 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,442 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,442 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,442 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,442 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-cb8eqds1
2025-09-05 09:25:25,442 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,443 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:43751
2025-09-05 09:25:25,444 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:43751
2025-09-05 09:25:25,444 - distributed.worker - INFO -          dashboard at:          10.6.105.14:40327
2025-09-05 09:25:25,444 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,444 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,444 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,444 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,444 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-j49ll28p
2025-09-05 09:25:25,444 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,447 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:46087
2025-09-05 09:25:25,447 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:46087
2025-09-05 09:25:25,447 - distributed.worker - INFO -          dashboard at:          10.6.105.14:35817
2025-09-05 09:25:25,447 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,447 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,447 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,447 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,447 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-85bkq8u2
2025-09-05 09:25:25,447 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,450 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:41409
2025-09-05 09:25:25,450 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:41409
2025-09-05 09:25:25,450 - distributed.worker - INFO -          dashboard at:          10.6.105.14:42407
2025-09-05 09:25:25,450 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,450 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,450 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,450 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,450 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-h9bzz7pm
2025-09-05 09:25:25,450 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,460 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,460 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,461 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,462 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,463 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,464 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,465 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:46331
2025-09-05 09:25:25,465 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:46331
2025-09-05 09:25:25,465 - distributed.worker - INFO -          dashboard at:          10.6.105.14:38025
2025-09-05 09:25:25,465 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,465 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,465 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,465 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,465 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-5x_wkg6b
2025-09-05 09:25:25,465 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,465 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,465 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:41379
2025-09-05 09:25:25,465 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:41379
2025-09-05 09:25:25,465 - distributed.worker - INFO -          dashboard at:          10.6.105.14:39529
2025-09-05 09:25:25,465 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,466 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,466 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,466 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,466 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-8qkrsqnv
2025-09-05 09:25:25,466 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,467 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,468 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,468 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:44307
2025-09-05 09:25:25,468 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:44307
2025-09-05 09:25:25,468 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,468 - distributed.worker - INFO -          dashboard at:          10.6.105.14:38199
2025-09-05 09:25:25,468 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,468 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,468 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,468 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,468 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-0a1qer0o
2025-09-05 09:25:25,468 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,469 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,471 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,472 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,472 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,473 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,473 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,474 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,475 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,476 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,478 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,478 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,478 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,479 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,484 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:40271
2025-09-05 09:25:25,484 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:40271
2025-09-05 09:25:25,484 - distributed.worker - INFO -          dashboard at:          10.6.105.14:40347
2025-09-05 09:25:25,485 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,485 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,485 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,485 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,485 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-opm97yji
2025-09-05 09:25:25,485 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,485 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,486 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,486 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,488 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,493 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,494 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,494 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,495 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,496 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:38273
2025-09-05 09:25:25,497 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:38273
2025-09-05 09:25:25,497 - distributed.worker - INFO -          dashboard at:          10.6.105.14:40009
2025-09-05 09:25:25,497 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,497 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,497 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,497 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,497 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-01eksz3w
2025-09-05 09:25:25,497 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,498 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:38841
2025-09-05 09:25:25,498 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:38841
2025-09-05 09:25:25,498 - distributed.worker - INFO -          dashboard at:          10.6.105.14:34579
2025-09-05 09:25:25,498 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,498 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,498 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,498 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,498 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-e558cegr
2025-09-05 09:25:25,498 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,499 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:37101
2025-09-05 09:25:25,500 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:37101
2025-09-05 09:25:25,500 - distributed.worker - INFO -          dashboard at:          10.6.105.14:38697
2025-09-05 09:25:25,500 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,500 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,500 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,500 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,500 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-bkgik8ys
2025-09-05 09:25:25,500 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,505 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,507 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,507 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,508 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,518 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,519 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,519 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,520 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,520 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,521 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,521 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,522 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,522 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,523 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,523 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,525 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,526 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:42363
2025-09-05 09:25:25,527 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:42363
2025-09-05 09:25:25,527 - distributed.worker - INFO -          dashboard at:          10.6.105.14:43773
2025-09-05 09:25:25,527 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,527 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,527 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,527 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-hdr9tz3h
2025-09-05 09:25:25,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,538 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,538 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,538 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,539 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,554 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:36645
2025-09-05 09:25:25,554 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:36645
2025-09-05 09:25:25,554 - distributed.worker - INFO -          dashboard at:          10.6.105.14:42747
2025-09-05 09:25:25,554 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,554 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,554 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,554 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,554 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-omo45x5p
2025-09-05 09:25:25,554 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,554 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:37263
2025-09-05 09:25:25,554 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:37263
2025-09-05 09:25:25,554 - distributed.worker - INFO -          dashboard at:          10.6.105.14:45777
2025-09-05 09:25:25,555 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,555 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,555 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,555 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,555 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-mnabf0eo
2025-09-05 09:25:25,555 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,555 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:39161
2025-09-05 09:25:25,555 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:39161
2025-09-05 09:25:25,555 - distributed.worker - INFO -          dashboard at:          10.6.105.14:37069
2025-09-05 09:25:25,555 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,555 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,555 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,555 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,555 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-8jhv4tcq
2025-09-05 09:25:25,555 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,561 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:42711
2025-09-05 09:25:25,561 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:42711
2025-09-05 09:25:25,561 - distributed.worker - INFO -          dashboard at:          10.6.105.14:46117
2025-09-05 09:25:25,561 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,561 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,561 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,561 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,561 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-8b5s8zhg
2025-09-05 09:25:25,561 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,576 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:44691
2025-09-05 09:25:25,576 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:44691
2025-09-05 09:25:25,576 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,576 - distributed.worker - INFO -          dashboard at:          10.6.105.14:36587
2025-09-05 09:25:25,576 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,576 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,577 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,577 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,577 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-o1q42u98
2025-09-05 09:25:25,577 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,577 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,577 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,579 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,582 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,583 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,583 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,585 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,585 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,586 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,586 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,588 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,591 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,592 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,592 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,594 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,595 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:35763
2025-09-05 09:25:25,595 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:35763
2025-09-05 09:25:25,595 - distributed.worker - INFO -          dashboard at:          10.6.105.14:46551
2025-09-05 09:25:25,595 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,595 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,595 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,595 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,595 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-prcmqmrt
2025-09-05 09:25:25,595 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,603 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:40141
2025-09-05 09:25:25,603 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:40141
2025-09-05 09:25:25,603 - distributed.worker - INFO -          dashboard at:          10.6.105.14:44091
2025-09-05 09:25:25,603 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,603 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,603 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,603 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,603 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-kbiet2c4
2025-09-05 09:25:25,603 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,606 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:37229
2025-09-05 09:25:25,606 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:37229
2025-09-05 09:25:25,606 - distributed.worker - INFO -          dashboard at:          10.6.105.14:35329
2025-09-05 09:25:25,606 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,606 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,606 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,606 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,606 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-fhwj44zf
2025-09-05 09:25:25,606 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,606 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,607 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,607 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,608 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,615 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.14:36265
2025-09-05 09:25:25,615 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.14:36265
2025-09-05 09:25:25,615 - distributed.worker - INFO -          dashboard at:          10.6.105.14:42733
2025-09-05 09:25:25,615 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,615 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,615 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:25:25,615 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:25:25,615 - distributed.worker - INFO -       Local Directory: /jobfs/148853240.gadi-pbs/dask-scratch-space/worker-jaqy4igt
2025-09-05 09:25:25,615 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,617 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,617 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,618 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,619 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,626 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,627 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,627 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,628 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,630 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,630 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,631 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,632 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:25,636 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:25:25,637 - distributed.worker - INFO -         Registered to:      tcp://10.6.105.1:8749
2025-09-05 09:25:25,637 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:25:25,638 - distributed.core - INFO - Starting established connection to tcp://10.6.105.1:8749
2025-09-05 09:25:50,833 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,833 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,834 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,834 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,834 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,834 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,834 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,834 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,834 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,834 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,834 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,835 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,835 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,835 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,835 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,835 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,835 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,835 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,835 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,836 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,836 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,836 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,836 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,836 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,836 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,836 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,836 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,836 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,836 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,836 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,836 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,837 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,837 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,837 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,837 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,837 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,836 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,837 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,837 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,837 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,837 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,837 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,837 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,837 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,837 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,837 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,837 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,837 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,837 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,838 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,838 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,838 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,838 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,838 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,837 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,838 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,837 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,838 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,839 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,838 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,839 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,838 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,839 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,838 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,839 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,838 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,839 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,838 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,838 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,838 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,839 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,838 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,838 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,838 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,839 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,839 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,840 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,840 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,840 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,840 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,840 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,840 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,839 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,841 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,841 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,841 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,842 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,842 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,840 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,842 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,842 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,842 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,843 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,842 - distributed.worker - INFO - Starting Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:25:50,845 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,845 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,846 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,841 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,846 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,841 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,848 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,849 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,850 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:50,852 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:25:53,580 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,580 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,580 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,581 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,581 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,581 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,581 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,581 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,581 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,581 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,582 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,582 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,582 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,582 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,582 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,582 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,582 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,582 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,582 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,582 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,582 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,583 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,582 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,583 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,583 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,584 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,584 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,584 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,584 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,584 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,583 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,584 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,584 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,583 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,584 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,584 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,584 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,584 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,584 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,584 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,585 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,585 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,585 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,585 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,584 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,584 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,584 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,585 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,584 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,585 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,584 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,585 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,582 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,585 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,585 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,585 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,585 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,585 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,585 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,585 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,585 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,585 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,585 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,586 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,586 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,586 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,584 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,585 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,586 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,586 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,586 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,584 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,586 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,586 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,586 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,587 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,587 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,587 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,587 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,587 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,587 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,588 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,588 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,588 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,588 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,588 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,588 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,586 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,585 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,589 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,589 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,585 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,589 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,589 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,589 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,586 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,590 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,590 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,586 - distributed.worker - INFO - Starting Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:25:53,591 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,593 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,593 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,593 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,595 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:25:53,963 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,963 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,963 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,963 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,964 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,964 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,964 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,964 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,964 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,964 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,964 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,964 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,965 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,965 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,965 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,965 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,965 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,965 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,965 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,965 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,965 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,966 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,966 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,966 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,966 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,966 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,966 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,966 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,967 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,967 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,967 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,967 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,967 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,967 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,967 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,967 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,967 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,967 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,968 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,968 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,968 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,968 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,968 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,968 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,968 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,968 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,968 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,968 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,968 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,968 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,969 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,968 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,969 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,969 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,969 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,969 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,969 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,969 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,969 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,969 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,969 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,969 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,969 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,969 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,969 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,970 - distributed.worker - INFO - Starting Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:25:53,970 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,970 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,970 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,971 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,970 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,970 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,970 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,970 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,970 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,971 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,971 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,971 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,971 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,971 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,971 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,971 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,971 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,971 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,972 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,972 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,972 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,972 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,973 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,973 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,973 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,973 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,973 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,973 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,973 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,973 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,973 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,973 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,973 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,973 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,974 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,974 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,974 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:53,975 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:25:54,342 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,342 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,342 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,342 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,342 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,342 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,342 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,342 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,342 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,343 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,343 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,343 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,343 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,343 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,343 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,343 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,344 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,344 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,344 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,344 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,344 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,344 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,344 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,344 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,344 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,344 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,345 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,345 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,345 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,345 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,345 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,345 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,345 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,344 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,345 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,345 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,344 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,345 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,345 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,345 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,345 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,345 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,345 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,345 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,345 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,346 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,346 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,346 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,346 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,346 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,346 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,346 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,346 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,346 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,346 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,346 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,346 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,346 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,346 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,346 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,346 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,346 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,346 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,346 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,347 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,347 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,347 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,347 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,347 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,347 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,347 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,347 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,347 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,347 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,347 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,347 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,347 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,347 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,347 - distributed.worker - INFO - Starting Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:25:54,348 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,348 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,348 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,348 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,348 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,348 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,348 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,349 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,349 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,349 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,349 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,349 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,350 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,350 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,350 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,350 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,350 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,350 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,350 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,351 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,351 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,351 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,351 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,351 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:25:54,361 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:33:00,751 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,835 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,840 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,873 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,884 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,934 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,937 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,946 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,948 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,960 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,977 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:00,986 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,053 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,054 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,071 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,100 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,101 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,118 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,131 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,132 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,149 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,162 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,162 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,165 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,172 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,176 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,182 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,183 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,187 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,254 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:01,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:49,028 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:51,465 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:51,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:51,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:51,567 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:51,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:52,035 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:52,084 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:52,782 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:52,784 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:53,289 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:53,293 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:53,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:53,686 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:53,686 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:53,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:53,987 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:54,248 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:54,249 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:54,250 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:54,251 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:54,253 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:54,800 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:55,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:57,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:57,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:57,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:57,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:57,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:57,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:57,732 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:57,739 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:57,978 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:58,434 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:58,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:59,528 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:33:59,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:00,837 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:01,321 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:01,322 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:01,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:01,757 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:05,273 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:05,766 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:05,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:06,259 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:06,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:06,742 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:06,754 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:07,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:09,551 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:09,729 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:10,589 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:10,963 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:13,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:13,098 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:13,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:23,561 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:48,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:50,540 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:54,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:56,187 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:59,247 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:59,268 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:59,431 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:34:59,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:01,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:01,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:01,864 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:01,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:02,102 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:02,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:02,919 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:03,120 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:03,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:03,121 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:03,298 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:03,395 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:03,783 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:04,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:04,908 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:04,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:04,995 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:05,755 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:06,414 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:06,904 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:06,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:06,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:06,928 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:07,641 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:08,344 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:09,977 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:09,978 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:09,979 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:09,979 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:09,979 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:10,064 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:13,319 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:13,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:13,495 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:13,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:13,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:13,554 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:13,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:14,080 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:14,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:14,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:14,917 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:15,206 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:15,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:15,618 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:15,618 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:15,687 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:16,028 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:16,801 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:16,814 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:16,819 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:16,978 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:17,023 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:17,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:18,207 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:18,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:19,398 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:24,952 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:24,959 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:25,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:25,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:26,465 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:27,284 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:28,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:31,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:31,656 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:32,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:32,843 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:32,972 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:33,162 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:33,178 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:33,824 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:33,967 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:34,248 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:34,312 - distributed.core - INFO - Event loop was unresponsive in Worker for 28.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:35,004 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:35,645 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:35,647 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:35,650 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:35,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:35,736 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:35,739 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:36,347 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:36,869 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:37,668 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:37,668 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:37,800 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:37,805 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:38,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:39,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:39,832 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:39,887 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:39,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:39,920 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:39,978 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:40,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:40,043 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:40,137 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:42,961 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:43,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:43,911 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:44,069 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:47,106 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:47,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:47,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:47,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:49,052 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:49,132 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:49,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:35:50,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:07,044 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:07,048 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:07,552 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:08,470 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:08,794 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:09,151 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:09,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:09,569 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:10,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:10,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:11,051 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:11,055 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:11,056 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:13,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:13,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:13,507 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:13,935 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:14,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:14,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:14,567 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:15,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:17,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:17,196 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:17,201 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:17,457 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:17,984 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:18,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:20,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:20,155 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:20,156 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:20,668 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:21,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:21,662 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:21,825 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:21,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:23,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:25,086 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:25,276 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:25,530 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:26,580 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:27,196 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:28,305 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:28,752 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:28,755 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:29,063 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:29,071 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:29,074 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:29,521 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:31,569 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:33,994 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:33,995 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:33,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:34,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:35,615 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:36,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:36,211 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:36,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:36,716 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:37,916 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:39,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:39,645 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:39,646 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:40,478 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:40,480 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:40,481 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:40,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:44,758 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:45,821 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:47,400 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:47,617 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:47,628 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:47,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:48,094 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:48,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:48,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:48,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:48,978 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:48,979 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:48,986 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:49,022 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:49,074 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:49,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:49,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:49,746 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:49,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:49,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:49,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:50,251 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:50,283 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:50,291 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:50,310 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:50,365 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:50,397 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:50,615 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:51,164 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:51,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:51,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:51,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:51,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:51,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:51,672 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:51,682 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:51,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:51,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:51,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:51,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:51,686 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:51,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:51,802 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:52,229 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:52,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:36:52,430 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:13,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:13,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:13,621 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:14,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:14,064 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:14,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:14,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:14,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:14,265 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:14,402 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:14,733 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:14,873 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:14,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:14,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:14,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:15,076 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:15,080 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:15,343 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:15,876 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:16,792 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:17,623 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:17,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:18,960 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:19,047 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:19,048 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:19,477 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:20,728 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:21,475 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:21,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:21,679 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:22,159 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:22,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:22,583 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:22,917 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:24,753 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:25,072 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:26,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:27,464 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:27,468 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:27,473 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:27,997 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:30,798 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:40,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 30.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:45,213 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:46,562 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:47,091 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:48,210 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:49,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:51,357 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:52,036 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:52,037 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:52,385 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:53,464 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:54,301 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:54,308 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:54,502 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:54,902 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:55,052 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:55,586 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:55,783 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:55,785 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:56,253 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:56,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:57,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:57,618 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:57,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:59,157 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:59,307 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:59,481 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:59,622 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:37:59,840 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:00,102 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:00,222 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:00,654 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:01,873 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:01,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:01,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:01,924 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:02,213 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:02,505 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:02,649 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:02,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:08,171 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:08,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:09,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:09,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:09,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:10,088 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:10,374 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:10,374 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:10,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:10,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:10,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:10,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:10,614 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:11,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:11,496 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:11,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:12,097 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:12,438 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:13,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:13,485 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:13,569 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:14,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:14,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:17,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:17,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:17,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:18,057 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:18,342 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:18,741 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:18,868 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:19,747 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:19,757 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:19,908 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:20,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:20,478 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:20,671 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:21,101 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:21,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:21,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:21,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:22,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:23,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:23,231 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:25,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:27,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:27,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:27,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:27,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:28,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:28,403 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:28,407 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:28,537 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:28,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:28,881 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:28,888 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:28,974 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:28,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:28,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:29,311 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:29,321 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:29,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:29,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:29,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:29,677 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:29,679 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:29,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:29,782 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:29,784 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:29,785 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:31,309 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:31,311 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:33,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:33,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:33,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:33,108 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:33,115 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:36,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:36,723 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:39,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:49,690 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:50,617 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:51,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:51,273 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:51,907 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:51,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:51,913 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:52,140 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:52,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:52,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:53,013 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:53,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:53,552 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:54,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,508 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,509 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,511 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,591 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,698 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,700 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,887 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,889 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,894 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,896 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:56,933 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:57,300 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:57,657 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:59,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:38:59,865 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:00,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:02,891 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:03,757 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:07,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:07,936 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:08,248 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:08,765 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:09,361 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:11,088 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:11,144 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:11,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:11,697 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:11,903 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:11,916 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:11,973 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:12,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:13,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:13,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:13,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:14,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:14,556 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:15,049 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:15,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:15,306 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:19,630 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:19,630 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:20,580 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:20,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:20,586 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:21,535 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:21,535 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:24,162 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:29,397 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:29,764 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:30,036 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:30,934 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:32,295 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:32,307 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:32,787 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:32,885 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:33,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:34,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:34,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:34,025 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:34,084 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:34,141 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:34,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:35,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:35,345 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:35,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:35,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:35,354 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:35,759 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:35,760 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:35,760 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:35,760 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:35,978 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:36,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:36,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:36,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:36,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:36,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:36,394 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:37,025 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:37,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:37,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:38,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:38,656 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:39,552 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:39,552 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:39,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:39,555 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:40,254 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:41,661 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:41,661 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:41,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:42,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:49,998 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:49,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:50,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:50,937 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:51,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:51,507 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:51,847 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:52,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:54,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:54,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:54,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:55,238 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:55,766 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:55,954 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:56,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:56,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:57,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:57,444 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:57,484 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:57,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:57,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:57,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:58,100 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:58,140 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:39:58,766 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:00,536 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:01,331 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:01,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:01,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:01,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:01,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:01,435 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:02,095 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:02,523 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:02,523 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:04,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:04,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:04,162 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:05,674 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:13,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:13,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:13,624 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:13,629 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:13,807 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:13,809 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:13,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:13,835 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:13,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:14,231 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:14,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:14,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:14,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:14,525 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:14,527 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:16,021 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:16,106 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:16,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:16,977 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:18,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:18,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:20,384 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:22,324 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:22,580 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:22,581 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:22,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:22,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:23,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:23,254 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:23,629 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:23,635 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:24,099 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:25,453 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:25,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:25,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:25,982 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:25,988 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:26,371 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:26,447 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:29,072 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:29,088 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:29,164 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:29,205 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:29,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:29,610 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:29,685 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:30,654 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:30,658 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:30,918 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:30,921 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:30,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:30,925 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:32,053 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:32,057 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:32,057 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:32,151 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:32,238 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:33,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:33,411 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:33,670 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:33,672 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:33,982 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:33,989 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:34,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:35,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:35,587 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:35,590 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:36,304 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:36,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:37,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:37,059 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:37,070 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:37,071 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:37,157 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:37,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:37,741 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:38,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:38,596 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:39,194 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:39,453 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:39,463 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:39,463 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:39,554 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:39,696 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:39,709 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:40,085 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:40,178 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:40,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:40,244 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:41,078 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:41,293 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:41,559 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:41,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:42,173 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:42,187 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:42,189 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:42,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:42,271 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:42,360 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:42,482 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:42,484 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:42,873 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:44,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:44,395 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:45,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:45,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:46,292 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:46,830 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:46,902 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:47,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:47,119 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:47,288 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:47,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:47,476 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:47,483 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:47,485 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:48,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:48,120 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:48,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:48,383 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:49,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:49,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:50,345 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:50,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:51,825 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:52,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:52,960 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:53,756 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:54,357 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:54,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:54,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:40:55,166 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:11,077 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:11,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:11,774 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:12,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:12,374 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:12,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:12,460 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:12,666 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:12,671 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:12,672 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:13,002 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:13,007 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:13,590 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:14,727 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:14,727 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:14,729 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:15,846 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:17,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:17,360 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:19,739 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:20,910 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:21,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:22,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:22,869 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:22,871 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:22,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:23,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:23,293 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:23,392 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:23,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:24,039 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:24,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:24,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:25,667 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:27,929 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:28,655 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:28,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:28,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:29,004 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:29,310 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:29,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:29,496 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:29,634 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:30,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:30,033 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:30,033 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:30,035 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:30,159 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:30,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:30,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:30,175 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:30,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:30,848 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:30,853 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:31,691 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:32,345 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:33,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:33,803 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:33,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:34,325 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:35,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:37,176 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:37,916 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:37,920 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:38,935 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:41,639 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:43,385 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:47,558 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:47,558 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:48,189 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:48,190 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:48,192 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:48,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:48,499 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:48,856 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:48,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:49,602 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:50,163 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:50,164 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:50,247 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:50,248 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:51,723 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:52,405 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:52,414 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:52,813 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:52,816 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:53,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:53,979 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:54,231 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:55,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:55,198 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:55,299 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:55,675 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:55,855 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:58,046 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:41:58,415 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:00,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:00,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:00,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:00,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:00,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:00,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:00,126 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:00,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:00,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:01,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:03,105 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:06,913 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:07,930 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:07,935 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:07,936 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:08,728 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:08,732 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:08,792 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:08,793 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:08,797 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:08,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:08,966 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:09,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:09,917 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:10,605 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:10,985 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:10,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:11,005 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:11,011 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:11,011 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:11,250 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:11,250 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:11,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:11,841 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:11,864 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:12,068 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:12,072 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:12,073 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:12,224 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:13,686 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:13,687 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:13,687 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:13,689 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:14,816 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:14,841 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:14,842 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:14,843 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:14,843 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:14,845 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:14,846 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:14,848 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:14,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:14,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:14,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:14,853 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:14,855 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:16,014 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:16,342 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:16,708 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:17,007 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:17,010 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:17,253 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:17,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:17,547 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:17,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:17,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:19,369 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:19,717 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:19,718 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:19,724 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:20,149 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:25,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:25,708 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:25,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:25,757 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:25,762 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:25,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:26,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:26,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:26,590 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:26,832 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:26,895 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:27,286 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:29,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:29,410 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:29,424 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:29,440 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:29,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:29,880 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:29,905 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:31,367 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:32,120 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:32,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:32,233 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:32,508 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:34,980 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:35,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:37,656 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:37,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:37,660 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:39,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:40,486 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:46,569 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:47,608 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:49,060 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:49,065 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:49,699 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:49,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:49,816 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:49,965 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:50,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:50,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:50,682 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:50,690 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:50,698 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:50,846 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:51,080 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:51,135 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:51,137 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:51,330 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:52,178 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:52,178 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:52,180 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:52,284 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:52,285 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:52,295 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:52,371 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:52,661 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:52,967 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:53,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:53,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:53,151 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:53,590 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:54,533 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:54,560 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:54,710 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:55,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:55,453 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:55,454 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:55,457 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:55,466 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:55,986 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:56,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:56,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:56,021 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:56,022 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:42:59,344 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:00,081 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:00,087 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:00,365 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:00,720 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:00,892 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:00,997 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,421 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,667 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,720 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,753 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:01,888 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:02,017 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:02,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:02,157 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:02,273 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:02,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:02,526 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:02,736 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:02,899 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:03,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:03,430 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:05,040 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:05,196 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:05,531 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:05,532 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:05,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:05,855 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:06,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:06,821 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:06,925 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:07,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:08,402 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:09,416 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:09,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:09,737 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:11,624 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:11,925 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:12,226 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:12,371 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:12,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:12,601 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:13,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:15,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:18,984 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:19,173 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:19,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:19,181 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:19,863 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:19,888 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:22,590 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:23,134 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:23,811 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:23,811 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:23,815 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:57,855 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:58,468 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:58,521 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:43:58,521 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:00,560 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:03,455 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:03,684 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:03,823 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:04,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:04,126 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:04,915 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:04,917 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:04,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:05,182 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:05,834 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:06,596 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:08,010 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:08,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:08,262 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:08,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:08,588 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:09,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:09,088 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:09,163 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:10,677 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:10,686 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:11,304 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:11,305 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:12,039 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:12,048 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:12,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:12,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:13,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:13,795 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:13,960 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:13,961 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:14,087 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:14,148 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:15,541 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:15,609 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:16,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:16,830 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:16,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:16,972 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:21,105 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:21,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:21,849 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:22,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:22,477 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:22,867 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:23,353 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:23,354 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:23,360 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:23,361 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:23,466 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:23,620 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:23,788 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:24,087 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:24,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:24,898 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:25,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:25,256 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:25,776 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:26,074 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:26,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:27,085 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:27,126 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:27,356 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:27,703 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:27,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:27,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:27,873 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:27,874 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:28,478 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:28,757 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:28,759 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:28,760 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:28,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:29,163 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:29,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:29,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:29,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:30,719 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:30,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:32,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:32,761 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:36,764 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:40,853 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:40,854 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:41,290 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:43,077 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:44,464 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:44,783 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:45,033 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:45,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:46,609 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:46,615 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:46,835 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:47,136 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:47,257 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:47,467 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:48,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:49,843 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:51,041 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:51,196 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:51,470 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:52,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:52,818 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:52,820 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:53,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:54,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:54,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:54,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:54,193 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:54,982 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:55,489 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:55,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:55,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:55,973 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:57,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:57,672 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:57,680 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:57,824 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:58,106 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:58,371 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:59,215 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:59,590 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:44:59,677 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:01,048 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:02,321 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:02,627 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:02,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:03,066 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:03,067 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:04,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:04,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:04,087 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:05,297 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:06,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:06,031 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:06,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:06,806 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:07,815 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:08,189 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:08,189 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:08,189 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:08,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:08,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:09,360 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:11,851 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:11,860 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:11,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:13,241 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:14,161 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:14,162 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:14,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:14,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:15,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:15,363 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:15,685 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:15,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:17,922 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:17,922 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:17,924 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:25,485 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:25,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:25,544 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:26,875 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:26,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:27,349 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:27,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:27,356 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:27,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:27,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:27,398 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:29,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:29,407 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:29,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:30,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:30,332 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:31,033 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:31,411 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:31,540 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:33,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:34,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:34,323 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:35,076 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:35,987 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:35,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:36,089 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:36,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:36,704 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:36,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:36,835 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:36,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:37,208 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:37,598 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:37,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:37,754 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:37,825 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:38,056 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:38,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:38,554 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:38,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:38,774 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:38,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:38,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:38,798 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:45:39,409 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:19,395 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:19,567 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:20,270 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:21,211 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:21,212 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:23,446 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:23,679 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:26,835 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:27,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:27,641 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:27,649 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:27,852 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:27,953 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:27,954 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:28,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:28,859 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:29,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:29,758 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:30,317 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:30,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:30,321 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:30,442 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:30,814 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:30,822 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:31,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:31,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:31,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:31,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:31,604 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:31,902 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:31,904 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:31,927 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:33,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:33,511 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:33,511 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:34,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:34,520 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:35,342 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:37,964 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:38,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:38,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:38,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:42,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 26.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:48,911 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:51,197 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:51,205 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:51,497 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:52,048 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:53,579 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:54,077 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:54,306 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:54,306 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:54,307 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:54,524 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:55,015 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:56,434 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:56,434 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:56,447 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:56,904 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:57,149 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:57,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:57,365 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:57,398 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:57,399 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:57,604 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:57,667 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:58,172 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:59,564 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:59,772 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:59,772 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:59,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:46:59,810 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:00,485 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:01,685 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:02,616 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:03,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:03,201 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:03,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:03,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:04,120 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:08,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:08,957 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:09,042 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:09,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:09,142 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:09,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:11,250 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:11,275 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:12,318 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:13,099 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:13,370 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:13,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:13,375 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:13,376 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:13,377 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:13,380 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:14,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:14,381 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:15,223 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:15,224 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:15,225 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:15,227 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:15,574 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:15,577 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:16,602 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:17,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:18,110 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:18,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:18,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:18,415 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:18,428 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:18,734 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:18,740 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:20,783 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:20,787 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:21,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:27,223 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:27,625 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:28,103 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:28,108 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:28,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:28,674 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:29,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:29,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:29,187 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:29,189 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:29,190 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:29,198 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:29,203 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:29,204 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:30,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:30,928 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:31,281 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:31,997 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:31,999 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:32,003 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:32,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:32,624 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:32,627 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:33,478 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:33,656 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:33,661 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:33,744 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:33,747 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:34,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:34,565 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:34,623 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:34,628 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:34,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:34,643 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:34,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:35,004 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:35,370 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:35,388 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:35,456 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:36,428 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:36,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:37,443 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:37,728 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:37,729 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:37,730 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:37,734 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:41,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:42,591 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:47,638 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:51,052 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:52,732 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:52,953 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:52,954 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:52,954 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:52,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:53,217 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:53,419 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:53,869 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:53,870 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:53,935 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:54,247 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:54,508 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:55,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:55,681 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:56,877 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:57,472 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:57,473 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:47:59,132 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:00,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:00,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:00,950 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:01,639 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:17,996 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:18,909 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:18,916 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:19,832 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:19,833 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:19,837 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:19,918 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:20,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:20,630 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:20,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:20,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:21,365 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:21,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:23,538 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:23,538 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:23,539 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:23,934 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:25,550 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:26,478 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:27,179 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:28,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:28,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:28,268 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:28,431 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:28,576 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:29,158 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:29,159 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:31,237 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:32,738 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:32,742 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:32,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:32,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:33,804 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:33,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:33,811 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:33,943 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:35,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:35,913 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:35,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:37,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:38,949 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:39,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:41,273 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:41,934 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:41,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:43,174 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:43,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:44,485 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:45,972 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:46,457 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:46,666 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:46,666 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:46,986 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:47,479 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:48,663 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:49,238 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:49,305 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:49,821 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:49,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:49,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:50,540 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:50,733 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:52,493 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:52,495 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:53,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:55,348 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:55,833 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:55,836 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:55,908 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:55,917 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:56,847 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:56,849 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:58,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:58,649 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:48:59,054 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:00,741 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:01,512 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:01,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:01,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 23.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:01,842 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:01,842 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:01,846 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:02,107 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:03,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:03,360 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:04,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:07,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:14,153 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:14,155 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:14,159 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:14,708 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:15,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:15,206 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:15,303 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:15,492 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:15,843 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:17,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:17,184 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:17,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:17,456 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:17,712 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:18,400 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:18,436 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:19,149 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:19,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:19,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:19,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:19,758 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:19,758 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:20,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:20,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:20,344 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:20,718 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:20,796 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:20,806 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:20,823 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:25,764 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:25,922 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:26,468 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:26,477 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:26,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:26,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:27,218 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:27,222 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:28,034 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:28,114 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:28,116 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:28,117 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:28,123 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:28,306 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:28,439 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:28,715 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:29,049 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:29,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:29,848 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:30,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:30,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:30,267 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:30,268 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:31,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:31,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:31,010 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:31,014 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:31,943 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:32,176 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:32,690 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:34,628 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:37,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:37,963 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:38,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:38,336 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:38,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:38,988 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:39,054 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:39,181 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:40,945 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:40,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:41,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:43,152 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:43,231 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:43,552 - distributed.core - INFO - Event loop was unresponsive in Worker for 21.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:43,602 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:43,885 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:44,214 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:46,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:46,712 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:46,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:49,126 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:49,562 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:50,182 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:50,220 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:50,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:50,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:52,082 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:52,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:52,089 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:52,367 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:52,767 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,767 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,767 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,767 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,768 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:40611. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,768 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:36017. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,768 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:41309. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,768 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:44691. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,767 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,767 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,767 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,767 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,767 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,767 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,767 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,768 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,767 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,767 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,769 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:40141. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,767 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,770 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:42739. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,770 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:37225. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,770 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:40485. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,770 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:46841. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,770 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:45039. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,770 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:38273. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,770 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:43319. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,768 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,770 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:36465. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,770 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:43797. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,768 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,768 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,771 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:33341. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,768 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,771 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:40271. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,771 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:37229. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,771 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:37101. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,771 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:42711. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,767 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,770 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,770 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,767 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.105.14:38948 remote=tcp://10.6.105.1:8749>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:49:52,770 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,769 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,770 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,772 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:46087. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,772 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:45783. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,772 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:37769. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,773 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:45539. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,773 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:42363. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,773 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:41409. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,771 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,773 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:52,773 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:45993. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,774 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:45223. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,780 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:38623'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,783 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.11:34929, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,783 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763541, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,783 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.17:43877, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,783 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763693, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,784 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,784 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,784 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,784 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,784 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,786 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:46839'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,786 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:42139'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,787 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:37843'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,787 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 766618, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,787 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764322, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,788 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.24:44241, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,788 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,788 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765711, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,788 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,788 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763571, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,788 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,788 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.14:41431, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,788 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,788 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,788 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,789 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.32:33059, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,789 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,789 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,789 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,789 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,789 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,789 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,789 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,789 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,790 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,795 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:34565'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,795 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:44029'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,796 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:44185'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,796 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:34047'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,796 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:44241'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,796 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763477, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,796 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 766228, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,797 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:44045'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,797 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763466, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,797 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 766218, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,797 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763103, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,797 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:40295'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,797 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763102, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,797 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:37037'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,797 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,798 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,798 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:36229'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,797 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.14:44307, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,798 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,798 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764734, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,798 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 768296, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,798 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,798 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:37313'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,798 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,798 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.14:36643, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,798 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764735, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,798 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,798 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763869, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,798 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,798 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,798 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:38293'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,798 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,798 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765562, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,798 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,798 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,798 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,799 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:36279'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,799 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765093, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,799 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764836, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,799 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764856, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,799 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:45715'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,799 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 768159, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,799 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763384, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,799 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763101, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,799 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:45655'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,799 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,800 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,800 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763100, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,800 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,800 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,800 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:46387'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,800 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,800 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,800 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 766128, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,800 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,800 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,800 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:34059'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,800 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765931, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,800 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,800 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,800 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,800 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 766130, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,800 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765909, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,800 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:36267'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,800 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,800 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.26:33411, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,801 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,801 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,801 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:44247'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,801 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764833, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,801 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,801 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,801 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764834, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,801 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:43935'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,801 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,801 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,801 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 766255, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,801 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,801 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764401, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,801 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:36901'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,801 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,801 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,801 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,801 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764419, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,801 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:46141'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,801 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,801 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764815, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,802 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,802 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,802 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,802 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:37557'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,802 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,802 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763105, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,802 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763457, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,802 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,802 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:39221'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:52,802 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,802 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763106, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,802 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,802 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 766077, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,802 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,802 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,802 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 766071, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,802 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,802 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 766263, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,803 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,803 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,803 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,803 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,803 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,803 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,803 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,803 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.14:40315, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,803 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,803 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,803 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,803 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,803 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,803 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,803 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,803 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,803 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.17:33909, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,803 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,803 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,803 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.24:44241, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,804 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,804 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,804 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,804 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,804 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,804 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,804 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763754, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,804 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,804 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,804 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,804 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,804 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763753, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,804 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 768091, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,804 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,804 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,804 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,804 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,804 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,804 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.14:36643, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,805 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,805 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,805 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 764965, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,805 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,805 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,805 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,805 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.14:40969, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:52,805 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,805 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,805 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,805 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,805 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,806 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:52,807 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:52,808 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:52,835 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:52,854 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:52,856 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:52,866 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:53,900 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:54,194 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:54,196 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:39163. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:54,212 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:54,218 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.14:38412 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:54,228 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:38745'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:54,229 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765745, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:54,229 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:54,229 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:54,229 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:54,229 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:54,229 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:54,239 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14ac9b2c5cd0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:54,256 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:54,258 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:41431. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:54,278 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:54,284 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.14:38312 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:54,288 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:38599'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:54,289 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:54,289 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:54,289 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:54,289 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:54,289 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:54,299 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14ba38430310>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:54,306 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:54,354 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:54,354 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:42809. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:54,359 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:54,362 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.14:38284 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:54,364 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:38207'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:54,365 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765814, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:54,365 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:54,365 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:54,365 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:54,365 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:54,365 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:54,369 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1466504d9d10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:54,686 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:54,688 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:32921. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:54,706 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:54,710 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.14:38356 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:54,715 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:36285'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:54,715 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:54,716 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:54,716 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:54,716 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:54,716 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:54,724 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x148981799110>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:54,732 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:54,839 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:54,857 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:54,860 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:54,872 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:55,485 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:36901'. Reason: nanny-close-gracefully
2025-09-05 09:49:55,488 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:36901' closed.
2025-09-05 09:49:55,491 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:37843'. Reason: nanny-close-gracefully
2025-09-05 09:49:55,492 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:37843' closed.
2025-09-05 09:49:55,495 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:55,497 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:34047'. Reason: nanny-close-gracefully
2025-09-05 09:49:55,498 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:46141'. Reason: nanny-close-gracefully
2025-09-05 09:49:55,501 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:34047' closed.
2025-09-05 09:49:55,501 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:46141' closed.
2025-09-05 09:49:55,905 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:56,130 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:56,309 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:56,390 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:43935'. Reason: nanny-close-gracefully
2025-09-05 09:49:56,391 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:43935' closed.
2025-09-05 09:49:56,735 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:56,757 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:56,782 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:38599'. Reason: nanny-close-gracefully
2025-09-05 09:49:56,783 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:38599' closed.
2025-09-05 09:49:56,895 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:56,899 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:37263. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:56,926 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:56,930 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.14:38596 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:56,934 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:46305'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:56,934 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765835, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:56,934 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:56,935 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:56,935 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:56,935 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:56,935 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:56,940 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x148718d4c090>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:56,989 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:57,197 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:57,200 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:37317. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:57,210 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:36285'. Reason: nanny-close-gracefully
2025-09-05 09:49:57,211 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:36285' closed.
2025-09-05 09:49:57,214 - distributed.worker - ERROR - failed during get data with tcp://10.6.105.14:37317 -> tcp://10.6.105.16:39749
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.105.14:37317 remote=tcp://10.6.105.16:36738>: Stream is closed
2025-09-05 09:49:57,215 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:57,217 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:36265. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:57,221 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:57,219 - distributed.worker - ERROR - failed during get data with tcp://10.6.105.14:36265 -> tcp://10.6.105.11:40919
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.105.14:36265 remote=tcp://10.6.105.11:53400>: Stream is closed
2025-09-05 09:49:57,225 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.14:38250 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:57,226 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.105.8:33267
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.105.14:54698 remote=tcp://10.6.105.8:33267>: Stream is closed
2025-09-05 09:49:57,227 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:57,228 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:44145'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:57,228 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:57,229 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:57,229 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:57,229 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:57,229 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:57,231 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.14:38672 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.105.14:38672 remote=tcp://10.6.105.1:8749>: Stream is closed
2025-09-05 09:49:57,234 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:35987'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:57,235 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.23:35535, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:57,235 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.4:33153, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:57,235 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.14:40315, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:57,235 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:57,236 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:57,236 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:57,236 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:57,236 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:57,235 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1551929868d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:57,240 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14c832476a10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:57,241 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:57,246 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:57,501 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:57,557 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:57,880 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:57,883 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:40315. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:57,898 - distributed.worker - ERROR - failed during get data with tcp://10.6.105.14:40315 -> tcp://10.6.105.14:41409
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.105.14:40315 remote=tcp://10.6.105.14:57350>: Stream is closed
2025-09-05 09:49:57,905 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:57,910 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.14:38444 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:57,920 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:34813'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:57,921 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:57,921 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:57,921 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:57,921 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:57,921 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:57,927 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x147532324890>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:57,934 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:57,950 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:57,949 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:57,951 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:32843. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:57,952 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:43751. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:57,953 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.105.30:33241
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.105.14:50008 remote=tcp://10.6.105.30:33241>: Stream is closed
2025-09-05 09:49:57,960 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:57,963 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.14:38476 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.105.14:38476 remote=tcp://10.6.105.1:8749>: Stream is closed
2025-09-05 09:49:57,966 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:34779'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:57,967 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:57,967 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:57,967 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:57,967 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:57,967 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:57,972 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x145e46cc20d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:57,972 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.105.27:38863
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.105.14:38682 remote=tcp://10.6.105.27:38863>: Stream is closed
2025-09-05 09:49:57,975 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:57,977 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:57,978 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:44247'. Reason: nanny-close-gracefully
2025-09-05 09:49:57,979 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:44247' closed.
2025-09-05 09:49:57,978 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.14:38338 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:57,981 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:37915'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:57,982 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.14:40969, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:57,982 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:57,982 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:57,982 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:57,982 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:57,982 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:57,987 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1502f1d4c0d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:57,992 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:58,133 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:58,229 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:58,231 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:39161. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:58,231 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:58,233 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:46331. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:58,234 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:58,236 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:38841. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:58,247 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:58,248 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:58,253 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:58,254 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:58,258 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:58,257 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.14:38506 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:58,257 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.14:38600 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:58,260 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:38761'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:58,261 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765923, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:58,262 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:58,262 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:58,262 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:46315'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:58,262 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:58,262 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:58,262 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:58,262 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 765662, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:58,263 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:58,263 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:58,263 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:58,263 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:58,263 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:58,261 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.14:38558 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:58,265 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:33703'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:58,265 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 766567, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:58,265 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:58,266 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:58,266 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:58,266 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:58,266 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:58,267 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x155285b80610>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:58,268 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x152903e1d4d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:58,270 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x153405a4fe50>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:58,291 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:58,293 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:33567. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:58,310 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.105.30:45175
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.105.14:57962 remote=tcp://10.6.105.30:45175>: Stream is closed
2025-09-05 09:49:58,317 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:58,321 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.14:38332 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:58,323 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:37417'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:58,324 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:58,324 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:58,324 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:58,324 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:58,324 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:58,330 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14e59c1b6e10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:58,334 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:58,566 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:38207'. Reason: nanny-close-gracefully
2025-09-05 09:49:58,567 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:38207' closed.
2025-09-05 09:49:58,761 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:58,993 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:59,036 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:59,037 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:41457. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:59,037 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:59,041 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:59,041 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:44307. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:59,041 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:59,043 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:36643. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:59,058 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:59,058 - distributed.worker - ERROR - failed during get data with tcp://10.6.105.14:36643 -> tcp://10.6.105.14:42739
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.105.14:36643 remote=tcp://10.6.105.14:54486>: Stream is closed
2025-09-05 09:49:59,061 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:59,061 - distributed.worker - ERROR - failed during get data with tcp://10.6.105.14:36643 -> tcp://10.6.105.14:45783
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.105.14:36643 remote=tcp://10.6.105.14:47812>: Stream is closed
2025-09-05 09:49:59,063 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.14:38176 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:59,066 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:40775'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:59,066 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.14:38538 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:59,067 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:59,067 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:59,067 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:59,067 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:59,067 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:59,068 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:34271'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:59,069 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 766559, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:59,069 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:59,069 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:59,069 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:59,069 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:59,069 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:59,070 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.14:38322 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:59,072 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:37059'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:59,073 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:59,073 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:59,073 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:59,073 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:59,073 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:59,072 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14aaefc1f890>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:59,073 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14ebdc0ffc10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:59,077 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:59,077 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14fcc26f1350>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:59,081 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:59,136 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:59,138 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:40969. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:59,154 - distributed.worker - ERROR - failed during get data with tcp://10.6.105.14:40969 -> tcp://10.6.105.14:32843
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.105.14:40969 remote=tcp://10.6.105.14:45342>: Stream is closed
2025-09-05 09:49:59,157 - distributed.worker - ERROR - failed during get data with tcp://10.6.105.14:40969 -> tcp://10.6.105.14:45783
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.105.14:40969 remote=tcp://10.6.105.14:45336>: Stream is closed
2025-09-05 09:49:59,162 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:59,166 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.14:38402 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:59,168 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:45293'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:59,169 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:59,169 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:59,169 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:59,169 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:59,169 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:59,174 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14c4183fb2d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:59,177 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:59,177 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:36645. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:59,179 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:59,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:59,188 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.14:38590 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:59,190 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:44299'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:59,190 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 766568, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:49:59,190 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:59,190 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:59,191 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:59,191 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:59,191 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:59,193 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x15141469e150>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:59,237 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:45655'. Reason: nanny-close-gracefully
2025-09-05 09:49:59,239 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:45655' closed.
2025-09-05 09:49:59,244 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:59,248 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:59,335 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:59,336 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:59,337 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:59,337 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:59,340 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:44543. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:59,360 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:59,365 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.14:38222 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:59,368 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:38063'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:59,369 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:59,369 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:59,369 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:59,369 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:59,369 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:59,373 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1462de50bc90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:59,379 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:59,411 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:49:59,411 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:41379. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:59,414 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:59,420 - distributed.worker - ERROR - failed during get data with tcp://10.6.105.14:41379 -> tcp://10.6.105.22:35095
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.105.14:41379 remote=tcp://10.6.105.22:58118>: Stream is closed
2025-09-05 09:49:59,422 - distributed.worker - ERROR - failed during get data with tcp://10.6.105.14:41379 -> tcp://10.6.105.6:34251
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.105.14:41379 remote=tcp://10.6.105.6:35378>: Stream is closed
2025-09-05 09:49:59,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:49:59,428 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.14:38522 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:49:59,430 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:40199'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:49:59,431 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:49:59,431 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:49:59,431 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:49:59,431 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:49:59,431 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:49:59,435 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x15442780a910>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:49:59,439 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:59,456 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:44241'. Reason: nanny-close-gracefully
2025-09-05 09:49:59,464 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:44241' closed.
2025-09-05 09:49:59,562 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:59,566 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:59,567 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:59,732 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:59,732 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:59,733 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:59,782 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:35987'. Reason: nanny-close-gracefully
2025-09-05 09:49:59,783 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:35987' closed.
2025-09-05 09:49:59,821 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:59,832 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:44145'. Reason: nanny-close-gracefully
2025-09-05 09:49:59,833 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:44145' closed.
2025-09-05 09:49:59,872 - distributed.nanny - INFO - Worker closed
2025-09-05 09:49:59,937 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:59,980 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:49:59,994 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:00,026 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:34565'. Reason: nanny-close-gracefully
2025-09-05 09:50:00,027 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:34565' closed.
2025-09-05 09:50:00,050 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,252 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:00,337 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:00,389 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,421 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:34813'. Reason: nanny-close-gracefully
2025-09-05 09:50:00,422 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:34813' closed.
2025-09-05 09:50:00,540 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:34779'. Reason: nanny-close-gracefully
2025-09-05 09:50:00,541 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:34779' closed.
2025-09-05 09:50:00,566 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,568 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:50:00,570 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:42185. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:50:00,588 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:50:00,589 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:35763. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:50:00,590 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:50:00,590 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.105.24:38593
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.105.14:47912 remote=tcp://10.6.105.24:38593>: Stream is closed
2025-09-05 09:50:00,593 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:40097. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:50:00,593 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:37915'. Reason: nanny-close-gracefully
2025-09-05 09:50:00,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:50:00,595 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:37915' closed.
2025-09-05 09:50:00,594 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.105.28:43977
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.105.14:38114 remote=tcp://10.6.105.28:43977>: Stream is closed
2025-09-05 09:50:00,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:50:00,598 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.14:38416 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:50:00,600 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.105.16:40371
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.105.14:51684 remote=tcp://10.6.105.16:40371>: Stream is closed
2025-09-05 09:50:00,602 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:33817'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:50:00,601 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.14:38634 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.105.14:38634 remote=tcp://10.6.105.1:8749>: Stream is closed
2025-09-05 09:50:00,602 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:50:00,603 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:50:00,603 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:50:00,603 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:50:00,603 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:50:00,603 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:50:00,604 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:40935'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:50:00,605 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.16:34617, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:50:00,605 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.2:40225, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:50:00,605 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.105.32:35351, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:50:00,605 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:50:00,605 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:50:00,605 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:50:00,605 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:50:00,605 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:50:00,606 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.14:38168 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 308, in write
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.6.105.14:38168 remote=tcp://10.6.105.1:8749>: Stream is closed
2025-09-05 09:50:00,605 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14b17e9a9fd0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:50:00,608 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14b178b02b90>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:50:00,611 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,613 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,615 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:39961'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:50:00,615 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('concatenate-getitem-open_dataset-tas-mean_chunk-1c3f3d05e3caff38f115635339059401', 763696, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:50:00,616 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:50:00,616 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:50:00,616 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:50:00,616 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:50:00,616 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:50:00,618 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14d381b82cd0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:50:00,708 - distributed.core - INFO - Connection to tcp://10.6.105.1:8749 has been closed.
2025-09-05 09:50:00,710 - distributed.worker - INFO - Stopping worker at tcp://10.6.105.14:39465. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:50:00,712 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:44185'. Reason: nanny-close-gracefully
2025-09-05 09:50:00,714 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:44185' closed.
2025-09-05 09:50:00,727 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:37313'. Reason: nanny-close-gracefully
2025-09-05 09:50:00,728 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:37313' closed.
2025-09-05 09:50:00,730 - distributed.worker - ERROR - Worker stream died during communication: tcp://10.6.105.17:44399
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2073, in gather_dep
    response = await get_data_from_worker(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 2879, in get_data_from_worker
    response = await send_recv(
               ^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://10.6.105.14:39300 remote=tcp://10.6.105.17:44399>: Stream is closed
2025-09-05 09:50:00,737 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:50:00,741 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.105.14:38378 remote=tcp://10.6.105.1:8749>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:50:00,743 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.105.14:37873'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:50:00,744 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:50:00,744 - distributed.worker - INFO - Removing Worker plugin qme_utils.py130ff37b-c346-49a5-a967-7c2c59f09424
2025-09-05 09:50:00,744 - distributed.worker - INFO - Removing Worker plugin qme_vars.py5bee52b1-a7c4-4bda-93e5-efbbdc4f8f76
2025-09-05 09:50:00,744 - distributed.worker - INFO - Removing Worker plugin qme_train.py7afc963c-292d-48e9-887f-e88c60b0ede7
2025-09-05 09:50:00,744 - distributed.worker - INFO - Removing Worker plugin qme_apply.py4ef0170c-f139-4c55-9a27-e4b48cb725a6
2025-09-05 09:50:00,746 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14d4ca9b9090>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:50:00,751 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,851 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:37417'. Reason: nanny-close-gracefully
2025-09-05 09:50:00,852 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:37417' closed.
2025-09-05 09:50:00,864 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,895 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,948 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,956 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:00,990 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,080 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:01,084 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:01,182 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:01,201 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,340 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:01,340 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:01,381 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:01,387 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,417 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:01,442 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:01,475 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,479 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,525 - distributed.nanny - INFO - Worker closed
2025-09-05 09:50:01,547 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:40775'. Reason: nanny-close-gracefully
2025-09-05 09:50:01,549 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:40775' closed.
2025-09-05 09:50:01,569 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:01,571 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:01,588 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:37059'. Reason: nanny-close-gracefully
2025-09-05 09:50:01,589 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:37059' closed.
2025-09-05 09:50:01,664 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:45293'. Reason: nanny-close-gracefully
2025-09-05 09:50:01,666 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:45293' closed.
2025-09-05 09:50:01,735 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:01,736 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:01,769 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:44299'. Reason: nanny-close-gracefully
2025-09-05 09:50:01,770 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:44299' closed.
2025-09-05 09:50:01,805 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:36279'. Reason: nanny-close-gracefully
2025-09-05 09:50:01,806 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:36279' closed.
2025-09-05 09:50:01,825 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:01,857 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:38063'. Reason: nanny-close-gracefully
2025-09-05 09:50:01,858 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:38063' closed.
2025-09-05 09:50:01,870 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:34271'. Reason: nanny-close-gracefully
2025-09-05 09:50:01,871 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:34271' closed.
2025-09-05 09:50:01,875 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:01,957 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:40199'. Reason: nanny-close-gracefully
2025-09-05 09:50:01,961 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:40199' closed.
2025-09-05 09:50:01,979 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:38293'. Reason: nanny-close-gracefully
2025-09-05 09:50:01,980 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:38293' closed.
2025-09-05 09:50:02,025 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:38623'. Reason: nanny-close-gracefully
2025-09-05 09:50:02,026 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:38623' closed.
2025-09-05 09:50:02,055 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,059 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:37557'. Reason: nanny-close-gracefully
2025-09-05 09:50:02,063 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:37557' closed.
2025-09-05 09:50:02,226 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:36267'. Reason: nanny-close-gracefully
2025-09-05 09:50:02,227 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:36267' closed.
2025-09-05 09:50:02,327 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:39221'. Reason: nanny-close-gracefully
2025-09-05 09:50:02,328 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:39221' closed.
2025-09-05 09:50:02,340 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:44045'. Reason: nanny-close-gracefully
2025-09-05 09:50:02,341 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:44045' closed.
2025-09-05 09:50:02,358 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:33703'. Reason: nanny-close-gracefully
2025-09-05 09:50:02,359 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:33703' closed.
2025-09-05 09:50:02,393 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,525 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:37037'. Reason: nanny-close-gracefully
2025-09-05 09:50:02,526 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:37037' closed.
2025-09-05 09:50:02,562 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:46315'. Reason: nanny-close-gracefully
2025-09-05 09:50:02,562 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:46315' closed.
2025-09-05 09:50:02,570 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,614 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,616 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,757 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,868 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,898 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:34059'. Reason: nanny-close-gracefully
2025-09-05 09:50:02,899 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,905 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:34059' closed.
2025-09-05 09:50:02,952 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,960 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:02,994 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:03,062 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:46305'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,063 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:46305' closed.
2025-09-05 09:50:03,093 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:33817'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,094 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:33817' closed.
2025-09-05 09:50:03,197 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:40935'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,198 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:40935' closed.
2025-09-05 09:50:03,205 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:03,333 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:37873'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,335 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:37873' closed.
2025-09-05 09:50:03,375 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:36229'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,376 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:36229' closed.
2025-09-05 09:50:03,385 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:38745'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,386 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:38745' closed.
2025-09-05 09:50:03,390 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:03,439 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:46387'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,440 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:46387' closed.
2025-09-05 09:50:03,447 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:39961'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,448 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:39961' closed.
2025-09-05 09:50:03,479 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:03,483 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:38761'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,483 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:03,483 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:38761' closed.
2025-09-05 09:50:03,529 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:50:03,708 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:44029'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,709 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:44029' closed.
2025-09-05 09:50:03,841 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:42139'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,842 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:42139' closed.
2025-09-05 09:50:03,948 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:40295'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,949 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:40295' closed.
2025-09-05 09:50:03,952 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:45715'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,953 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:45715' closed.
2025-09-05 09:50:03,979 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.105.14:46839'. Reason: nanny-close-gracefully
2025-09-05 09:50:03,980 - distributed.nanny - INFO - Nanny at 'tcp://10.6.105.14:46839' closed.
2025-09-05 09:50:03,983 - distributed.dask_worker - INFO - End worker
