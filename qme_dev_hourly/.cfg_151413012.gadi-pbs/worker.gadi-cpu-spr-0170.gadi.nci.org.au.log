Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-10-02 09:24:42,640 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:40377'
2025-10-02 09:24:42,649 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:45949'
2025-10-02 09:24:42,652 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:39137'
2025-10-02 09:24:42,656 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:42057'
2025-10-02 09:24:42,660 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:45585'
2025-10-02 09:24:42,664 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:41581'
2025-10-02 09:24:42,669 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:37845'
2025-10-02 09:24:42,672 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:38745'
2025-10-02 09:24:42,676 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:36119'
2025-10-02 09:24:42,681 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:39721'
2025-10-02 09:24:42,685 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:36179'
2025-10-02 09:24:42,689 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:41879'
2025-10-02 09:24:42,692 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:40081'
2025-10-02 09:24:42,696 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:34963'
2025-10-02 09:24:42,701 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:33713'
2025-10-02 09:24:42,705 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:45421'
2025-10-02 09:24:42,709 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:46585'
2025-10-02 09:24:42,713 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:44803'
2025-10-02 09:24:42,717 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:45123'
2025-10-02 09:24:42,722 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:43977'
2025-10-02 09:24:42,854 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:37147'
2025-10-02 09:24:42,859 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:41501'
2025-10-02 09:24:42,864 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:33055'
2025-10-02 09:24:42,869 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:42981'
2025-10-02 09:24:42,873 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:38241'
2025-10-02 09:24:42,878 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:46265'
2025-10-02 09:24:42,883 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:37267'
2025-10-02 09:24:42,888 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:34471'
2025-10-02 09:24:42,893 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:39211'
2025-10-02 09:24:42,898 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:36093'
2025-10-02 09:24:42,902 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:46239'
2025-10-02 09:24:42,907 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:37563'
2025-10-02 09:24:42,912 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:43817'
2025-10-02 09:24:42,916 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:33485'
2025-10-02 09:24:42,921 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:40975'
2025-10-02 09:24:42,926 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:44815'
2025-10-02 09:24:42,930 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:41069'
2025-10-02 09:24:42,936 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:44965'
2025-10-02 09:24:42,940 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:42423'
2025-10-02 09:24:42,945 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:39601'
2025-10-02 09:24:42,949 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:43573'
2025-10-02 09:24:42,951 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:36403'
2025-10-02 09:24:42,954 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:33743'
2025-10-02 09:24:42,957 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:42967'
2025-10-02 09:24:42,962 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:43735'
2025-10-02 09:24:42,967 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:46093'
2025-10-02 09:24:42,970 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:37393'
2025-10-02 09:24:42,976 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:33215'
2025-10-02 09:24:42,979 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:36977'
2025-10-02 09:24:42,987 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:33633'
2025-10-02 09:24:42,993 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:42151'
2025-10-02 09:24:42,996 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.26:35871'
2025-10-02 09:24:43,994 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:38611
2025-10-02 09:24:43,994 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:45987
2025-10-02 09:24:43,994 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:34599
2025-10-02 09:24:43,994 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:43831
2025-10-02 09:24:43,994 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:42069
2025-10-02 09:24:43,994 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:35655
2025-10-02 09:24:43,994 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:44831
2025-10-02 09:24:43,994 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:43247
2025-10-02 09:24:43,994 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:38229
2025-10-02 09:24:43,994 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:41715
2025-10-02 09:24:43,994 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:45907
2025-10-02 09:24:43,994 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:36383
2025-10-02 09:24:43,994 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:38611
2025-10-02 09:24:43,994 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:34065
2025-10-02 09:24:43,994 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:43445
2025-10-02 09:24:43,994 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:36763
2025-10-02 09:24:43,994 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:45987
2025-10-02 09:24:43,994 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:34599
2025-10-02 09:24:43,994 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:43831
2025-10-02 09:24:43,994 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:42069
2025-10-02 09:24:43,994 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:35655
2025-10-02 09:24:43,994 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:44831
2025-10-02 09:24:43,994 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:43247
2025-10-02 09:24:43,994 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:38229
2025-10-02 09:24:43,994 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:41715
2025-10-02 09:24:43,994 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:45907
2025-10-02 09:24:43,994 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:36383
2025-10-02 09:24:43,994 - distributed.worker - INFO -          dashboard at:           10.6.83.26:35227
2025-10-02 09:24:43,995 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:34065
2025-10-02 09:24:43,995 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:43445
2025-10-02 09:24:43,995 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:36763
2025-10-02 09:24:43,995 - distributed.worker - INFO -          dashboard at:           10.6.83.26:32961
2025-10-02 09:24:43,995 - distributed.worker - INFO -          dashboard at:           10.6.83.26:38283
2025-10-02 09:24:43,995 - distributed.worker - INFO -          dashboard at:           10.6.83.26:40775
2025-10-02 09:24:43,995 - distributed.worker - INFO -          dashboard at:           10.6.83.26:35001
2025-10-02 09:24:43,995 - distributed.worker - INFO -          dashboard at:           10.6.83.26:44093
2025-10-02 09:24:43,995 - distributed.worker - INFO -          dashboard at:           10.6.83.26:43187
2025-10-02 09:24:43,995 - distributed.worker - INFO -          dashboard at:           10.6.83.26:44363
2025-10-02 09:24:43,995 - distributed.worker - INFO -          dashboard at:           10.6.83.26:34651
2025-10-02 09:24:43,995 - distributed.worker - INFO -          dashboard at:           10.6.83.26:41561
2025-10-02 09:24:43,995 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:43,995 - distributed.worker - INFO -          dashboard at:           10.6.83.26:40947
2025-10-02 09:24:43,995 - distributed.worker - INFO -          dashboard at:           10.6.83.26:39693
2025-10-02 09:24:43,995 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:43,995 - distributed.worker - INFO -          dashboard at:           10.6.83.26:36793
2025-10-02 09:24:43,995 - distributed.worker - INFO -          dashboard at:           10.6.83.26:34593
2025-10-02 09:24:43,995 - distributed.worker - INFO -          dashboard at:           10.6.83.26:40007
2025-10-02 09:24:43,995 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:43,995 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:43,995 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:43,995 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:43,995 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:43,995 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:43,995 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:43,995 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:43,995 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:43,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,995 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:43,995 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:43,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,995 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:43,995 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:43,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,995 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:43,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,995 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:43,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,995 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:43,995 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:43,995 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:43,995 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:43,995 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:43,995 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:43,995 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:43,995 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:43,995 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:43,995 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:43,995 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:43,995 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:43,995 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:43,995 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:43,995 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:43,995 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:43,995 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:43,995 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:43,995 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:43,995 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:43,995 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:43,995 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:43,995 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:43,995 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:43,995 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:43,995 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:43,995 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-w9qgg6ai
2025-10-02 09:24:43,995 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:43,995 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-l11wkf7t
2025-10-02 09:24:43,995 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:43,995 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-8alo_4u2
2025-10-02 09:24:43,995 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-ljd5bt8i
2025-10-02 09:24:43,995 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-ea2iwj4d
2025-10-02 09:24:43,995 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-ukw_7uzy
2025-10-02 09:24:43,995 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-azz5bihp
2025-10-02 09:24:43,995 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-uwu2a1vc
2025-10-02 09:24:43,995 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-3qa3ip4_
2025-10-02 09:24:43,995 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-4tdeab0b
2025-10-02 09:24:43,995 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-9xutai__
2025-10-02 09:24:43,995 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-4aoaba2m
2025-10-02 09:24:43,995 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-pvatvg52
2025-10-02 09:24:43,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,995 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-g7cbnnar
2025-10-02 09:24:43,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,995 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-2yqw875_
2025-10-02 09:24:43,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,995 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,995 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:33487
2025-10-02 09:24:43,996 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:33487
2025-10-02 09:24:43,996 - distributed.worker - INFO -          dashboard at:           10.6.83.26:42593
2025-10-02 09:24:43,996 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:43,996 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,996 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:43,996 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:43,996 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-ou7jsyey
2025-10-02 09:24:43,996 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,998 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:38587
2025-10-02 09:24:43,999 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:38587
2025-10-02 09:24:43,999 - distributed.worker - INFO -          dashboard at:           10.6.83.26:33477
2025-10-02 09:24:43,999 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:43,999 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:43,999 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:43,999 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:43,999 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-zu0j17u8
2025-10-02 09:24:43,999 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,001 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:43051
2025-10-02 09:24:44,002 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:43051
2025-10-02 09:24:44,002 - distributed.worker - INFO -          dashboard at:           10.6.83.26:45129
2025-10-02 09:24:44,002 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,002 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,002 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,002 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,002 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-kta6__ig
2025-10-02 09:24:44,002 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,004 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:32857
2025-10-02 09:24:44,004 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:32857
2025-10-02 09:24:44,004 - distributed.worker - INFO -          dashboard at:           10.6.83.26:33817
2025-10-02 09:24:44,004 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,004 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,004 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,004 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,004 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-mcw4y99u
2025-10-02 09:24:44,004 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,023 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,023 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,023 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,024 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,030 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,031 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,031 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,032 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,038 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,039 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,039 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,041 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,042 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,043 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,043 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,045 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,046 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,047 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,047 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,049 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,050 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,051 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,051 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,053 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,053 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,055 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,055 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,056 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,056 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,057 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,057 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,059 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,059 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,060 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,060 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,062 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,062 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,063 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,063 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,064 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,064 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,065 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,066 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,067 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,067 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,068 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,068 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,069 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,070 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,070 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,070 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,071 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,072 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,072 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,072 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,073 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,074 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,075 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,075 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,076 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,076 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,077 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,077 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,078 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:38365
2025-10-02 09:24:44,078 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:38365
2025-10-02 09:24:44,078 - distributed.worker - INFO -          dashboard at:           10.6.83.26:36623
2025-10-02 09:24:44,078 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,078 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,078 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,078 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,078 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-am3zb_bd
2025-10-02 09:24:44,078 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,079 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,079 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,080 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,081 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,081 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,082 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,083 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,083 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,084 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,084 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,085 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,085 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,087 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,107 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,130 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:35155
2025-10-02 09:24:44,135 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:46411
2025-10-02 09:24:44,139 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:35155
2025-10-02 09:24:44,139 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:46411
2025-10-02 09:24:44,139 - distributed.worker - INFO -          dashboard at:           10.6.83.26:36343
2025-10-02 09:24:44,139 - distributed.worker - INFO -          dashboard at:           10.6.83.26:35831
2025-10-02 09:24:44,139 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,139 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,139 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,139 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,139 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,139 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,139 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,140 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,140 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-duhmb0fh
2025-10-02 09:24:44,140 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-vw16w3jf
2025-10-02 09:24:44,140 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,140 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,140 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,140 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,142 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,164 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,165 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,165 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,167 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,173 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,174 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,174 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,176 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,219 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:33847
2025-10-02 09:24:44,219 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:33847
2025-10-02 09:24:44,219 - distributed.worker - INFO -          dashboard at:           10.6.83.26:44441
2025-10-02 09:24:44,219 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,219 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,219 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,219 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,220 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-48ftzxdm
2025-10-02 09:24:44,220 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,233 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:35903
2025-10-02 09:24:44,233 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:35903
2025-10-02 09:24:44,233 - distributed.worker - INFO -          dashboard at:           10.6.83.26:34173
2025-10-02 09:24:44,233 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,233 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,233 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,233 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,233 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-9r95acs3
2025-10-02 09:24:44,233 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,246 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,247 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,247 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,249 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,258 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,259 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,259 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,260 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,292 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:46081
2025-10-02 09:24:44,292 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:46081
2025-10-02 09:24:44,292 - distributed.worker - INFO -          dashboard at:           10.6.83.26:36045
2025-10-02 09:24:44,292 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,292 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,292 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,292 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,292 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-nimlnxvu
2025-10-02 09:24:44,292 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,293 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:38853
2025-10-02 09:24:44,293 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:38853
2025-10-02 09:24:44,293 - distributed.worker - INFO -          dashboard at:           10.6.83.26:42577
2025-10-02 09:24:44,293 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,293 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,293 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,293 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,293 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-yy8hsyxz
2025-10-02 09:24:44,293 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,311 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:39943
2025-10-02 09:24:44,311 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:39943
2025-10-02 09:24:44,311 - distributed.worker - INFO -          dashboard at:           10.6.83.26:44267
2025-10-02 09:24:44,311 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,311 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,311 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,311 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,311 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-bcfzvm19
2025-10-02 09:24:44,311 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,313 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:39495
2025-10-02 09:24:44,313 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:39495
2025-10-02 09:24:44,313 - distributed.worker - INFO -          dashboard at:           10.6.83.26:36509
2025-10-02 09:24:44,313 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,313 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,313 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,313 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,313 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-ybd_boog
2025-10-02 09:24:44,313 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,314 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:40447
2025-10-02 09:24:44,314 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:40447
2025-10-02 09:24:44,314 - distributed.worker - INFO -          dashboard at:           10.6.83.26:40607
2025-10-02 09:24:44,314 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,314 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,314 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,314 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,314 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-u2vn6olq
2025-10-02 09:24:44,314 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,318 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,319 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,320 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,320 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:43773
2025-10-02 09:24:44,320 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:43773
2025-10-02 09:24:44,320 - distributed.worker - INFO -          dashboard at:           10.6.83.26:40651
2025-10-02 09:24:44,320 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,320 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,320 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,320 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,320 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-lvvxo7e3
2025-10-02 09:24:44,320 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,321 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,326 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,327 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,328 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,329 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,352 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,353 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,353 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,353 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:43205
2025-10-02 09:24:44,354 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:43205
2025-10-02 09:24:44,354 - distributed.worker - INFO -          dashboard at:           10.6.83.26:34109
2025-10-02 09:24:44,354 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,354 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,354 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,354 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,354 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-kdiatlpm
2025-10-02 09:24:44,354 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,355 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,359 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,360 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,360 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,362 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,365 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,366 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,366 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,368 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,369 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,370 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,370 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,372 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,390 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,391 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,391 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,392 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:34115
2025-10-02 09:24:44,392 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:34115
2025-10-02 09:24:44,392 - distributed.worker - INFO -          dashboard at:           10.6.83.26:39757
2025-10-02 09:24:44,392 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,392 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,392 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,392 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,392 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-i0su8scl
2025-10-02 09:24:44,392 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,392 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,394 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:41013
2025-10-02 09:24:44,394 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:41013
2025-10-02 09:24:44,394 - distributed.worker - INFO -          dashboard at:           10.6.83.26:35093
2025-10-02 09:24:44,395 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,395 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,395 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,395 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,395 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-3fopdzzg
2025-10-02 09:24:44,395 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,397 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:44737
2025-10-02 09:24:44,397 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:44737
2025-10-02 09:24:44,397 - distributed.worker - INFO -          dashboard at:           10.6.83.26:36801
2025-10-02 09:24:44,397 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,397 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,397 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,397 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,397 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-q5ds857h
2025-10-02 09:24:44,397 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,401 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:37485
2025-10-02 09:24:44,401 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:37485
2025-10-02 09:24:44,401 - distributed.worker - INFO -          dashboard at:           10.6.83.26:36883
2025-10-02 09:24:44,401 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,401 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,401 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,401 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,401 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-wuiwl98o
2025-10-02 09:24:44,401 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,405 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:39833
2025-10-02 09:24:44,406 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:39833
2025-10-02 09:24:44,406 - distributed.worker - INFO -          dashboard at:           10.6.83.26:42737
2025-10-02 09:24:44,406 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,406 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,406 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,406 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,406 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-h6ns41_o
2025-10-02 09:24:44,406 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,409 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:38341
2025-10-02 09:24:44,409 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:38341
2025-10-02 09:24:44,409 - distributed.worker - INFO -          dashboard at:           10.6.83.26:40709
2025-10-02 09:24:44,409 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,409 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,409 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,409 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,409 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-34lb_fqy
2025-10-02 09:24:44,409 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,410 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:38691
2025-10-02 09:24:44,410 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:38691
2025-10-02 09:24:44,410 - distributed.worker - INFO -          dashboard at:           10.6.83.26:37579
2025-10-02 09:24:44,410 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,410 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,410 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,410 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,410 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-w3m8tmj4
2025-10-02 09:24:44,410 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,413 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:40897
2025-10-02 09:24:44,413 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:40897
2025-10-02 09:24:44,413 - distributed.worker - INFO -          dashboard at:           10.6.83.26:37295
2025-10-02 09:24:44,413 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,413 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,413 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,413 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,413 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-xlb4hcjy
2025-10-02 09:24:44,413 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,414 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:40087
2025-10-02 09:24:44,414 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:40087
2025-10-02 09:24:44,414 - distributed.worker - INFO -          dashboard at:           10.6.83.26:32871
2025-10-02 09:24:44,414 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,414 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,414 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,414 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,414 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-sj2bn80q
2025-10-02 09:24:44,414 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,415 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:37013
2025-10-02 09:24:44,415 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:45937
2025-10-02 09:24:44,415 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:37013
2025-10-02 09:24:44,415 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:45937
2025-10-02 09:24:44,415 - distributed.worker - INFO -          dashboard at:           10.6.83.26:43689
2025-10-02 09:24:44,415 - distributed.worker - INFO -          dashboard at:           10.6.83.26:33371
2025-10-02 09:24:44,415 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,415 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,415 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,415 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,415 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,415 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,415 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,415 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,415 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-ikuj8s4a
2025-10-02 09:24:44,415 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-fn4nag1_
2025-10-02 09:24:44,415 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,415 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,416 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,417 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,417 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,417 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:39051
2025-10-02 09:24:44,417 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:39051
2025-10-02 09:24:44,417 - distributed.worker - INFO -          dashboard at:           10.6.83.26:43253
2025-10-02 09:24:44,417 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:35361
2025-10-02 09:24:44,417 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,417 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,417 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:35361
2025-10-02 09:24:44,417 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,417 - distributed.worker - INFO -          dashboard at:           10.6.83.26:43629
2025-10-02 09:24:44,417 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,417 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,417 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-ptv5wg_w
2025-10-02 09:24:44,417 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,417 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,417 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,417 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,417 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-gpx5xjf0
2025-10-02 09:24:44,417 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,418 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,423 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,424 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,424 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,426 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,428 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,429 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,429 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,430 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,431 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:45939
2025-10-02 09:24:44,431 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:45939
2025-10-02 09:24:44,431 - distributed.worker - INFO -          dashboard at:           10.6.83.26:33171
2025-10-02 09:24:44,431 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,431 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,431 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,431 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,431 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-y7z238kw
2025-10-02 09:24:44,431 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,433 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:41637
2025-10-02 09:24:44,433 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:41637
2025-10-02 09:24:44,433 - distributed.worker - INFO -          dashboard at:           10.6.83.26:37665
2025-10-02 09:24:44,433 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,433 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,433 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,433 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,433 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-6ho03fdb
2025-10-02 09:24:44,433 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,433 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:33293
2025-10-02 09:24:44,433 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:33293
2025-10-02 09:24:44,433 - distributed.worker - INFO -          dashboard at:           10.6.83.26:36555
2025-10-02 09:24:44,433 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,433 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,433 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,433 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,433 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-1hzxkbye
2025-10-02 09:24:44,434 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,435 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,436 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,436 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,437 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:41223
2025-10-02 09:24:44,437 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:41223
2025-10-02 09:24:44,437 - distributed.worker - INFO -          dashboard at:           10.6.83.26:44341
2025-10-02 09:24:44,437 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,437 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,437 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,437 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,437 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-klo7kfyd
2025-10-02 09:24:44,437 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,437 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,439 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:42859
2025-10-02 09:24:44,439 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:44789
2025-10-02 09:24:44,439 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:42859
2025-10-02 09:24:44,439 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:44789
2025-10-02 09:24:44,439 - distributed.worker - INFO -          dashboard at:           10.6.83.26:45007
2025-10-02 09:24:44,439 - distributed.worker - INFO -          dashboard at:           10.6.83.26:42149
2025-10-02 09:24:44,439 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,439 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,439 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,439 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,439 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,439 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,439 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,439 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,439 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-iurv8yzj
2025-10-02 09:24:44,439 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,439 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,439 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-_h23d5e0
2025-10-02 09:24:44,439 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,440 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,440 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,440 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:38817
2025-10-02 09:24:44,440 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:38817
2025-10-02 09:24:44,440 - distributed.worker - INFO -          dashboard at:           10.6.83.26:44273
2025-10-02 09:24:44,440 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,440 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,440 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,440 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,440 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-0_n7dcxn
2025-10-02 09:24:44,440 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,441 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.26:44451
2025-10-02 09:24:44,441 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.26:44451
2025-10-02 09:24:44,441 - distributed.worker - INFO -          dashboard at:           10.6.83.26:40491
2025-10-02 09:24:44,441 - distributed.worker - INFO - Waiting to connect to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,441 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,441 - distributed.worker - INFO -               Threads:                          2
2025-10-02 09:24:44,441 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-10-02 09:24:44,441 - distributed.worker - INFO -       Local Directory: /jobfs/151413012.gadi-pbs/dask-scratch-space/worker-r9y3wck3
2025-10-02 09:24:44,441 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,441 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,443 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,444 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,445 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,446 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,446 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,448 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,448 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,449 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,449 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,450 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,450 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,452 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,452 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,453 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,453 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,454 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,454 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,454 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,454 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,455 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,458 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,459 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,459 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,460 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,461 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,461 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,462 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,463 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,463 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,464 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,464 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,465 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,466 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,466 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,466 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,467 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,468 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,468 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,468 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,469 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,470 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,471 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,471 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,472 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,473 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,474 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,474 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,474 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,475 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,476 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,476 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,477 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,477 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,478 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,479 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,480 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,480 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,481 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,481 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,482 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-02 09:24:44,483 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:44,483 - distributed.worker - INFO -         Registered to:       tcp://10.6.83.1:8728
2025-10-02 09:24:44,484 - distributed.worker - INFO - -------------------------------------------------
2025-10-02 09:24:44,485 - distributed.core - INFO - Starting established connection to tcp://10.6.83.1:8728
2025-10-02 09:24:56,359 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,359 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,359 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,359 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,359 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,360 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,360 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,360 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,360 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,360 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,360 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,360 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,361 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,361 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,361 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,361 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,361 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,361 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,361 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,362 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,362 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,362 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,362 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,362 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,362 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,362 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,362 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,362 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,363 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,363 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,363 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,363 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,363 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,363 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,363 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,363 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,363 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,363 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,363 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,363 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,363 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,363 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,364 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,364 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,364 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,364 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,364 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,364 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,364 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,364 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,364 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,364 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,364 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,364 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,364 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,364 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,364 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,364 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,364 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,364 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,364 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,365 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,365 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,365 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,365 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,365 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,365 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,365 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,365 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,365 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,365 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,365 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,365 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,365 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,365 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,364 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,366 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,366 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,364 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,366 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,366 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,366 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,366 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,367 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,367 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,367 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,367 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,367 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,368 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,366 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,368 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,368 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,366 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,368 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,367 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,367 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,368 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 09:24:56,373 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,373 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,375 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,375 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,376 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,376 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:56,376 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-10-02 09:24:59,115 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,115 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,115 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,115 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,116 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,116 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,116 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,116 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,116 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,116 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,116 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,116 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,116 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,117 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,117 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,117 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,117 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,117 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,118 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,118 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,117 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,118 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,118 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,117 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,118 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,118 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,118 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,118 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,118 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,118 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,118 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,118 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,118 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,118 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,119 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,119 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,119 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,119 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,118 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,119 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,118 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,119 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,119 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,119 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,119 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,119 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,119 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,119 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,119 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,119 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,119 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,119 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,119 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,119 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,119 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,120 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,120 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,120 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,120 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,120 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,120 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,120 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,120 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,120 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,120 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,121 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,120 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,121 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,120 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,120 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,121 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,120 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,121 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,121 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,121 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,121 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,121 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,121 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,121 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,121 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,122 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,122 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,122 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,122 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,121 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,122 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,122 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,122 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,120 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,122 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,123 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,123 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,123 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,123 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,123 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,123 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,123 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,123 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,124 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,124 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,123 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,125 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,122 - distributed.worker - INFO - Starting Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 09:24:59,130 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-10-02 09:24:59,519 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,520 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,520 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,520 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,520 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,520 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,520 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,520 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,521 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,521 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,521 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,521 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,521 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,521 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,521 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,522 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,522 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,522 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,522 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,522 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,522 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,522 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,522 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,523 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,523 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,523 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,523 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,523 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,523 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,523 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,523 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,524 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,524 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,524 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,524 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,524 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,524 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,524 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,524 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,524 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,525 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,525 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,525 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,525 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,525 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,525 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,525 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,525 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,525 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,525 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,525 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,525 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,525 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,525 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,525 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,525 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,525 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,526 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,526 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,526 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,526 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,526 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,526 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,526 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,526 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,526 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,527 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,527 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,527 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,527 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,527 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,527 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,527 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,527 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,527 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,527 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,527 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,527 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,528 - distributed.worker - INFO - Starting Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 09:24:59,528 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,528 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,528 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,528 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,528 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,528 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,528 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,529 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,529 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,529 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,530 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,530 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,530 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,530 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,530 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,530 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,530 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,530 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,530 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,530 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,530 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,530 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,531 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,531 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,531 - distributed.utils - INFO - Reload module qme_train from .py file
2025-10-02 09:24:59,950 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,950 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,950 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,951 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,951 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,951 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,951 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,951 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,951 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,951 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,951 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,951 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,952 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,952 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,952 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,952 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,952 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,952 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,952 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,952 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,953 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,953 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,953 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,953 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,953 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,953 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,953 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,953 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,953 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,953 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,953 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,953 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,953 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,953 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,953 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,953 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,953 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,953 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,954 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,954 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,954 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,953 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,954 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,954 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,954 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,954 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,954 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,954 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,954 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,954 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,955 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,954 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,955 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,955 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,955 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,955 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,955 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,955 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,955 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,956 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,955 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,955 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,955 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,956 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,956 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,956 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,956 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,956 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,956 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,956 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,956 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,956 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,956 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,956 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,956 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,956 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,956 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,956 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,957 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,957 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,957 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,957 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,957 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,957 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,958 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,958 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,958 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,958 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,958 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,958 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,958 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,958 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,959 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,959 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,959 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,959 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,959 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,959 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,959 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,959 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,958 - distributed.worker - INFO - Starting Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 09:24:59,960 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,961 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:24:59,965 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-10-02 09:25:23,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,594 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,596 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,597 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,602 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,605 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,607 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,617 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,618 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,621 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,628 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,633 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,633 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,636 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,637 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,641 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,645 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,650 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,655 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,656 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,658 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,658 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,659 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 09:25:23,660 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 13:54:36,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 14:08:31,370 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 14:57:45,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 15:11:47,968 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 15:18:46,062 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 15:25:54,903 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 15:33:03,054 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 15:40:03,529 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:01:13,140 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:08:16,571 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:15:22,585 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:43:44,676 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 16:57:57,744 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 17:05:12,334 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 17:40:38,252 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 17:47:42,451 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 18:09:06,895 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 18:16:16,407 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 18:23:25,335 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 18:37:38,170 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 19:06:03,648 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-02 19:25:43,466 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,467 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,467 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,467 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,468 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,467 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,468 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,467 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,468 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,468 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,468 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:32857. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,468 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:33487. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,468 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:38611. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,468 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:42069. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,466 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,468 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,469 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:33293. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,469 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:43247. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,466 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,469 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:37485. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,466 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,469 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:36763. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,469 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:38853. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,469 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:35155. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,469 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:43445. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,469 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:44451. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,469 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:36383. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,470 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:43051. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,468 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,467 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,470 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:35903. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,468 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,468 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,468 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,471 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:41223. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,468 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,471 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:40087. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,471 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:43205. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,471 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:44737. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,468 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,472 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:40447. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,467 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,472 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:43773. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,472 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:45987. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,468 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,475 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:41013. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,472 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.26:53926 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,471 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.26:53960 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,471 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.26:53972 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,471 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.26:53936 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,471 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.26:53914 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,472 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.26:53874 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,471 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.26:53994 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,472 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.26:53946 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,472 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.26:53986 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,475 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.26:53882 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,481 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,489 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:44789. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,490 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:39137'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,487 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,492 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,492 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,493 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,493 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,493 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,492 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,498 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:45907. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,499 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:40081'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,500 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:33633'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,500 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:40377'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,500 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,500 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:41715. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,500 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:37267'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,501 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:36179'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,501 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,501 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,501 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:41069'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,501 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,501 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,501 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,501 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,501 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:45123'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,501 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,501 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,501 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:36119'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,501 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,502 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,502 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,502 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,502 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:43573'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,502 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,502 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,502 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,502 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,502 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,502 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,502 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:41501'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,502 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,502 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,502 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,502 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,502 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,502 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,502 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:36093'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,502 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,502 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,502 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,502 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,503 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:37147'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,503 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,503 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,503 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,503 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,503 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,503 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:33215'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,503 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,503 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,503 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,503 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,503 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,503 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,503 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:35871'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,503 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,503 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,503 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,503 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,503 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,503 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,503 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:44965'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,503 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,503 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,503 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,503 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,503 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,504 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,504 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,504 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,504 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,504 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,504 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,504 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,504 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,504 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,504 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,504 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,504 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,504 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,504 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,504 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,504 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,502 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,505 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,505 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,505 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,505 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,505 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,505 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,505 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,505 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,505 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,507 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,507 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,508 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,508 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,509 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,509 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,509 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,510 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:44831. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,511 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,509 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,513 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,515 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:37563'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,515 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:43735'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,515 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:37845'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,516 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:40975'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,516 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:39721'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,516 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,516 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:34963'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,516 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,516 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:45585'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,516 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,516 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,516 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,517 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,517 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,517 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,517 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,517 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,517 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,517 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,517 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,517 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,517 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,517 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,517 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:45939. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,517 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,517 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,517 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,517 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,517 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,518 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,518 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,518 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,518 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,518 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,518 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,518 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,518 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,516 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,518 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,519 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,519 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,519 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,520 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,520 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,520 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,520 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,520 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,520 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,521 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,519 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,523 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,523 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,523 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,523 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,524 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:43831. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,523 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,526 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,528 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:35361. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,521 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,526 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,532 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:42859. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,523 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,534 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:45937. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,535 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:41637. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,536 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,538 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,539 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:46411. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,541 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,550 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:38817. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,550 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,558 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:38229. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,556 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,562 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:43,565 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:38365. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,543 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.26:53888 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,599 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,608 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:34065. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,632 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,635 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:39943. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,671 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,680 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:39495. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,705 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,714 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:46081. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,484 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.26:54010 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,734 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,743 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:37013. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,505 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.26:53902 remote=tcp://10.6.83.1:8728>: Stream is closed
2025-10-02 19:25:43,757 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,500 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.26:53916 remote=tcp://10.6.83.1:8728>: Stream is closed
2025-10-02 19:25:43,760 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:34599. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,760 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,768 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:39051. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,782 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:36977'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,786 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,805 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:42151'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,805 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:35655. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,810 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:33743'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,822 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 15, 15, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:43,828 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,833 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,836 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 7, 17, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:43,839 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,839 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 6, 0, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:43,841 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,844 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,845 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,849 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,849 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:38691. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,851 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,856 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,543 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.26:53876 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,861 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,863 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,867 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,551 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.26:53872 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,868 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,874 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,877 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,879 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,878 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,884 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:43,884 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:38587. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,886 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:33847. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,889 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,895 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:34115. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,547 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.26:53862 remote=tcp://10.6.83.1:8728>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-10-02 19:25:43,908 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:46585'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,913 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:42423'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,920 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:36403'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,936 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 0, 8, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:43,954 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,954 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:43,962 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:43,962 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:40897. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,962 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:39833. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:43,968 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:43,973 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:43,978 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:43,983 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,000 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:37393'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,007 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 9, 12, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,005 - distributed.core - INFO - Connection to tcp://10.6.83.1:8728 has been closed.
2025-10-02 19:25:44,008 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:44815'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,011 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:46093'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,013 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.26:38341. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,013 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:46239'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,025 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:39601'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,143 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:43977'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,175 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:43817'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,205 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:33055'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,209 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:41879'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,249 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:33485'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,260 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 1, 16, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,264 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 2, 10, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,269 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 12, 18, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,281 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 11, 2, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,299 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 10, 3, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,301 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:38745'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,306 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 12, 4, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,313 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,314 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 16, 15, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,316 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 11, 3, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,316 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:42967'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,318 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,319 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,319 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,320 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,320 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,324 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,325 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,329 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,332 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,334 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,337 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:44803'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,337 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,343 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,348 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,353 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,378 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:42057'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,387 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:34471'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,389 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,394 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,395 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,399 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,401 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,405 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,406 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,410 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,411 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,416 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,424 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:46265'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,451 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:39211'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,453 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,459 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,462 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 15, 3, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,464 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:33713'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,464 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,466 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 0, 0, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,470 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,472 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,472 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,475 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,478 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,478 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,483 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,483 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,488 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,488 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,493 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,494 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,496 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 0, 9, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,523 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,528 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,533 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,536 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 5, 8, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,539 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,544 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,571 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:45949'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,580 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:42981'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,588 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:38241'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,601 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,607 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,612 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,617 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,622 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,659 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 10, 18, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,701 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 15, 18, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,760 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 3, 16, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,761 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,761 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,761 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,761 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,761 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,812 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,818 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,822 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:41581'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,823 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,828 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,833 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:44,845 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 7, 3, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,541 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1510778dcf50>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:44,947 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:44,953 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:44,958 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:44,963 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:44,969 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:45,082 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 5, 14, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:45,208 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.26:45421'. Reason: worker-handle-scheduler-connection-broken
2025-10-02 19:25:44,798 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14b688b359d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:45,226 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:45,231 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:45,237 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:45,242 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:45,248 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:45,258 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 15, 9, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:44,944 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x148dffe337d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:45,333 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:45,338 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:45,344 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:45,349 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:45,354 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:45,408 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 8, 4, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:45,109 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1465df200510>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:45,507 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,511 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,511 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,512 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,512 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,512 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,513 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,513 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,514 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,516 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,517 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 15, 16, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:45,519 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,523 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,523 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,525 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,525 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,527 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,527 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,527 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,531 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,540 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,542 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,565 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:45,568 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:45,574 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:45,579 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:45,584 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:45,590 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:45,597 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 3, 5, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:45,640 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:45,253 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14ecb03b4610>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:45,238 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:45,646 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:45,651 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:45,657 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:45,662 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:45,683 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:45,688 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:45,694 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:45,699 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:45,704 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:45,747 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-open_dataset-tas-vectorize_count_dist-vectorize_count_dist_0-transpose-8ed21b9130c99dbf2e08f6c98849c361', 7, 4, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:45,820 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.26:45123'. Reason: nanny-close-gracefully
2025-10-02 19:25:45,823 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.26:39721'. Reason: nanny-close-gracefully
2025-10-02 19:25:45,823 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.26:33633'. Reason: nanny-close-gracefully
2025-10-02 19:25:45,824 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.26:36119'. Reason: nanny-close-gracefully
2025-10-02 19:25:45,826 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.26:37267'. Reason: nanny-close-gracefully
2025-10-02 19:25:45,827 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.26:45123' closed.
2025-10-02 19:25:45,828 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.26:39721' closed.
2025-10-02 19:25:45,829 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.26:33633' closed.
2025-10-02 19:25:45,829 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.26:36119' closed.
2025-10-02 19:25:45,830 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.26:37267' closed.
2025-10-02 19:25:45,842 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:45,843 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:45,843 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:45,843 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:45,844 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:45,868 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.26:34963'. Reason: nanny-close-gracefully
2025-10-02 19:25:45,869 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.26:34963' closed.
2025-10-02 19:25:45,874 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.26:40377'. Reason: nanny-close-gracefully
2025-10-02 19:25:45,878 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.26:40377' closed.
2025-10-02 19:25:45,887 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.26:36093'. Reason: nanny-close-gracefully
2025-10-02 19:25:45,888 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.26:45585'. Reason: nanny-close-gracefully
2025-10-02 19:25:45,889 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.26:36093' closed.
2025-10-02 19:25:45,889 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.26:45585' closed.
2025-10-02 19:25:45,908 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.26:40081'. Reason: nanny-close-gracefully
2025-10-02 19:25:45,909 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.26:40081' closed.
2025-10-02 19:25:45,928 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.26:41069'. Reason: nanny-close-gracefully
2025-10-02 19:25:45,931 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.26:41069' closed.
2025-10-02 19:25:45,977 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:45,982 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:45,985 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 8, 9, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:45,988 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:45,993 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:45,998 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:46,002 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:46,007 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:46,013 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:46,018 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:46,023 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:46,033 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:46,039 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:46,044 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:46,050 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:46,055 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:46,059 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.26:37845'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,062 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.26:39137'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,067 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.26:37845' closed.
2025-10-02 19:25:46,073 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.26:39137' closed.
2025-10-02 19:25:45,772 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:46,076 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.26:41501'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,077 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.26:44965'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,078 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.26:41501' closed.
2025-10-02 19:25:46,079 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.26:44965' closed.
2025-10-02 19:25:46,088 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('getitem-vectorize_count_dist-vectorize_count_dist_0-transpose-b96e810c809ee6f674372a61e71c6a58', 3, 12, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-10-02 19:25:46,101 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.26:37147'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,102 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.26:37147' closed.
2025-10-02 19:25:45,710 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:46,123 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.26:36179'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,124 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.26:36179' closed.
2025-10-02 19:25:46,134 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.26:43573'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,135 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.26:43573' closed.
2025-10-02 19:25:46,140 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.26:35871'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,140 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:46,140 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.26:35871' closed.
2025-10-02 19:25:46,143 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.26:33215'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,144 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.26:33215' closed.
2025-10-02 19:25:46,146 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:46,151 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:46,156 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:46,161 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:46,172 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:46,177 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:46,182 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:46,188 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:46,193 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:46,220 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.26:37563'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,221 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.26:37563' closed.
2025-10-02 19:25:46,238 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.26:40975'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,239 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.26:40975' closed.
2025-10-02 19:25:46,300 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.26:43735'. Reason: nanny-close-gracefully
2025-10-02 19:25:46,301 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.26:43735' closed.
2025-10-02 19:25:46,101 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:46,583 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-10-02 19:25:46,589 - distributed.worker - INFO - Removing Worker plugin qme_utils.pyded5daa7-7007-47f6-ad27-3b0ac9da2b87
2025-10-02 19:25:46,594 - distributed.worker - INFO - Removing Worker plugin qme_vars.py562be83c-1ca7-4bdc-9337-bb154c7b8f74
2025-10-02 19:25:46,600 - distributed.worker - INFO - Removing Worker plugin qme_train.pyb23b4f7a-3d99-4e2f-8599-9faead5e5cc0
2025-10-02 19:25:46,605 - distributed.worker - INFO - Removing Worker plugin qme_apply.py52c91d12-ae3f-4ab5-b0e4-d44fef9dc495
2025-10-02 19:25:46,451 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14e905ed7850>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:46,270 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1499ce250dd0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:46,413 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:46,516 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x153caf97c510>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:46,558 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14db07eb4c50>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:46,645 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1508252fa390>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:46,890 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:46,862 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:46,997 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-02 19:25:48,840 - distributed.nanny - INFO - Worker closed
2025-10-02 19:25:50,844 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-10-02 19:25:51,346 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.26:33713'. Reason: nanny-close-gracefully
2025-10-02 19:25:51,347 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.26:33713' closed.
