Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-15 10:50:27,180 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:38831'
2025-09-15 10:50:27,189 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:46613'
2025-09-15 10:50:27,193 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:44259'
2025-09-15 10:50:27,198 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:42183'
2025-09-15 10:50:27,202 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:45701'
2025-09-15 10:50:27,206 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:34039'
2025-09-15 10:50:27,210 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:45675'
2025-09-15 10:50:27,214 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:46549'
2025-09-15 10:50:27,219 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:46091'
2025-09-15 10:50:27,225 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:39511'
2025-09-15 10:50:27,229 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:37503'
2025-09-15 10:50:27,234 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:36843'
2025-09-15 10:50:27,237 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:35299'
2025-09-15 10:50:27,241 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:33461'
2025-09-15 10:50:27,246 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:39183'
2025-09-15 10:50:27,251 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:45357'
2025-09-15 10:50:27,256 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:40519'
2025-09-15 10:50:27,260 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:33693'
2025-09-15 10:50:27,264 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:35547'
2025-09-15 10:50:27,269 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:38343'
2025-09-15 10:50:27,366 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:45047'
2025-09-15 10:50:27,370 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:37479'
2025-09-15 10:50:27,375 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:43415'
2025-09-15 10:50:27,380 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:40969'
2025-09-15 10:50:27,385 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:39651'
2025-09-15 10:50:27,389 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:35437'
2025-09-15 10:50:27,394 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:40973'
2025-09-15 10:50:27,398 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:36483'
2025-09-15 10:50:27,402 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:39115'
2025-09-15 10:50:27,406 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:38311'
2025-09-15 10:50:27,409 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:43721'
2025-09-15 10:50:27,413 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:41241'
2025-09-15 10:50:27,417 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:45753'
2025-09-15 10:50:27,422 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:43539'
2025-09-15 10:50:27,427 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:41535'
2025-09-15 10:50:27,431 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:44383'
2025-09-15 10:50:27,437 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:37115'
2025-09-15 10:50:27,442 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:42675'
2025-09-15 10:50:27,447 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:42279'
2025-09-15 10:50:27,451 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:32987'
2025-09-15 10:50:27,456 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:42197'
2025-09-15 10:50:27,460 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:40309'
2025-09-15 10:50:27,466 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:41523'
2025-09-15 10:50:27,471 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:46591'
2025-09-15 10:50:27,476 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:40731'
2025-09-15 10:50:27,481 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:34805'
2025-09-15 10:50:27,486 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:38589'
2025-09-15 10:50:27,489 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:33141'
2025-09-15 10:50:27,493 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:35337'
2025-09-15 10:50:27,501 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:38541'
2025-09-15 10:50:27,505 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:34953'
2025-09-15 10:50:27,507 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.83.66:34503'
2025-09-15 10:50:28,417 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:44101
2025-09-15 10:50:28,417 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:41547
2025-09-15 10:50:28,417 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:41171
2025-09-15 10:50:28,417 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:42979
2025-09-15 10:50:28,418 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:44101
2025-09-15 10:50:28,418 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:41547
2025-09-15 10:50:28,418 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:41171
2025-09-15 10:50:28,418 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:41235
2025-09-15 10:50:28,418 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:42979
2025-09-15 10:50:28,418 - distributed.worker - INFO -          dashboard at:           10.6.83.66:45133
2025-09-15 10:50:28,418 - distributed.worker - INFO -          dashboard at:           10.6.83.66:32877
2025-09-15 10:50:28,418 - distributed.worker - INFO -          dashboard at:           10.6.83.66:41643
2025-09-15 10:50:28,418 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:41235
2025-09-15 10:50:28,418 - distributed.worker - INFO -          dashboard at:           10.6.83.66:42115
2025-09-15 10:50:28,418 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,418 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,418 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,418 - distributed.worker - INFO -          dashboard at:           10.6.83.66:33309
2025-09-15 10:50:28,418 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,418 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,418 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,418 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,418 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,418 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,418 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,418 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,418 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,418 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,418 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,418 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,418 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,418 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,418 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,418 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,418 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-9uj78vq7
2025-09-15 10:50:28,418 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-doeqxzfy
2025-09-15 10:50:28,418 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,418 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-on9xya48
2025-09-15 10:50:28,418 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-wsaci7vm
2025-09-15 10:50:28,418 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,418 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,418 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-yisq9slb
2025-09-15 10:50:28,418 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,418 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,418 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,429 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:43011
2025-09-15 10:50:28,430 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:43011
2025-09-15 10:50:28,430 - distributed.worker - INFO -          dashboard at:           10.6.83.66:38011
2025-09-15 10:50:28,430 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,430 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,430 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,430 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,430 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-ohbm23xc
2025-09-15 10:50:28,430 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,437 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:34363
2025-09-15 10:50:28,437 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:34363
2025-09-15 10:50:28,437 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:46735
2025-09-15 10:50:28,437 - distributed.worker - INFO -          dashboard at:           10.6.83.66:44171
2025-09-15 10:50:28,437 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,437 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:46735
2025-09-15 10:50:28,437 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,437 - distributed.worker - INFO -          dashboard at:           10.6.83.66:46717
2025-09-15 10:50:28,437 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,437 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,437 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,437 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,437 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,437 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-debxyx0s
2025-09-15 10:50:28,437 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,438 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,438 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-52l6alg0
2025-09-15 10:50:28,438 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,441 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:33849
2025-09-15 10:50:28,441 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:33849
2025-09-15 10:50:28,441 - distributed.worker - INFO -          dashboard at:           10.6.83.66:40931
2025-09-15 10:50:28,441 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,441 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,441 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,441 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,442 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-skf8h0bq
2025-09-15 10:50:28,442 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,442 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:36389
2025-09-15 10:50:28,442 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:36389
2025-09-15 10:50:28,442 - distributed.worker - INFO -          dashboard at:           10.6.83.66:39863
2025-09-15 10:50:28,442 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,442 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,442 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,442 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,442 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-5hv7ap4p
2025-09-15 10:50:28,442 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:38987
2025-09-15 10:50:28,442 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,442 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:38987
2025-09-15 10:50:28,443 - distributed.worker - INFO -          dashboard at:           10.6.83.66:46027
2025-09-15 10:50:28,443 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,443 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,443 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,443 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,443 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-gsu1wk_i
2025-09-15 10:50:28,443 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,443 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:34491
2025-09-15 10:50:28,444 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:34491
2025-09-15 10:50:28,444 - distributed.worker - INFO -          dashboard at:           10.6.83.66:43773
2025-09-15 10:50:28,444 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,444 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,444 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,444 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,444 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-riz76sno
2025-09-15 10:50:28,444 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,447 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,448 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,448 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,449 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,455 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,456 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,456 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,457 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,458 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:34997
2025-09-15 10:50:28,458 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:34997
2025-09-15 10:50:28,458 - distributed.worker - INFO -          dashboard at:           10.6.83.66:32985
2025-09-15 10:50:28,458 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,458 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,458 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,458 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,458 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-zcw88aaq
2025-09-15 10:50:28,458 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,462 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:38997
2025-09-15 10:50:28,462 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:38997
2025-09-15 10:50:28,462 - distributed.worker - INFO -          dashboard at:           10.6.83.66:42677
2025-09-15 10:50:28,462 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,462 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,462 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,462 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,462 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-5doo97nw
2025-09-15 10:50:28,462 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,463 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,464 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,466 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,468 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,469 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,470 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,471 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,473 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,474 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,475 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,476 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,478 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,479 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,479 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,480 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,483 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,483 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:35209
2025-09-15 10:50:28,483 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:35209
2025-09-15 10:50:28,483 - distributed.worker - INFO -          dashboard at:           10.6.83.66:42215
2025-09-15 10:50:28,483 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,483 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,483 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,483 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,483 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-kltsn84v
2025-09-15 10:50:28,483 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,484 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:40119
2025-09-15 10:50:28,484 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:40119
2025-09-15 10:50:28,484 - distributed.worker - INFO -          dashboard at:           10.6.83.66:43497
2025-09-15 10:50:28,484 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,484 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,484 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,484 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,484 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-eyvhk2nz
2025-09-15 10:50:28,484 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,484 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,484 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,486 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,488 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,488 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:41429
2025-09-15 10:50:28,489 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:41429
2025-09-15 10:50:28,489 - distributed.worker - INFO -          dashboard at:           10.6.83.66:46783
2025-09-15 10:50:28,489 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,489 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,489 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,489 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,489 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-6p2d_6y1
2025-09-15 10:50:28,489 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,489 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,490 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,491 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:43109
2025-09-15 10:50:28,491 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:43109
2025-09-15 10:50:28,491 - distributed.worker - INFO -          dashboard at:           10.6.83.66:39289
2025-09-15 10:50:28,491 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,491 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,491 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,491 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,491 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,491 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-or8ga8cc
2025-09-15 10:50:28,491 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,493 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,494 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,494 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,496 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,498 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,499 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,499 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,501 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:43593
2025-09-15 10:50:28,501 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:43593
2025-09-15 10:50:28,501 - distributed.worker - INFO -          dashboard at:           10.6.83.66:39879
2025-09-15 10:50:28,501 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,501 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,501 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,501 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,501 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-vk6p5vtn
2025-09-15 10:50:28,501 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,501 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,502 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,504 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,504 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,506 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,508 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,509 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,509 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,511 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,526 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,527 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,529 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,531 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,532 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,532 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,534 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,535 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:40941
2025-09-15 10:50:28,535 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:40941
2025-09-15 10:50:28,535 - distributed.worker - INFO -          dashboard at:           10.6.83.66:45647
2025-09-15 10:50:28,535 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,535 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,535 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,535 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,535 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-qhn5jymb
2025-09-15 10:50:28,535 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,536 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,537 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,537 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,539 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,541 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,542 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,542 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,544 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,546 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,547 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,547 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,549 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,550 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,551 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,551 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,553 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,554 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,556 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,556 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,557 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,578 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,579 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:35307
2025-09-15 10:50:28,579 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:35307
2025-09-15 10:50:28,579 - distributed.worker - INFO -          dashboard at:           10.6.83.66:33611
2025-09-15 10:50:28,579 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,579 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,579 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,579 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,579 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-ys413k_o
2025-09-15 10:50:28,579 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,579 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,580 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,581 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,592 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,593 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,593 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,593 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,683 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:42015
2025-09-15 10:50:28,683 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:42015
2025-09-15 10:50:28,683 - distributed.worker - INFO -          dashboard at:           10.6.83.66:44445
2025-09-15 10:50:28,683 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,683 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,683 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,683 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,683 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-kd_f957h
2025-09-15 10:50:28,683 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,697 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:46425
2025-09-15 10:50:28,697 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:46425
2025-09-15 10:50:28,697 - distributed.worker - INFO -          dashboard at:           10.6.83.66:39143
2025-09-15 10:50:28,697 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,697 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,697 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,697 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,697 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-hps656_c
2025-09-15 10:50:28,697 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,718 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:34717
2025-09-15 10:50:28,718 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:34717
2025-09-15 10:50:28,718 - distributed.worker - INFO -          dashboard at:           10.6.83.66:36965
2025-09-15 10:50:28,718 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,718 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,718 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,718 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,718 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-317nkvpx
2025-09-15 10:50:28,718 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,718 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,719 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,719 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,721 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,728 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,729 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,730 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,731 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,753 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,754 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,754 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,755 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,764 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:36187
2025-09-15 10:50:28,764 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:36187
2025-09-15 10:50:28,764 - distributed.worker - INFO -          dashboard at:           10.6.83.66:35207
2025-09-15 10:50:28,764 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,764 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,764 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,764 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,764 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-qsg_f32q
2025-09-15 10:50:28,764 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,766 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:32977
2025-09-15 10:50:28,766 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:32977
2025-09-15 10:50:28,766 - distributed.worker - INFO -          dashboard at:           10.6.83.66:34589
2025-09-15 10:50:28,766 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,766 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,766 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,766 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,766 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-km4iu0vw
2025-09-15 10:50:28,766 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,796 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,797 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,797 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,798 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,805 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:40023
2025-09-15 10:50:28,805 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:40023
2025-09-15 10:50:28,805 - distributed.worker - INFO -          dashboard at:           10.6.83.66:40043
2025-09-15 10:50:28,805 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,805 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,805 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,805 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,805 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-ziei5683
2025-09-15 10:50:28,805 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,806 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,808 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,808 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,810 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,820 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:42395
2025-09-15 10:50:28,820 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:42395
2025-09-15 10:50:28,820 - distributed.worker - INFO -          dashboard at:           10.6.83.66:43613
2025-09-15 10:50:28,820 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,820 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,820 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,820 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,820 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-kj4bked6
2025-09-15 10:50:28,821 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,827 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:33671
2025-09-15 10:50:28,827 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:33671
2025-09-15 10:50:28,827 - distributed.worker - INFO -          dashboard at:           10.6.83.66:38703
2025-09-15 10:50:28,827 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,827 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,827 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,827 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,827 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-kwh1pbyg
2025-09-15 10:50:28,827 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,827 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:35287
2025-09-15 10:50:28,828 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:35287
2025-09-15 10:50:28,828 - distributed.worker - INFO -          dashboard at:           10.6.83.66:32879
2025-09-15 10:50:28,828 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,828 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,828 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,828 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,828 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-k2hn0g46
2025-09-15 10:50:28,828 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,829 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:34873
2025-09-15 10:50:28,829 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:34873
2025-09-15 10:50:28,829 - distributed.worker - INFO -          dashboard at:           10.6.83.66:35211
2025-09-15 10:50:28,829 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,829 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,829 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,829 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,829 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-_p0a5xt6
2025-09-15 10:50:28,829 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,829 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:37055
2025-09-15 10:50:28,829 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:37055
2025-09-15 10:50:28,829 - distributed.worker - INFO -          dashboard at:           10.6.83.66:42545
2025-09-15 10:50:28,829 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,829 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,829 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,829 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,829 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-ofc5p_nx
2025-09-15 10:50:28,830 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,836 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,837 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,837 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,838 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,839 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:37117
2025-09-15 10:50:28,839 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:37117
2025-09-15 10:50:28,839 - distributed.worker - INFO -          dashboard at:           10.6.83.66:36953
2025-09-15 10:50:28,839 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,839 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,839 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,839 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,839 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-pvitdhdf
2025-09-15 10:50:28,839 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,844 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:36373
2025-09-15 10:50:28,844 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:36373
2025-09-15 10:50:28,844 - distributed.worker - INFO -          dashboard at:           10.6.83.66:38607
2025-09-15 10:50:28,844 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,844 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,844 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,844 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,844 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-nowtmuno
2025-09-15 10:50:28,844 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,846 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:46007
2025-09-15 10:50:28,846 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:46007
2025-09-15 10:50:28,847 - distributed.worker - INFO -          dashboard at:           10.6.83.66:33815
2025-09-15 10:50:28,847 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:39771
2025-09-15 10:50:28,847 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,847 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:39771
2025-09-15 10:50:28,847 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,847 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,847 - distributed.worker - INFO -          dashboard at:           10.6.83.66:42565
2025-09-15 10:50:28,847 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,847 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,847 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,847 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-5v2zdtao
2025-09-15 10:50:28,847 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,847 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,847 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,847 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-31qoluic
2025-09-15 10:50:28,847 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,848 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,848 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:40627
2025-09-15 10:50:28,848 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:40627
2025-09-15 10:50:28,848 - distributed.worker - INFO -          dashboard at:           10.6.83.66:36193
2025-09-15 10:50:28,848 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,848 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,848 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,848 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,848 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-4znicxrf
2025-09-15 10:50:28,848 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,849 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,849 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,850 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:41437
2025-09-15 10:50:28,850 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:41437
2025-09-15 10:50:28,850 - distributed.worker - INFO -          dashboard at:           10.6.83.66:36123
2025-09-15 10:50:28,850 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,850 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,850 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,850 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,850 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-kyanqt2e
2025-09-15 10:50:28,850 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,850 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,852 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:35415
2025-09-15 10:50:28,852 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:35415
2025-09-15 10:50:28,852 - distributed.worker - INFO -          dashboard at:           10.6.83.66:38351
2025-09-15 10:50:28,852 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,852 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,852 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,852 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,852 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-ozyqkl03
2025-09-15 10:50:28,852 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,857 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:43267
2025-09-15 10:50:28,857 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:43267
2025-09-15 10:50:28,857 - distributed.worker - INFO -          dashboard at:           10.6.83.66:34169
2025-09-15 10:50:28,857 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,857 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,857 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,857 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,857 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-cpstr2x6
2025-09-15 10:50:28,857 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,858 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:42513
2025-09-15 10:50:28,858 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:42513
2025-09-15 10:50:28,858 - distributed.worker - INFO -          dashboard at:           10.6.83.66:40871
2025-09-15 10:50:28,858 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,858 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,858 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,858 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,858 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,858 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-xl6hahm9
2025-09-15 10:50:28,858 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,859 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,859 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,860 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,863 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:45757
2025-09-15 10:50:28,863 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:45757
2025-09-15 10:50:28,863 - distributed.worker - INFO -          dashboard at:           10.6.83.66:33531
2025-09-15 10:50:28,863 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,863 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,863 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,863 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,863 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-q37ey6uk
2025-09-15 10:50:28,863 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:43643
2025-09-15 10:50:28,864 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,864 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:43643
2025-09-15 10:50:28,864 - distributed.worker - INFO -          dashboard at:           10.6.83.66:40139
2025-09-15 10:50:28,864 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,864 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,864 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,864 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,864 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-uox1faa8
2025-09-15 10:50:28,864 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,864 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:42329
2025-09-15 10:50:28,864 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:42329
2025-09-15 10:50:28,864 - distributed.worker - INFO -          dashboard at:           10.6.83.66:37917
2025-09-15 10:50:28,864 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,864 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,864 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,864 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,864 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-hyb7h26m
2025-09-15 10:50:28,864 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,865 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,866 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,866 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,867 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,867 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:44331
2025-09-15 10:50:28,867 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:44331
2025-09-15 10:50:28,868 - distributed.worker - INFO -          dashboard at:           10.6.83.66:36467
2025-09-15 10:50:28,868 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,868 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,868 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,868 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,868 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-19po1lzq
2025-09-15 10:50:28,868 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,869 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:34229
2025-09-15 10:50:28,869 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:34229
2025-09-15 10:50:28,869 - distributed.worker - INFO -          dashboard at:           10.6.83.66:40409
2025-09-15 10:50:28,869 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,869 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,869 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,869 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,869 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-ry38f9ww
2025-09-15 10:50:28,869 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,869 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,871 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,871 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,872 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:37181
2025-09-15 10:50:28,872 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:37181
2025-09-15 10:50:28,872 - distributed.worker - INFO -          dashboard at:           10.6.83.66:40977
2025-09-15 10:50:28,872 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,872 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,872 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,872 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,872 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,872 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-8i2vhaqi
2025-09-15 10:50:28,872 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,874 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,875 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,875 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,877 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,877 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:44425
2025-09-15 10:50:28,877 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:44425
2025-09-15 10:50:28,877 - distributed.worker - INFO -          dashboard at:           10.6.83.66:44677
2025-09-15 10:50:28,877 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,877 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,877 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,878 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,878 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-ba67hvjt
2025-09-15 10:50:28,878 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,878 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,879 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:45677
2025-09-15 10:50:28,879 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:45677
2025-09-15 10:50:28,879 - distributed.worker - INFO -          dashboard at:           10.6.83.66:38301
2025-09-15 10:50:28,879 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,879 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,879 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,879 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,879 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-gzm6g_rf
2025-09-15 10:50:28,879 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,880 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,880 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,881 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:33433
2025-09-15 10:50:28,881 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:33433
2025-09-15 10:50:28,881 - distributed.worker - INFO -          dashboard at:           10.6.83.66:34677
2025-09-15 10:50:28,881 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,881 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,881 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,881 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,881 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-5aktjm5f
2025-09-15 10:50:28,881 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,881 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,882 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:45571
2025-09-15 10:50:28,882 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:45571
2025-09-15 10:50:28,882 - distributed.worker - INFO -          dashboard at:           10.6.83.66:36763
2025-09-15 10:50:28,882 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,882 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,882 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,882 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,882 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-0ink98xz
2025-09-15 10:50:28,883 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,882 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,883 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,884 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,885 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,887 - distributed.worker - INFO -       Start worker at:     tcp://10.6.83.66:45039
2025-09-15 10:50:28,887 - distributed.worker - INFO -          Listening to:     tcp://10.6.83.66:45039
2025-09-15 10:50:28,887 - distributed.worker - INFO -          dashboard at:           10.6.83.66:42507
2025-09-15 10:50:28,887 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,887 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,887 - distributed.worker - INFO -               Threads:                          2
2025-09-15 10:50:28,887 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-15 10:50:28,887 - distributed.worker - INFO -       Local Directory: /jobfs/149690456.gadi-pbs/dask-scratch-space/worker-thvlyxfa
2025-09-15 10:50:28,887 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,889 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,890 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,890 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,892 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,894 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,895 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,895 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,896 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,898 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,899 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,899 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,902 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,902 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,903 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,903 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,905 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,907 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,908 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,908 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,910 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,911 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,913 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,913 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,914 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,916 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,917 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,917 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,919 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,920 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,921 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,921 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,922 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,924 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,925 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,925 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,927 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,928 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,929 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,929 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,931 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,932 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,933 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,933 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,935 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,936 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,937 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,937 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,939 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,940 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,941 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,941 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,943 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,944 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,945 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,945 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,947 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,948 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,949 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,950 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,951 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,955 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,956 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,956 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,957 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,970 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,971 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,971 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,972 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:28,974 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-15 10:50:28,975 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.36:8734
2025-09-15 10:50:28,975 - distributed.worker - INFO - -------------------------------------------------
2025-09-15 10:50:28,976 - distributed.core - INFO - Starting established connection to tcp://10.6.82.36:8734
2025-09-15 10:50:58,677 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,677 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,677 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,677 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,677 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,677 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,677 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,677 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:45571. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,677 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,677 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,677 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,677 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,677 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,677 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,677 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,677 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,678 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:44425. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,679 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,677 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,678 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,678 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:42329. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,678 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:43643. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,678 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,678 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,678 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:37181. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,678 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,678 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:39771. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,678 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:36373. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,678 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:34229. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,678 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:33849. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,679 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:42513. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,679 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:41235. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,679 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,679 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:36389. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,679 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,679 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,679 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:41437. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,679 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,679 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,679 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:41547. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,679 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:41171. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,679 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:36187. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,679 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,680 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:45677. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,680 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:35415. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,680 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:37055. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,680 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:42979. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,680 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:44331. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,680 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:37117. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,681 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:40023. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,678 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,679 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,681 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:34717. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,681 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:35287. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,679 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,681 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:35307. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,682 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:46425. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,677 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.66:57182 remote=tcp://10.6.82.36:8734>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-15 10:50:58,680 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,682 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,680 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,683 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:34363. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,681 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,681 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,681 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,683 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:33433. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,683 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:44101. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,677 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.66:57206 remote=tcp://10.6.82.36:8734>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-15 10:50:58,677 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.66:57270 remote=tcp://10.6.82.36:8734>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-15 10:50:58,677 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.66:57238 remote=tcp://10.6.82.36:8734>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-15 10:50:58,677 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.66:57208 remote=tcp://10.6.82.36:8734>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-15 10:50:58,677 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.66:57276 remote=tcp://10.6.82.36:8734>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-15 10:50:58,677 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.66:57254 remote=tcp://10.6.82.36:8734>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-15 10:50:58,677 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.66:57156 remote=tcp://10.6.82.36:8734>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-15 10:50:58,677 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.66:57172 remote=tcp://10.6.82.36:8734>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-15 10:50:58,682 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,684 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:45039. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,682 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,684 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:33671. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,685 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,685 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,685 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:42395. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,685 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,685 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:32977. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,685 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,685 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:40627. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,685 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,685 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,685 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:42015. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,685 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,686 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:35209. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,686 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:43109. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,686 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:34997. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,683 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,686 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,686 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:38997. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,684 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,686 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:43593. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,686 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:40119. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,686 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:34491. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,686 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:43011. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,687 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:46007. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,688 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:34873. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,689 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:45757. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,689 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:43267. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,681 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.66:57140 remote=tcp://10.6.82.36:8734>: Stream is closed
2025-09-15 10:50:58,681 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.66:57224 remote=tcp://10.6.82.36:8734>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-15 10:50:58,681 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.66:57282 remote=tcp://10.6.82.36:8734>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-15 10:50:58,695 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,682 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.83.66:57190 remote=tcp://10.6.82.36:8734>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-15 10:50:58,696 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,697 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:38987. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,697 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,697 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:41429. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,697 - distributed.core - INFO - Connection to tcp://10.6.82.36:8734 has been closed.
2025-09-15 10:50:58,698 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:40941. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,699 - distributed.worker - INFO - Stopping worker at tcp://10.6.83.66:46735. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,698 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:35337'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,701 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,704 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,710 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:41241'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,711 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:38343'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,711 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,711 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:40309'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,711 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,711 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:39115'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,712 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:32987'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,712 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:41535'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,712 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,712 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,712 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:34805'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,712 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,713 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,713 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:36843'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,713 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,713 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:46549'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,713 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:34503'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,713 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,714 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:40519'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,714 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,714 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:40731'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,714 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:42675'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,714 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,714 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:45675'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,714 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,714 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,715 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:44259'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,715 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,715 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:38589'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,715 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,715 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,715 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:41523'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,715 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,715 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:40969'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,716 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,716 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:46613'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,716 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,716 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,716 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,716 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:44383'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,716 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,716 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,717 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,717 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,717 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,717 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,717 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,718 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,718 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,718 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,719 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,719 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,719 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,719 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,720 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,720 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,720 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,721 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,721 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,721 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,723 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:35437'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,723 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:36483'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,724 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:38541'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,724 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:33141'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,724 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:37115'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,724 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,724 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,724 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:33461'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,724 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,724 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:45753'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,724 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,725 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:35547'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,725 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,725 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,725 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:34039'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,725 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,725 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:45701'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,725 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,725 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:46091'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,725 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,726 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:45357'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,726 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:33693'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,726 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,726 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:37503'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,726 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,726 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:39511'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,726 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,726 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:42197'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,726 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,727 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:38831'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,726 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,727 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:39651'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,727 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,727 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:46591'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,727 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:43721'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,727 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,727 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:38311'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,727 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:42279'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,727 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,728 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:37479'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,728 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,728 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,728 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:43539'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,728 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,728 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,728 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,728 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,728 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,728 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,728 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,728 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,729 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,729 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,729 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,730 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,730 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,730 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,730 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,730 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,730 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,731 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:40973'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,731 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,731 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,731 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:34953'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,731 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:43415'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,731 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:39183'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,731 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:35299'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,732 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:42183'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,732 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.83.66:45047'. Reason: worker-handle-scheduler-connection-broken
2025-09-15 10:50:58,732 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,732 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,732 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,732 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,732 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,732 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,732 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,732 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-15 10:50:58,740 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,742 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,742 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,743 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,743 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,744 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,744 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,744 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,745 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,746 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,746 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,747 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,747 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,747 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,748 - distributed.nanny - INFO - Worker closed
2025-09-15 10:50:58,748 - distributed.nanny - INFO - Worker closed
2025-09-15 10:51:00,720 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-15 10:51:00,720 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-15 10:51:00,720 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-15 10:51:00,722 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-15 10:51:00,723 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-15 10:51:00,724 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-15 10:51:00,732 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-15 10:51:00,732 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-15 10:51:00,733 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-15 10:51:00,737 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-15 10:51:00,752 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-15 10:51:00,753 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-15 10:51:00,755 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-15 10:51:00,757 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-15 10:51:00,909 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:35337'. Reason: nanny-close-gracefully
2025-09-15 10:51:00,911 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:35337' closed.
2025-09-15 10:51:00,925 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:38343'. Reason: nanny-close-gracefully
2025-09-15 10:51:00,926 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:38343' closed.
2025-09-15 10:51:00,928 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:40309'. Reason: nanny-close-gracefully
2025-09-15 10:51:00,929 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:40309' closed.
2025-09-15 10:51:00,934 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:34805'. Reason: nanny-close-gracefully
2025-09-15 10:51:00,935 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:34503'. Reason: nanny-close-gracefully
2025-09-15 10:51:00,935 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:34805' closed.
2025-09-15 10:51:00,936 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:34503' closed.
2025-09-15 10:51:00,942 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:38589'. Reason: nanny-close-gracefully
2025-09-15 10:51:00,946 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:38589' closed.
2025-09-15 10:51:00,954 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:44383'. Reason: nanny-close-gracefully
2025-09-15 10:51:00,954 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:46613'. Reason: nanny-close-gracefully
2025-09-15 10:51:00,956 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:44383' closed.
2025-09-15 10:51:00,956 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:46613' closed.
2025-09-15 10:51:00,958 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:41535'. Reason: nanny-close-gracefully
2025-09-15 10:51:00,958 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:41535' closed.
2025-09-15 10:51:00,970 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:35547'. Reason: nanny-close-gracefully
2025-09-15 10:51:00,971 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:35547' closed.
2025-09-15 10:51:00,981 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:45753'. Reason: nanny-close-gracefully
2025-09-15 10:51:00,985 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:39115'. Reason: nanny-close-gracefully
2025-09-15 10:51:00,987 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:45753' closed.
2025-09-15 10:51:00,987 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:41523'. Reason: nanny-close-gracefully
2025-09-15 10:51:00,988 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:39115' closed.
2025-09-15 10:51:00,988 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:41523' closed.
2025-09-15 10:51:00,990 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:41241'. Reason: nanny-close-gracefully
2025-09-15 10:51:00,993 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:41241' closed.
2025-09-15 10:51:00,999 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:46091'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,000 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:46091' closed.
2025-09-15 10:51:01,012 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:36483'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,014 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:37115'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,014 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:36483' closed.
2025-09-15 10:51:01,015 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:37115' closed.
2025-09-15 10:51:01,020 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:42675'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,021 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:42675' closed.
2025-09-15 10:51:01,026 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:32987'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,027 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:32987' closed.
2025-09-15 10:51:01,031 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:40519'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,041 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:40519' closed.
2025-09-15 10:51:01,054 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:35437'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,054 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:34039'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,055 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:35437' closed.
2025-09-15 10:51:01,055 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:34039' closed.
2025-09-15 10:51:01,060 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:38311'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,061 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:38311' closed.
2025-09-15 10:51:01,069 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:45047'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,071 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:45357'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,072 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:45047' closed.
2025-09-15 10:51:01,072 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:45357' closed.
2025-09-15 10:51:01,088 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:43721'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,092 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:34953'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,092 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:38831'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,093 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:43721' closed.
2025-09-15 10:51:01,094 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:34953' closed.
2025-09-15 10:51:01,094 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:38831' closed.
2025-09-15 10:51:01,096 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:42183'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,098 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:39651'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,099 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:42183' closed.
2025-09-15 10:51:01,100 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:39651' closed.
2025-09-15 10:51:01,104 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:42279'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,105 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:42279' closed.
2025-09-15 10:51:01,106 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:43415'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,107 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:43415' closed.
2025-09-15 10:51:01,107 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:39183'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,109 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:43539'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,110 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:39183' closed.
2025-09-15 10:51:01,110 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:42197'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,111 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:43539' closed.
2025-09-15 10:51:01,111 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:42197' closed.
2025-09-15 10:51:01,114 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:40973'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,115 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:40973' closed.
2025-09-15 10:51:01,115 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:35299'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,117 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:33693'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,117 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:35299' closed.
2025-09-15 10:51:01,117 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:33693' closed.
2025-09-15 10:51:01,120 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:33461'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,120 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:33461' closed.
2025-09-15 10:51:01,124 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:39511'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,126 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:37479'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,128 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:38541'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,129 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:46591'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,129 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:40969'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,129 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:46549'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,130 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:33141'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,131 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:39511' closed.
2025-09-15 10:51:01,132 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:37479' closed.
2025-09-15 10:51:01,133 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:45675'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,134 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:38541' closed.
2025-09-15 10:51:01,134 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:40731'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,134 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:46591' closed.
2025-09-15 10:51:01,134 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:40969' closed.
2025-09-15 10:51:01,135 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:44259'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,135 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:46549' closed.
2025-09-15 10:51:01,135 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:33141' closed.
2025-09-15 10:51:01,136 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:45675' closed.
2025-09-15 10:51:01,137 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:40731' closed.
2025-09-15 10:51:01,137 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:45701'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,137 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:44259' closed.
2025-09-15 10:51:01,138 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:36843'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,139 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:45701' closed.
2025-09-15 10:51:01,139 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:36843' closed.
2025-09-15 10:51:01,144 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.83.66:37503'. Reason: nanny-close-gracefully
2025-09-15 10:51:01,145 - distributed.nanny - INFO - Nanny at 'tcp://10.6.83.66:37503' closed.
2025-09-15 10:51:01,147 - distributed.dask_worker - INFO - End worker
