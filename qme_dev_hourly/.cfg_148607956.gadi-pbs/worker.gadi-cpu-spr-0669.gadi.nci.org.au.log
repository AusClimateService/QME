Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-03 10:52:52,812 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:45625'
2025-09-03 10:52:52,823 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:35813'
2025-09-03 10:52:52,829 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:39009'
2025-09-03 10:52:52,835 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:40231'
2025-09-03 10:52:52,842 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:43591'
2025-09-03 10:52:52,952 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:34907'
2025-09-03 10:52:52,956 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:36325'
2025-09-03 10:52:52,960 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:44253'
2025-09-03 10:52:52,964 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:43259'
2025-09-03 10:52:52,971 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:37189'
2025-09-03 10:52:52,975 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:46345'
2025-09-03 10:52:52,979 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:45591'
2025-09-03 10:52:53,262 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:36739'
2025-09-03 10:52:53,266 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:35835'
2025-09-03 10:52:53,270 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:44339'
2025-09-03 10:52:53,275 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:46825'
2025-09-03 10:52:53,280 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:35147'
2025-09-03 10:52:53,580 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:36755'
2025-09-03 10:52:53,584 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:39641'
2025-09-03 10:52:53,589 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:43405'
2025-09-03 10:52:53,594 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:33163'
2025-09-03 10:52:53,599 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:45931'
2025-09-03 10:52:53,604 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:33387'
2025-09-03 10:52:53,608 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:38821'
2025-09-03 10:52:53,613 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:37955'
2025-09-03 10:52:53,618 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:42723'
2025-09-03 10:52:53,623 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:34739'
2025-09-03 10:52:53,628 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:34233'
2025-09-03 10:52:53,632 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:39183'
2025-09-03 10:52:53,730 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:40949
2025-09-03 10:52:53,730 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:40949
2025-09-03 10:52:53,730 - distributed.worker - INFO -          dashboard at:          10.6.105.21:46281
2025-09-03 10:52:53,730 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,730 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,730 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:45867
2025-09-03 10:52:53,730 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,730 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,730 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:45867
2025-09-03 10:52:53,730 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-g9fl46p_
2025-09-03 10:52:53,730 - distributed.worker - INFO -          dashboard at:          10.6.105.21:39049
2025-09-03 10:52:53,730 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,730 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,730 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,730 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,730 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,730 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-xl0_xsmr
2025-09-03 10:52:53,730 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,733 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:41289
2025-09-03 10:52:53,734 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:41289
2025-09-03 10:52:53,734 - distributed.worker - INFO -          dashboard at:          10.6.105.21:33521
2025-09-03 10:52:53,734 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,734 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,734 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,734 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:37511
2025-09-03 10:52:53,734 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,734 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-c3k0j9zh
2025-09-03 10:52:53,734 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:37511
2025-09-03 10:52:53,734 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,734 - distributed.worker - INFO -          dashboard at:          10.6.105.21:45219
2025-09-03 10:52:53,734 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,734 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,734 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,734 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,734 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-1cw85_j5
2025-09-03 10:52:53,734 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,750 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:34591
2025-09-03 10:52:53,750 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:34591
2025-09-03 10:52:53,750 - distributed.worker - INFO -          dashboard at:          10.6.105.21:32951
2025-09-03 10:52:53,750 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,750 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,750 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,750 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,750 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-2pskq3wv
2025-09-03 10:52:53,750 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,753 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:38589
2025-09-03 10:52:53,753 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:38589
2025-09-03 10:52:53,753 - distributed.worker - INFO -          dashboard at:          10.6.105.21:34159
2025-09-03 10:52:53,753 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,753 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,753 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,753 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,753 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-pf9lgxiz
2025-09-03 10:52:53,753 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,763 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:37953
2025-09-03 10:52:53,763 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:37953
2025-09-03 10:52:53,763 - distributed.worker - INFO -          dashboard at:          10.6.105.21:37993
2025-09-03 10:52:53,763 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,763 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,764 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,764 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,764 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-dj9wsytd
2025-09-03 10:52:53,764 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,782 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:41609
2025-09-03 10:52:53,782 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:41609
2025-09-03 10:52:53,782 - distributed.worker - INFO -          dashboard at:          10.6.105.21:40077
2025-09-03 10:52:53,782 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,782 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,782 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,782 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,782 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-s7kbvbm2
2025-09-03 10:52:53,782 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,786 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:37061
2025-09-03 10:52:53,786 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:37061
2025-09-03 10:52:53,786 - distributed.worker - INFO -          dashboard at:          10.6.105.21:44757
2025-09-03 10:52:53,786 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,786 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,786 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,786 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,787 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-uhxsvs8g
2025-09-03 10:52:53,787 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,805 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:35997
2025-09-03 10:52:53,805 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:35997
2025-09-03 10:52:53,805 - distributed.worker - INFO -          dashboard at:          10.6.105.21:40691
2025-09-03 10:52:53,805 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,805 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,805 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,805 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,805 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ja_hw5wx
2025-09-03 10:52:53,806 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,833 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:38867
2025-09-03 10:52:53,833 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:38867
2025-09-03 10:52:53,833 - distributed.worker - INFO -          dashboard at:          10.6.105.21:38953
2025-09-03 10:52:53,833 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,833 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,834 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,834 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,834 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-0deocwd_
2025-09-03 10:52:53,834 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,851 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:41445
2025-09-03 10:52:53,852 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:41445
2025-09-03 10:52:53,852 - distributed.worker - INFO -          dashboard at:          10.6.105.21:32963
2025-09-03 10:52:53,852 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:53,852 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:53,852 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:53,852 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:53,852 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-tyka082s
2025-09-03 10:52:53,852 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,055 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:41077
2025-09-03 10:52:54,055 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:41077
2025-09-03 10:52:54,056 - distributed.worker - INFO -          dashboard at:          10.6.105.21:33719
2025-09-03 10:52:54,056 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,056 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,056 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,056 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,056 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-x4kg59m9
2025-09-03 10:52:54,056 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,056 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:38005
2025-09-03 10:52:54,056 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:38005
2025-09-03 10:52:54,056 - distributed.worker - INFO -          dashboard at:          10.6.105.21:42117
2025-09-03 10:52:54,056 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,056 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,056 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,056 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,056 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-l2_mwhmu
2025-09-03 10:52:54,056 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,060 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:44361
2025-09-03 10:52:54,060 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:44361
2025-09-03 10:52:54,060 - distributed.worker - INFO -          dashboard at:          10.6.105.21:34613
2025-09-03 10:52:54,060 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,060 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,060 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,060 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,060 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-olgwuxvm
2025-09-03 10:52:54,060 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,063 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:43495
2025-09-03 10:52:54,063 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:43495
2025-09-03 10:52:54,063 - distributed.worker - INFO -          dashboard at:          10.6.105.21:38433
2025-09-03 10:52:54,063 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,063 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,063 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,063 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,063 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-3nbfq3s3
2025-09-03 10:52:54,063 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,065 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:45979
2025-09-03 10:52:54,065 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:45979
2025-09-03 10:52:54,065 - distributed.worker - INFO -          dashboard at:          10.6.105.21:37253
2025-09-03 10:52:54,065 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,065 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,065 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,065 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,065 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-vrvej7_d
2025-09-03 10:52:54,065 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,132 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:42003'
2025-09-03 10:52:54,136 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:46769'
2025-09-03 10:52:54,141 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:41523'
2025-09-03 10:52:54,146 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:37271'
2025-09-03 10:52:54,150 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:41251'
2025-09-03 10:52:54,156 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:46671'
2025-09-03 10:52:54,164 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:42259'
2025-09-03 10:52:54,168 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:34825'
2025-09-03 10:52:54,173 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:44751'
2025-09-03 10:52:54,176 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:45715'
2025-09-03 10:52:54,181 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:39247'
2025-09-03 10:52:54,189 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:37611'
2025-09-03 10:52:54,192 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:42317'
2025-09-03 10:52:54,273 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:35787'
2025-09-03 10:52:54,278 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:37069'
2025-09-03 10:52:54,284 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:34143'
2025-09-03 10:52:54,289 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:34181'
2025-09-03 10:52:54,292 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:42523'
2025-09-03 10:52:54,298 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:39611'
2025-09-03 10:52:54,302 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:44767'
2025-09-03 10:52:54,307 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:36301'
2025-09-03 10:52:54,312 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:35425'
2025-09-03 10:52:54,316 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:39437'
2025-09-03 10:52:54,322 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:33103'
2025-09-03 10:52:54,326 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:34851'
2025-09-03 10:52:54,333 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:41295'
2025-09-03 10:52:54,338 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:39085'
2025-09-03 10:52:54,343 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:36079'
2025-09-03 10:52:54,347 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:35925'
2025-09-03 10:52:54,353 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:35041'
2025-09-03 10:52:54,357 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:37653'
2025-09-03 10:52:54,362 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:41631'
2025-09-03 10:52:54,366 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:41749'
2025-09-03 10:52:54,372 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:40843'
2025-09-03 10:52:54,377 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:39127'
2025-09-03 10:52:54,383 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:37023'
2025-09-03 10:52:54,388 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:41659'
2025-09-03 10:52:54,392 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:36299'
2025-09-03 10:52:54,397 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:36151'
2025-09-03 10:52:54,402 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:35947'
2025-09-03 10:52:54,404 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:34561
2025-09-03 10:52:54,404 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:34561
2025-09-03 10:52:54,404 - distributed.worker - INFO -          dashboard at:          10.6.105.21:41131
2025-09-03 10:52:54,404 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,404 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,404 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,404 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,404 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-kv38dnfx
2025-09-03 10:52:54,405 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,407 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:41179'
2025-09-03 10:52:54,407 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:40007
2025-09-03 10:52:54,407 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:40007
2025-09-03 10:52:54,407 - distributed.worker - INFO -          dashboard at:          10.6.105.21:40859
2025-09-03 10:52:54,407 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,407 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,407 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,407 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,407 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-cq8856pg
2025-09-03 10:52:54,407 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,412 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:35329'
2025-09-03 10:52:54,415 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:46219'
2025-09-03 10:52:54,417 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:42503'
2025-09-03 10:52:54,419 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:40267'
2025-09-03 10:52:54,422 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:37869'
2025-09-03 10:52:54,426 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:34731'
2025-09-03 10:52:54,431 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:42113'
2025-09-03 10:52:54,434 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:43779'
2025-09-03 10:52:54,440 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:37729'
2025-09-03 10:52:54,451 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:45275'
2025-09-03 10:52:54,453 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:40269
2025-09-03 10:52:54,453 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:40269
2025-09-03 10:52:54,453 - distributed.worker - INFO -          dashboard at:          10.6.105.21:35109
2025-09-03 10:52:54,453 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,453 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,453 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,453 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,453 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-cavp6hiu
2025-09-03 10:52:54,454 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,456 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:37439'
2025-09-03 10:52:54,461 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:41115'
2025-09-03 10:52:54,465 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:37059'
2025-09-03 10:52:54,474 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:39937
2025-09-03 10:52:54,474 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:39937
2025-09-03 10:52:54,474 - distributed.worker - INFO -          dashboard at:          10.6.105.21:41467
2025-09-03 10:52:54,474 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,474 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,474 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,474 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,474 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ugmfo9pp
2025-09-03 10:52:54,474 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,482 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:33109
2025-09-03 10:52:54,482 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:33109
2025-09-03 10:52:54,482 - distributed.worker - INFO -          dashboard at:          10.6.105.21:44501
2025-09-03 10:52:54,482 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,482 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,482 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,482 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,482 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-09wlojo9
2025-09-03 10:52:54,482 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,485 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:39629'
2025-09-03 10:52:54,490 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:33311'
2025-09-03 10:52:54,492 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:44189'
2025-09-03 10:52:54,494 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:35087'
2025-09-03 10:52:54,497 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:40651'
2025-09-03 10:52:54,498 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:35799'
2025-09-03 10:52:54,499 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:45369
2025-09-03 10:52:54,499 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:45369
2025-09-03 10:52:54,499 - distributed.worker - INFO -          dashboard at:          10.6.105.21:43891
2025-09-03 10:52:54,499 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,499 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,499 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,499 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,499 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-z2nquijz
2025-09-03 10:52:54,499 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,502 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:45921
2025-09-03 10:52:54,502 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:45921
2025-09-03 10:52:54,502 - distributed.worker - INFO -          dashboard at:          10.6.105.21:34951
2025-09-03 10:52:54,502 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,502 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,502 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,502 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,502 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-6z5vpi96
2025-09-03 10:52:54,502 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,504 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:42151'
2025-09-03 10:52:54,516 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:33919
2025-09-03 10:52:54,516 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:33919
2025-09-03 10:52:54,516 - distributed.worker - INFO -          dashboard at:          10.6.105.21:32803
2025-09-03 10:52:54,516 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,516 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,516 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,516 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,516 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-kpdiexww
2025-09-03 10:52:54,516 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,529 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:46017
2025-09-03 10:52:54,529 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:46017
2025-09-03 10:52:54,529 - distributed.worker - INFO -          dashboard at:          10.6.105.21:38759
2025-09-03 10:52:54,529 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,529 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,530 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,530 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,530 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-yp29mcds
2025-09-03 10:52:54,530 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,531 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:33063
2025-09-03 10:52:54,531 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:33063
2025-09-03 10:52:54,531 - distributed.worker - INFO -          dashboard at:          10.6.105.21:42597
2025-09-03 10:52:54,531 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,531 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,531 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,531 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,531 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-vj2ta8y0
2025-09-03 10:52:54,531 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,533 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:41639
2025-09-03 10:52:54,533 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:41639
2025-09-03 10:52:54,534 - distributed.worker - INFO -          dashboard at:          10.6.105.21:40503
2025-09-03 10:52:54,534 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,534 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,534 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,534 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,534 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-c8tkzh6i
2025-09-03 10:52:54,534 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,535 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:41503
2025-09-03 10:52:54,535 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:41503
2025-09-03 10:52:54,535 - distributed.worker - INFO -          dashboard at:          10.6.105.21:35297
2025-09-03 10:52:54,535 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:54,535 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:54,535 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:54,535 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:54,535 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-0cgl9ts4
2025-09-03 10:52:54,535 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,030 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:33281
2025-09-03 10:52:55,030 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:33281
2025-09-03 10:52:55,030 - distributed.worker - INFO -          dashboard at:          10.6.105.21:37893
2025-09-03 10:52:55,030 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,030 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,030 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,030 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,030 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-6_rsdbu2
2025-09-03 10:52:55,030 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,364 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:38001
2025-09-03 10:52:55,364 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:38001
2025-09-03 10:52:55,364 - distributed.worker - INFO -          dashboard at:          10.6.105.21:40325
2025-09-03 10:52:55,364 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,364 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,364 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,364 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,364 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-mzi6mzxe
2025-09-03 10:52:55,364 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,368 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:34307
2025-09-03 10:52:55,368 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:34307
2025-09-03 10:52:55,368 - distributed.worker - INFO -          dashboard at:          10.6.105.21:38071
2025-09-03 10:52:55,368 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,368 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,368 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,368 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,368 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-r84v43vo
2025-09-03 10:52:55,368 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,373 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:55,373 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:35027
2025-09-03 10:52:55,373 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:35027
2025-09-03 10:52:55,373 - distributed.worker - INFO -          dashboard at:          10.6.105.21:36273
2025-09-03 10:52:55,373 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,373 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,373 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,373 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,373 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ratzxo16
2025-09-03 10:52:55,373 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,374 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,374 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,376 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:55,395 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:34359
2025-09-03 10:52:55,395 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:34359
2025-09-03 10:52:55,395 - distributed.worker - INFO -          dashboard at:          10.6.105.21:44577
2025-09-03 10:52:55,395 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,395 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,395 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,395 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,395 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-storqex1
2025-09-03 10:52:55,396 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,396 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:37541
2025-09-03 10:52:55,397 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:37541
2025-09-03 10:52:55,397 - distributed.worker - INFO -          dashboard at:          10.6.105.21:38043
2025-09-03 10:52:55,397 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,397 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,397 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,397 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,397 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-9n1tllgi
2025-09-03 10:52:55,397 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,405 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:55,406 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,406 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,408 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:55,412 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:41203
2025-09-03 10:52:55,412 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:41203
2025-09-03 10:52:55,412 - distributed.worker - INFO -          dashboard at:          10.6.105.21:45315
2025-09-03 10:52:55,412 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,412 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,412 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,412 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,412 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-hvuzw_wc
2025-09-03 10:52:55,412 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,421 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:46663
2025-09-03 10:52:55,422 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:46663
2025-09-03 10:52:55,422 - distributed.worker - INFO -          dashboard at:          10.6.105.21:39689
2025-09-03 10:52:55,422 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,422 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,422 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,422 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,422 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-7dr8sm_7
2025-09-03 10:52:55,422 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:55,422 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,423 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,423 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,425 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:55,438 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:55,439 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,439 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,440 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:55,443 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:37903
2025-09-03 10:52:55,443 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:37903
2025-09-03 10:52:55,443 - distributed.worker - INFO -          dashboard at:          10.6.105.21:43035
2025-09-03 10:52:55,443 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,443 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,443 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,443 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,443 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-k0cyo2_o
2025-09-03 10:52:55,443 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,458 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:41673
2025-09-03 10:52:55,458 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:41673
2025-09-03 10:52:55,458 - distributed.worker - INFO -          dashboard at:          10.6.105.21:43235
2025-09-03 10:52:55,458 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,458 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,458 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,458 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,458 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-mhsinjnt
2025-09-03 10:52:55,458 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,463 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:41073
2025-09-03 10:52:55,463 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:41073
2025-09-03 10:52:55,463 - distributed.worker - INFO -          dashboard at:          10.6.105.21:39615
2025-09-03 10:52:55,463 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,463 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,463 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,463 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,463 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-c6kkg1wc
2025-09-03 10:52:55,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,476 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:35243
2025-09-03 10:52:55,476 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:35243
2025-09-03 10:52:55,476 - distributed.worker - INFO -          dashboard at:          10.6.105.21:35353
2025-09-03 10:52:55,476 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,476 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,476 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,476 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,476 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ekmw84nm
2025-09-03 10:52:55,476 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,486 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:43637
2025-09-03 10:52:55,486 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:43637
2025-09-03 10:52:55,486 - distributed.worker - INFO -          dashboard at:          10.6.105.21:33323
2025-09-03 10:52:55,486 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,487 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,487 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,487 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,487 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-346xbqcd
2025-09-03 10:52:55,487 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,487 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:55,488 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,489 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,490 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:55,562 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:36393
2025-09-03 10:52:55,562 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:36393
2025-09-03 10:52:55,562 - distributed.worker - INFO -          dashboard at:          10.6.105.21:43101
2025-09-03 10:52:55,562 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,562 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,562 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,562 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,562 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ragklk01
2025-09-03 10:52:55,562 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,567 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:37255
2025-09-03 10:52:55,567 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:37255
2025-09-03 10:52:55,567 - distributed.worker - INFO -          dashboard at:          10.6.105.21:33005
2025-09-03 10:52:55,567 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,567 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,567 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,567 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,567 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-avgvpe7t
2025-09-03 10:52:55,567 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,624 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:37261
2025-09-03 10:52:55,624 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:37261
2025-09-03 10:52:55,624 - distributed.worker - INFO -          dashboard at:          10.6.105.21:34687
2025-09-03 10:52:55,624 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,624 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,624 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,624 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,624 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-25ovpsff
2025-09-03 10:52:55,624 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,630 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:41341
2025-09-03 10:52:55,630 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:41341
2025-09-03 10:52:55,630 - distributed.worker - INFO -          dashboard at:          10.6.105.21:38941
2025-09-03 10:52:55,630 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,630 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,630 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,630 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,630 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-b9xqb_n9
2025-09-03 10:52:55,630 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,706 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:34111
2025-09-03 10:52:55,706 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:34111
2025-09-03 10:52:55,706 - distributed.worker - INFO -          dashboard at:          10.6.105.21:34461
2025-09-03 10:52:55,706 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,706 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,706 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,706 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,706 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ofry98lc
2025-09-03 10:52:55,706 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,972 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:33765
2025-09-03 10:52:55,972 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:33765
2025-09-03 10:52:55,972 - distributed.worker - INFO -          dashboard at:          10.6.105.21:34519
2025-09-03 10:52:55,972 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,972 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,972 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,972 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,972 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-dvlfrfsk
2025-09-03 10:52:55,972 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,978 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:45065
2025-09-03 10:52:55,978 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:45065
2025-09-03 10:52:55,978 - distributed.worker - INFO -          dashboard at:          10.6.105.21:43669
2025-09-03 10:52:55,978 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,978 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,978 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,978 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,979 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-yhlwy7gg
2025-09-03 10:52:55,979 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,988 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:40061
2025-09-03 10:52:55,989 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:40061
2025-09-03 10:52:55,989 - distributed.worker - INFO -          dashboard at:          10.6.105.21:45085
2025-09-03 10:52:55,989 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,989 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,989 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,989 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,989 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-28ai2xn4
2025-09-03 10:52:55,989 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,998 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:44485
2025-09-03 10:52:55,998 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:44485
2025-09-03 10:52:55,998 - distributed.worker - INFO -          dashboard at:          10.6.105.21:42697
2025-09-03 10:52:55,998 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:55,998 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:55,999 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:55,999 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:55,999 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-hpb87sfg
2025-09-03 10:52:55,999 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,000 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:44889
2025-09-03 10:52:56,000 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:44889
2025-09-03 10:52:56,000 - distributed.worker - INFO -          dashboard at:          10.6.105.21:35855
2025-09-03 10:52:56,000 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,000 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,000 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,000 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,000 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-nzkm0k7m
2025-09-03 10:52:56,000 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,000 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:41915
2025-09-03 10:52:56,000 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:41915
2025-09-03 10:52:56,000 - distributed.worker - INFO -          dashboard at:          10.6.105.21:36995
2025-09-03 10:52:56,001 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,001 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,001 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,001 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,001 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-lu82e_6a
2025-09-03 10:52:56,001 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,005 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:43909
2025-09-03 10:52:56,005 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:43909
2025-09-03 10:52:56,005 - distributed.worker - INFO -          dashboard at:          10.6.105.21:37323
2025-09-03 10:52:56,005 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,005 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,005 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,005 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,005 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-fyql09iu
2025-09-03 10:52:56,005 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,014 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:40781
2025-09-03 10:52:56,014 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:40781
2025-09-03 10:52:56,014 - distributed.worker - INFO -          dashboard at:          10.6.105.21:32859
2025-09-03 10:52:56,014 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,014 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,014 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,014 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,014 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-qziamc87
2025-09-03 10:52:56,014 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,015 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:41831
2025-09-03 10:52:56,015 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:41831
2025-09-03 10:52:56,015 - distributed.worker - INFO -          dashboard at:          10.6.105.21:35919
2025-09-03 10:52:56,016 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,016 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,016 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,016 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,016 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-zcq2t2uk
2025-09-03 10:52:56,016 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,017 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:44891
2025-09-03 10:52:56,017 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:44891
2025-09-03 10:52:56,017 - distributed.worker - INFO -          dashboard at:          10.6.105.21:43311
2025-09-03 10:52:56,017 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,017 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,017 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,018 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,018 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-csuvu5m7
2025-09-03 10:52:56,018 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,020 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:35959
2025-09-03 10:52:56,020 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:35959
2025-09-03 10:52:56,020 - distributed.worker - INFO -          dashboard at:          10.6.105.21:45073
2025-09-03 10:52:56,020 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,020 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,020 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,020 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,020 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-5p59_c82
2025-09-03 10:52:56,020 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,021 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:41789
2025-09-03 10:52:56,022 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:41789
2025-09-03 10:52:56,022 - distributed.worker - INFO -          dashboard at:          10.6.105.21:44137
2025-09-03 10:52:56,022 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,022 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,022 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,022 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,022 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-4h2o6a9t
2025-09-03 10:52:56,022 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,023 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:36941
2025-09-03 10:52:56,023 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:36941
2025-09-03 10:52:56,023 - distributed.worker - INFO -          dashboard at:          10.6.105.21:42969
2025-09-03 10:52:56,023 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,023 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,024 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,024 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,024 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-nt7vc6j3
2025-09-03 10:52:56,024 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,029 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:37087
2025-09-03 10:52:56,029 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:37087
2025-09-03 10:52:56,029 - distributed.worker - INFO -          dashboard at:          10.6.105.21:40043
2025-09-03 10:52:56,030 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,030 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,030 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,030 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,030 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-njbgp4ls
2025-09-03 10:52:56,030 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,031 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:36405
2025-09-03 10:52:56,031 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:36405
2025-09-03 10:52:56,031 - distributed.worker - INFO -          dashboard at:          10.6.105.21:34959
2025-09-03 10:52:56,032 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,032 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,032 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,032 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,032 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-f1lc1ckx
2025-09-03 10:52:56,032 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,033 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:40127
2025-09-03 10:52:56,033 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:40127
2025-09-03 10:52:56,034 - distributed.worker - INFO -          dashboard at:          10.6.105.21:39569
2025-09-03 10:52:56,034 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,034 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,034 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,034 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,034 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-atf_5f3q
2025-09-03 10:52:56,034 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,037 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:40805
2025-09-03 10:52:56,037 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:40805
2025-09-03 10:52:56,037 - distributed.worker - INFO -          dashboard at:          10.6.105.21:44213
2025-09-03 10:52:56,037 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,037 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,037 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,037 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,037 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-dm6dqlzv
2025-09-03 10:52:56,038 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,039 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:41901
2025-09-03 10:52:56,039 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:41901
2025-09-03 10:52:56,039 - distributed.worker - INFO -          dashboard at:          10.6.105.21:41191
2025-09-03 10:52:56,039 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,039 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,039 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,039 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,039 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:41979
2025-09-03 10:52:56,039 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-pcogf0zu
2025-09-03 10:52:56,039 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:41979
2025-09-03 10:52:56,039 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,039 - distributed.worker - INFO -          dashboard at:          10.6.105.21:42351
2025-09-03 10:52:56,039 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,039 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,039 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,039 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,039 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:36209
2025-09-03 10:52:56,039 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-sj3xz8wm
2025-09-03 10:52:56,039 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:36209
2025-09-03 10:52:56,039 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,039 - distributed.worker - INFO -          dashboard at:          10.6.105.21:40219
2025-09-03 10:52:56,039 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,039 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,039 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,039 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,039 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-0s4eyihu
2025-09-03 10:52:56,039 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,042 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:46833
2025-09-03 10:52:56,042 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:46833
2025-09-03 10:52:56,042 - distributed.worker - INFO -          dashboard at:          10.6.105.21:34347
2025-09-03 10:52:56,042 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,042 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,042 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,042 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,042 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-694quppw
2025-09-03 10:52:56,042 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,044 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:44599
2025-09-03 10:52:56,044 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:44599
2025-09-03 10:52:56,045 - distributed.worker - INFO -          dashboard at:          10.6.105.21:45483
2025-09-03 10:52:56,045 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,045 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,045 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,045 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,045 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-wgk1jxe5
2025-09-03 10:52:56,045 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,046 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:46147
2025-09-03 10:52:56,046 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:46147
2025-09-03 10:52:56,046 - distributed.worker - INFO -          dashboard at:          10.6.105.21:39411
2025-09-03 10:52:56,046 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,046 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,046 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,046 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,047 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-wddawzc2
2025-09-03 10:52:56,047 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,048 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:34257
2025-09-03 10:52:56,048 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:34257
2025-09-03 10:52:56,048 - distributed.worker - INFO -          dashboard at:          10.6.105.21:42535
2025-09-03 10:52:56,049 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,049 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,049 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,049 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,049 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-67wu3c6r
2025-09-03 10:52:56,049 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,049 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:46829
2025-09-03 10:52:56,050 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:46829
2025-09-03 10:52:56,050 - distributed.worker - INFO -          dashboard at:          10.6.105.21:46707
2025-09-03 10:52:56,050 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,050 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,050 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,050 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,050 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-uscdokew
2025-09-03 10:52:56,050 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,050 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:44357
2025-09-03 10:52:56,050 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:44357
2025-09-03 10:52:56,050 - distributed.worker - INFO -          dashboard at:          10.6.105.21:43013
2025-09-03 10:52:56,050 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,050 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,050 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,050 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,050 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-0_xqnd0k
2025-09-03 10:52:56,050 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,053 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:35667
2025-09-03 10:52:56,053 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:35667
2025-09-03 10:52:56,053 - distributed.worker - INFO -          dashboard at:          10.6.105.21:44461
2025-09-03 10:52:56,053 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,053 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,053 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,053 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,053 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ke0oc7j3
2025-09-03 10:52:56,053 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,054 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:34803
2025-09-03 10:52:56,054 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:34803
2025-09-03 10:52:56,055 - distributed.worker - INFO -          dashboard at:          10.6.105.21:46417
2025-09-03 10:52:56,055 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,055 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,055 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,055 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,055 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-590olnxb
2025-09-03 10:52:56,055 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,056 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:40563
2025-09-03 10:52:56,056 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:40563
2025-09-03 10:52:56,056 - distributed.worker - INFO -          dashboard at:          10.6.105.21:36177
2025-09-03 10:52:56,056 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,056 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,056 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,056 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,056 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-v5cu6zj9
2025-09-03 10:52:56,056 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,060 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:43571
2025-09-03 10:52:56,060 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:43571
2025-09-03 10:52:56,060 - distributed.worker - INFO -          dashboard at:          10.6.105.21:34289
2025-09-03 10:52:56,060 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,060 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,060 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,060 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,060 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-hizc40gm
2025-09-03 10:52:56,060 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,060 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:39737
2025-09-03 10:52:56,061 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:39737
2025-09-03 10:52:56,061 - distributed.worker - INFO -          dashboard at:          10.6.105.21:37629
2025-09-03 10:52:56,061 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,061 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,061 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,061 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,061 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-9kz9eaod
2025-09-03 10:52:56,061 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,062 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:44549
2025-09-03 10:52:56,063 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:44549
2025-09-03 10:52:56,063 - distributed.worker - INFO -          dashboard at:          10.6.105.21:40947
2025-09-03 10:52:56,063 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,063 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,063 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,063 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,063 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-k3p8rqw4
2025-09-03 10:52:56,063 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,063 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:39543
2025-09-03 10:52:56,063 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:39543
2025-09-03 10:52:56,063 - distributed.worker - INFO -          dashboard at:          10.6.105.21:46455
2025-09-03 10:52:56,063 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,063 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,063 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,063 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,063 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-gdmv95mw
2025-09-03 10:52:56,063 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,064 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:45673
2025-09-03 10:52:56,064 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:45673
2025-09-03 10:52:56,064 - distributed.worker - INFO -          dashboard at:          10.6.105.21:43557
2025-09-03 10:52:56,064 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,064 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,064 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,064 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,064 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-b86bi1nm
2025-09-03 10:52:56,064 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,068 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:35437
2025-09-03 10:52:56,068 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:35437
2025-09-03 10:52:56,068 - distributed.worker - INFO -          dashboard at:          10.6.105.21:39209
2025-09-03 10:52:56,068 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,068 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,068 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:36859
2025-09-03 10:52:56,068 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,068 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,068 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:36859
2025-09-03 10:52:56,068 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-lvrb_sub
2025-09-03 10:52:56,068 - distributed.worker - INFO -          dashboard at:          10.6.105.21:43639
2025-09-03 10:52:56,068 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,068 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,068 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,068 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,068 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,068 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-pd7vjxpb
2025-09-03 10:52:56,068 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,070 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:34717
2025-09-03 10:52:56,070 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:34717
2025-09-03 10:52:56,071 - distributed.worker - INFO -          dashboard at:          10.6.105.21:37575
2025-09-03 10:52:56,070 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:45525
2025-09-03 10:52:56,071 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,071 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,071 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:45525
2025-09-03 10:52:56,071 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,071 - distributed.worker - INFO -          dashboard at:          10.6.105.21:46315
2025-09-03 10:52:56,071 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,071 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,071 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-9hmshje6
2025-09-03 10:52:56,071 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,071 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,071 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,071 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,071 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-i3rjtee4
2025-09-03 10:52:56,071 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,071 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:34109
2025-09-03 10:52:56,071 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:34109
2025-09-03 10:52:56,071 - distributed.worker - INFO -          dashboard at:          10.6.105.21:40103
2025-09-03 10:52:56,071 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,071 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,071 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,071 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,071 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-tzjuymj2
2025-09-03 10:52:56,071 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,071 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:42105
2025-09-03 10:52:56,071 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:42105
2025-09-03 10:52:56,071 - distributed.worker - INFO -          dashboard at:          10.6.105.21:38513
2025-09-03 10:52:56,071 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,071 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,071 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,071 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,071 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-jelkjncf
2025-09-03 10:52:56,071 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,074 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:40811
2025-09-03 10:52:56,074 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:40811
2025-09-03 10:52:56,074 - distributed.worker - INFO -          dashboard at:          10.6.105.21:38353
2025-09-03 10:52:56,074 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,074 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,074 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,074 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,074 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-nzc7p78a
2025-09-03 10:52:56,074 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,074 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:34081
2025-09-03 10:52:56,074 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:34081
2025-09-03 10:52:56,074 - distributed.worker - INFO -          dashboard at:          10.6.105.21:46259
2025-09-03 10:52:56,075 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,075 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,075 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,075 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,075 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-ph4aso35
2025-09-03 10:52:56,075 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,075 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:42279
2025-09-03 10:52:56,076 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:42279
2025-09-03 10:52:56,076 - distributed.worker - INFO -          dashboard at:          10.6.105.21:36617
2025-09-03 10:52:56,076 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:56,076 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:56,076 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:56,076 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:56,076 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-u60mqq_e
2025-09-03 10:52:56,076 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,438 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:34391'
2025-09-03 10:52:57,444 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:37259'
2025-09-03 10:52:57,448 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:36351'
2025-09-03 10:52:57,453 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:36937'
2025-09-03 10:52:57,457 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:38835'
2025-09-03 10:52:57,460 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:45191'
2025-09-03 10:52:57,465 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:35851'
2025-09-03 10:52:57,470 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:46647'
2025-09-03 10:52:57,478 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:34735'
2025-09-03 10:52:57,485 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:40533'
2025-09-03 10:52:57,489 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,490 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,490 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,492 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,491 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:37283'
2025-09-03 10:52:57,498 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:42219'
2025-09-03 10:52:57,503 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:39829'
2025-09-03 10:52:57,509 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.105.21:38769'
2025-09-03 10:52:57,555 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,556 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,556 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,557 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,900 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,901 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,902 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,903 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,917 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,918 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,918 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,920 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,933 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,934 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,935 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,936 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:57,950 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:57,951 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:57,951 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:57,953 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,182 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,183 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,184 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,185 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,249 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,250 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,250 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,252 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,277 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:33523
2025-09-03 10:52:58,277 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:33523
2025-09-03 10:52:58,277 - distributed.worker - INFO -          dashboard at:          10.6.105.21:38505
2025-09-03 10:52:58,277 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,277 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,277 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,277 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,277 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-n9pl6qd8
2025-09-03 10:52:58,277 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,280 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:33715
2025-09-03 10:52:58,280 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:33715
2025-09-03 10:52:58,280 - distributed.worker - INFO -          dashboard at:          10.6.105.21:38445
2025-09-03 10:52:58,280 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,280 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,280 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,280 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,280 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-95_5g8gm
2025-09-03 10:52:58,280 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,292 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:43937
2025-09-03 10:52:58,292 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:43937
2025-09-03 10:52:58,292 - distributed.worker - INFO -          dashboard at:          10.6.105.21:41611
2025-09-03 10:52:58,292 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,292 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,292 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,292 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,292 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-qy25r77l
2025-09-03 10:52:58,292 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,296 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:38481
2025-09-03 10:52:58,296 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:38481
2025-09-03 10:52:58,296 - distributed.worker - INFO -          dashboard at:          10.6.105.21:37273
2025-09-03 10:52:58,296 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,296 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,297 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,297 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,297 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-1do2smc4
2025-09-03 10:52:58,297 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,307 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:34249
2025-09-03 10:52:58,308 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:34249
2025-09-03 10:52:58,308 - distributed.worker - INFO -          dashboard at:          10.6.105.21:37363
2025-09-03 10:52:58,308 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,308 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,308 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,308 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-a3ini2cs
2025-09-03 10:52:58,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,338 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:37207
2025-09-03 10:52:58,338 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:37207
2025-09-03 10:52:58,338 - distributed.worker - INFO -          dashboard at:          10.6.105.21:38235
2025-09-03 10:52:58,338 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,338 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,338 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,338 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,338 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-sox1tukx
2025-09-03 10:52:58,338 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,356 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:45053
2025-09-03 10:52:58,356 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:45053
2025-09-03 10:52:58,356 - distributed.worker - INFO -          dashboard at:          10.6.105.21:36145
2025-09-03 10:52:58,356 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,356 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,356 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,357 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,357 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-9ydvzedb
2025-09-03 10:52:58,357 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,371 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:45807
2025-09-03 10:52:58,371 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:45807
2025-09-03 10:52:58,371 - distributed.worker - INFO -          dashboard at:          10.6.105.21:46709
2025-09-03 10:52:58,371 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,371 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,371 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,371 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,372 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-2z3za4dt
2025-09-03 10:52:58,372 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,378 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:35231
2025-09-03 10:52:58,378 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:35231
2025-09-03 10:52:58,378 - distributed.worker - INFO -          dashboard at:          10.6.105.21:36135
2025-09-03 10:52:58,378 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,378 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,378 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,378 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,378 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-0n88_g15
2025-09-03 10:52:58,378 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,381 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:40257
2025-09-03 10:52:58,381 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:40257
2025-09-03 10:52:58,381 - distributed.worker - INFO -          dashboard at:          10.6.105.21:34811
2025-09-03 10:52:58,381 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,381 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,381 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,381 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,381 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-0ha6tx17
2025-09-03 10:52:58,381 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,381 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:38837
2025-09-03 10:52:58,381 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:38837
2025-09-03 10:52:58,381 - distributed.worker - INFO -          dashboard at:          10.6.105.21:45847
2025-09-03 10:52:58,381 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,381 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,381 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,381 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,381 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-__l97loz
2025-09-03 10:52:58,381 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,395 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:38479
2025-09-03 10:52:58,396 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:38479
2025-09-03 10:52:58,396 - distributed.worker - INFO -          dashboard at:          10.6.105.21:42519
2025-09-03 10:52:58,396 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,396 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,396 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,396 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,396 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-8jwf5f5t
2025-09-03 10:52:58,396 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,408 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:46271
2025-09-03 10:52:58,408 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:46271
2025-09-03 10:52:58,408 - distributed.worker - INFO -          dashboard at:          10.6.105.21:43291
2025-09-03 10:52:58,408 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,408 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,408 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,408 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-botubb40
2025-09-03 10:52:58,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,412 - distributed.worker - INFO -       Start worker at:    tcp://10.6.105.21:42643
2025-09-03 10:52:58,412 - distributed.worker - INFO -          Listening to:    tcp://10.6.105.21:42643
2025-09-03 10:52:58,412 - distributed.worker - INFO -          dashboard at:          10.6.105.21:38777
2025-09-03 10:52:58,412 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,412 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,412 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:52:58,412 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:52:58,412 - distributed.worker - INFO -       Local Directory: /jobfs/148607956.gadi-pbs/dask-scratch-space/worker-he4hatf7
2025-09-03 10:52:58,412 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,722 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,723 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,723 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,725 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,738 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,740 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,740 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,741 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,755 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,757 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,757 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,759 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,772 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,773 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,773 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,775 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:52:58,789 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:52:58,790 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:52:58,790 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:52:58,792 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:00,240 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:00,241 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:00,241 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:00,243 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:00,257 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:00,258 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:00,259 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:00,260 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:00,274 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:00,275 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:00,275 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:00,277 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:00,291 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:00,292 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:00,292 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:00,294 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:00,308 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:00,310 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:00,310 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:00,312 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:01,138 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:01,140 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,140 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,142 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:01,223 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:01,224 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,224 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,226 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:01,239 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:01,241 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,241 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,242 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:01,256 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:01,257 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,257 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,259 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:01,272 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:01,273 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,274 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,275 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:01,289 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:01,290 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,290 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,292 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:01,305 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:01,307 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,307 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,309 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:01,776 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:01,777 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,777 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,779 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:01,793 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:01,794 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,794 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,796 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:01,809 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:01,810 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,811 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,813 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:01,826 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:01,827 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,827 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,829 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:01,842 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:01,844 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,844 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,846 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:01,859 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:01,860 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,860 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,862 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:01,876 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:01,877 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:01,877 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:01,879 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:04,721 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:04,723 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:04,723 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:04,724 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:04,738 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:04,739 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:04,740 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:04,741 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:04,755 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:04,756 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:04,756 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:04,758 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:04,772 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:04,773 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:04,773 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:04,775 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:09,299 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:09,301 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:09,301 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:09,303 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:11,516 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:11,518 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:11,518 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:11,520 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:11,533 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:11,535 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:11,536 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:11,538 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:12,036 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:12,037 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:12,038 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:12,039 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:12,052 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:12,054 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:12,054 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:12,055 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:12,070 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:12,072 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:12,072 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:12,073 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:12,087 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:12,089 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:12,089 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:12,091 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:12,104 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:12,106 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:12,106 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:12,108 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:12,122 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:12,123 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:12,124 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:12,125 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:12,139 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:12,141 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:12,141 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:12,143 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:12,156 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:12,157 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:12,157 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:12,159 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:12,173 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:12,174 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:12,175 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:12,176 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:12,190 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:12,192 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:12,192 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:12,194 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:12,207 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:12,209 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:12,209 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:12,211 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:12,224 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:12,226 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:12,226 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:12,227 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:12,242 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:12,244 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:12,244 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:12,245 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:12,259 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:12,261 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:12,261 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:12,263 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:12,276 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:12,277 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:12,277 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:12,279 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:12,293 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:12,294 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:12,294 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:12,296 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:12,310 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:12,311 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:12,311 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:12,313 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:12,327 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:12,329 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:12,329 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:12,331 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:12,345 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:12,346 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:12,346 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:12,348 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:12,361 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:12,363 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:12,363 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:12,364 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:12,379 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:12,381 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:12,381 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:12,382 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:12,395 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:12,397 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:12,397 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:12,398 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:12,413 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:12,414 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:12,414 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:12,416 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:12,430 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:12,432 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:12,432 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:12,434 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:12,447 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:12,449 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:12,449 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:12,450 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:12,465 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:12,466 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:12,466 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:12,468 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:12,482 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:12,483 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:12,483 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:12,485 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:12,498 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:12,500 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:12,500 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:12,501 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:12,516 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:12,517 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:12,517 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:12,519 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:19,697 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:19,699 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:19,699 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:19,701 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:19,715 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:19,716 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:19,716 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:19,718 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:19,732 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:19,734 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:19,734 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:19,736 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:24,455 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:24,475 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:24,484 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:24,500 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:24,504 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,032 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,413 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,423 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,444 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,459 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,465 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,477 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,488 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,707 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,973 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,980 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:25,991 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:26,000 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:26,001 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:26,002 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:26,006 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:26,016 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:26,017 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:26,019 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:26,021 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:26,023 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:26,025 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:26,031 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.101.5:8785
2025-09-03 10:53:26,443 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:26,492 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:28,374 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:28,376 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:28,376 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:28,377 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:28,559 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:28,922 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:28,940 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:28,956 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:29,845 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:29,847 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:29,847 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:29,849 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:29,863 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:29,865 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:29,865 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:29,867 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:29,881 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:29,882 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:29,883 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:29,884 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:29,900 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:29,902 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:29,902 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:29,903 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:29,918 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:29,919 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:29,920 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:29,921 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:32,144 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:33,970 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:33,971 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:33,971 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:33,973 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:33,987 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:33,989 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:33,989 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:33,990 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:34,004 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:34,006 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:34,006 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:34,008 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:34,606 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:34,607 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:34,607 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:34,609 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:34,623 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:34,625 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:34,625 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:34,626 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:34,641 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:34,642 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:34,643 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:34,644 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:34,658 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:34,659 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:34,660 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:34,661 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:40,305 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:43,163 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:43,180 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:43,195 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:43,212 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:43,231 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:43,248 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:43,265 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:43,283 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:43,299 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:43,315 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:43,333 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:43,353 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:43,368 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:43,385 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:43,402 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:43,418 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:43,436 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:43,453 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:43,472 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:43,488 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:43,505 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:43,522 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.101.5:8785 after 30 s
2025-09-03 10:53:47,765 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:47,766 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:47,766 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:47,768 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:47,783 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:47,784 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:47,784 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:47,786 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:47,801 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:47,802 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:47,802 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:47,804 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:47,819 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:47,820 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:47,821 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:47,822 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:47,837 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:47,838 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:47,839 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:47,840 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:47,855 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:47,856 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:47,856 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:47,858 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:47,873 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:47,874 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:47,874 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:47,876 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:47,890 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:47,892 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:47,892 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:47,894 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:47,908 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:47,910 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:47,910 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:47,912 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:47,926 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:47,928 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:47,928 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:47,930 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:47,944 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:47,945 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:47,946 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:47,947 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:47,961 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:47,962 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:47,963 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:47,964 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:47,979 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:47,981 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:47,981 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:47,982 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:47,996 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:47,998 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:47,998 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:47,999 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:53:48,014 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:53:48,015 - distributed.worker - INFO -         Registered to:      tcp://10.6.101.5:8785
2025-09-03 10:53:48,015 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:53:48,017 - distributed.core - INFO - Starting established connection to tcp://10.6.101.5:8785
2025-09-03 10:56:37,882 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:37,883 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,269 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,271 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,287 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,292 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,467 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,472 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,480 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,482 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,690 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,696 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,697 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,700 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,709 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,714 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:38,754 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:38,755 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,382 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,385 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,544 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,545 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,606 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,608 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,654 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,657 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:39,938 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:39,943 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,216 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,217 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:40,526 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:40,528 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,021 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,024 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,063 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,071 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,261 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,263 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,270 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,275 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,306 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,316 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,323 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,324 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:41,933 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:41,936 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,327 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,328 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,397 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,402 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,551 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,552 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,620 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,626 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:42,851 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:42,853 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,138 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,139 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,567 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,573 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,685 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,687 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,852 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,853 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:43,971 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:43,974 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,060 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,066 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,259 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,261 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,562 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,564 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,566 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,575 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:44,893 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:44,894 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,194 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,204 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,383 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,384 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,514 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,515 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,522 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,527 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,584 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,589 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:45,708 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:45,710 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,383 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,384 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:46,951 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:46,952 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,168 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,169 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,175 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,176 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,185 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,186 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,206 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,207 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,351 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,352 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,378 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,379 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,408 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,413 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,504 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,505 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:47,958 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:47,963 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,197 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,198 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,577 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,579 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:48,920 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:48,924 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,173 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,176 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,348 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,350 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,395 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,397 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,625 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,629 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,630 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,631 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:49,820 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:49,822 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,029 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,032 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,246 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,247 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,246 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,252 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,267 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,272 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,442 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,444 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,512 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,515 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,781 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,783 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:50,897 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:50,899 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,114 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,115 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,399 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,405 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,413 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,414 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,688 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,693 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,765 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,770 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:51,906 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:51,911 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,130 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,132 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,300 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,302 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,459 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,461 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:52,984 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:52,989 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:53,595 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:53,596 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:53,679 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:53,681 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:54,054 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:54,059 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:54,203 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:54,204 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:54,759 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:54,761 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:54,878 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:54,879 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:55,013 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:55,020 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:55,020 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:55,022 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:55,388 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:55,389 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:55,513 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:55,520 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:56,419 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:56,424 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:56,831 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:56,832 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:56:57,332 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:56:57,337 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:00,442 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:00,444 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:00,505 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:00,507 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:01,212 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:01,217 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:01,708 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:01,710 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:03,054 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:03,056 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:03,142 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:03,145 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:03,394 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:03,399 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:03,905 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:03,906 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:05,194 - distributed.worker - INFO - Starting Worker plugin qme_utils.py99b60261-15ef-4b08-86b6-cf8026baba29
2025-09-03 10:57:05,199 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:57:09,989 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:09,992 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,002 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,005 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,009 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,011 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,018 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,021 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,074 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,074 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,077 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,079 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,332 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,335 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,335 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,337 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,338 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,346 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,348 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,349 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,362 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,364 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,496 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,501 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,504 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,503 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,506 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,508 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,515 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,517 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,518 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,518 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,520 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,520 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,520 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,521 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,522 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,523 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,532 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,533 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,622 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,628 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,650 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,652 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,675 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,680 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,748 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,750 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,756 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,760 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,785 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,787 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,884 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,886 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:10,892 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:10,894 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,108 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,110 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,111 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,113 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,123 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,128 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,132 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,134 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,170 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,175 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,182 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,185 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,280 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,281 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,282 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,282 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,283 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,284 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,284 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,288 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,289 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,290 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,368 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,371 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,397 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,399 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,413 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,415 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,465 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,467 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,467 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,467 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,469 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,469 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,487 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,488 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,489 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,490 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,504 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,505 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,506 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,507 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,546 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,549 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,566 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,572 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,589 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,591 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,589 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,594 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,608 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,610 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,612 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,614 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,682 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,687 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,708 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,710 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,745 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,745 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,747 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,747 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,756 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,757 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,759 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,759 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,793 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,796 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,824 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,826 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,941 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,943 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:11,943 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:11,945 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,034 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,037 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,101 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,103 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,163 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,167 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,253 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,254 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,255 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,255 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,286 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,288 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,400 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,403 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,409 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,408 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,411 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,410 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,412 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,413 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,446 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,448 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,448 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,450 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,450 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,452 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,470 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,472 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,526 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,533 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,536 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,541 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,574 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,576 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,630 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,633 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,688 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,690 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,691 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,693 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,701 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,703 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,785 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,787 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,815 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,817 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,830 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,832 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,936 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,938 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,944 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,946 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,966 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,971 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:12,995 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:12,996 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,006 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,013 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,043 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,045 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,050 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,052 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,112 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,114 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,275 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,277 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,285 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,287 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,295 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,297 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,615 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,617 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,645 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,658 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:13,693 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:13,695 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:57:14,394 - distributed.worker - INFO - Starting Worker plugin qme_vars.py123e1e1e-9078-49f9-9dd8-5d7f7279a79c
2025-09-03 10:57:14,397 - distributed.utils - INFO - Reload module qme_vars from .py file
