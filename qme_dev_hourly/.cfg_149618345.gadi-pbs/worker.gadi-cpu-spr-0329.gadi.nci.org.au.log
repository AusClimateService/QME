Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-13 18:55:03,632 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:34335'
2025-09-13 18:55:03,642 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:40317'
2025-09-13 18:55:03,646 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:35813'
2025-09-13 18:55:03,650 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:38411'
2025-09-13 18:55:03,653 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:40929'
2025-09-13 18:55:03,658 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:40767'
2025-09-13 18:55:03,662 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:39793'
2025-09-13 18:55:03,666 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:46041'
2025-09-13 18:55:03,670 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:36809'
2025-09-13 18:55:03,676 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:43615'
2025-09-13 18:55:03,680 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:43009'
2025-09-13 18:55:03,684 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:35655'
2025-09-13 18:55:03,688 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:43327'
2025-09-13 18:55:03,692 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:45023'
2025-09-13 18:55:03,697 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:33697'
2025-09-13 18:55:03,702 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:45083'
2025-09-13 18:55:03,706 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:46133'
2025-09-13 18:55:03,711 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:38955'
2025-09-13 18:55:03,714 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:33191'
2025-09-13 18:55:03,718 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:39003'
2025-09-13 18:55:03,859 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:43403'
2025-09-13 18:55:03,864 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:46253'
2025-09-13 18:55:03,868 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:41897'
2025-09-13 18:55:03,872 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:44903'
2025-09-13 18:55:03,876 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:33563'
2025-09-13 18:55:03,880 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:44905'
2025-09-13 18:55:03,885 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:32793'
2025-09-13 18:55:03,890 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:36605'
2025-09-13 18:55:03,894 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:37643'
2025-09-13 18:55:03,899 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:36233'
2025-09-13 18:55:03,903 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:45243'
2025-09-13 18:55:03,908 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:37803'
2025-09-13 18:55:03,912 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:33669'
2025-09-13 18:55:03,916 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:36361'
2025-09-13 18:55:03,920 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:36783'
2025-09-13 18:55:03,925 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:39893'
2025-09-13 18:55:03,929 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:45849'
2025-09-13 18:55:03,934 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:36487'
2025-09-13 18:55:03,938 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:46459'
2025-09-13 18:55:03,942 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:35191'
2025-09-13 18:55:03,946 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:33379'
2025-09-13 18:55:03,951 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:38189'
2025-09-13 18:55:03,954 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:37779'
2025-09-13 18:55:03,959 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:42601'
2025-09-13 18:55:03,963 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:40395'
2025-09-13 18:55:03,968 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:40311'
2025-09-13 18:55:03,971 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:37133'
2025-09-13 18:55:03,975 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:46557'
2025-09-13 18:55:03,979 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:35893'
2025-09-13 18:55:03,984 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:40331'
2025-09-13 18:55:03,987 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:35719'
2025-09-13 18:55:03,991 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.85.41:36493'
2025-09-13 18:55:05,045 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:44085
2025-09-13 18:55:05,045 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:41777
2025-09-13 18:55:05,045 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:37131
2025-09-13 18:55:05,045 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:34533
2025-09-13 18:55:05,045 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:38813
2025-09-13 18:55:05,045 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:43123
2025-09-13 18:55:05,045 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:37967
2025-09-13 18:55:05,045 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:34373
2025-09-13 18:55:05,045 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:39965
2025-09-13 18:55:05,045 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:33431
2025-09-13 18:55:05,045 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:44711
2025-09-13 18:55:05,045 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:41815
2025-09-13 18:55:05,045 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:44085
2025-09-13 18:55:05,045 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:45139
2025-09-13 18:55:05,045 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:41777
2025-09-13 18:55:05,045 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:34101
2025-09-13 18:55:05,045 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:37931
2025-09-13 18:55:05,045 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:43003
2025-09-13 18:55:05,045 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:37131
2025-09-13 18:55:05,046 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:34533
2025-09-13 18:55:05,046 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:38813
2025-09-13 18:55:05,045 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:33257
2025-09-13 18:55:05,046 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:43123
2025-09-13 18:55:05,046 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:37967
2025-09-13 18:55:05,046 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:34373
2025-09-13 18:55:05,046 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:39965
2025-09-13 18:55:05,046 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:39815
2025-09-13 18:55:05,046 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:33431
2025-09-13 18:55:05,046 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:44711
2025-09-13 18:55:05,046 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:41815
2025-09-13 18:55:05,046 - distributed.worker - INFO -          dashboard at:           10.6.85.41:38019
2025-09-13 18:55:05,046 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:45139
2025-09-13 18:55:05,046 - distributed.worker - INFO -          dashboard at:           10.6.85.41:35865
2025-09-13 18:55:05,046 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:34101
2025-09-13 18:55:05,046 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:37931
2025-09-13 18:55:05,046 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:43003
2025-09-13 18:55:05,046 - distributed.worker - INFO -          dashboard at:           10.6.85.41:45515
2025-09-13 18:55:05,046 - distributed.worker - INFO -          dashboard at:           10.6.85.41:41061
2025-09-13 18:55:05,046 - distributed.worker - INFO -          dashboard at:           10.6.85.41:32867
2025-09-13 18:55:05,046 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:33257
2025-09-13 18:55:05,046 - distributed.worker - INFO -          dashboard at:           10.6.85.41:36377
2025-09-13 18:55:05,046 - distributed.worker - INFO -          dashboard at:           10.6.85.41:33183
2025-09-13 18:55:05,046 - distributed.worker - INFO -          dashboard at:           10.6.85.41:35821
2025-09-13 18:55:05,046 - distributed.worker - INFO -          dashboard at:           10.6.85.41:38977
2025-09-13 18:55:05,046 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:39815
2025-09-13 18:55:05,046 - distributed.worker - INFO -          dashboard at:           10.6.85.41:46027
2025-09-13 18:55:05,046 - distributed.worker - INFO -          dashboard at:           10.6.85.41:46669
2025-09-13 18:55:05,046 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,046 - distributed.worker - INFO -          dashboard at:           10.6.85.41:41523
2025-09-13 18:55:05,046 - distributed.worker - INFO -          dashboard at:           10.6.85.41:33913
2025-09-13 18:55:05,046 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,046 - distributed.worker - INFO -          dashboard at:           10.6.85.41:41899
2025-09-13 18:55:05,046 - distributed.worker - INFO -          dashboard at:           10.6.85.41:37061
2025-09-13 18:55:05,046 - distributed.worker - INFO -          dashboard at:           10.6.85.41:38741
2025-09-13 18:55:05,046 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,046 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,046 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,046 - distributed.worker - INFO -          dashboard at:           10.6.85.41:40179
2025-09-13 18:55:05,046 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,046 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,046 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,046 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,046 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,046 - distributed.worker - INFO -          dashboard at:           10.6.85.41:34899
2025-09-13 18:55:05,046 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,046 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,046 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,046 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,046 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,046 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,046 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,046 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,046 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,046 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,046 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,046 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,046 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,046 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,046 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,046 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,046 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,046 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,046 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,046 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,046 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,046 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,046 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,046 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,046 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,046 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,046 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,046 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,046 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,046 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,046 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,046 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,046 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,046 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,046 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,046 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,046 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,046 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,046 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,046 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,046 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,046 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,046 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,046 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,046 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,047 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,047 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,047 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,047 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,047 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,047 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,047 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,047 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,047 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,047 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,047 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-2plm2qqd
2025-09-13 18:55:05,047 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,047 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,047 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-x6iqd69m
2025-09-13 18:55:05,047 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,047 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,047 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-myg7z091
2025-09-13 18:55:05,047 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,047 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-uhwj1h0z
2025-09-13 18:55:05,047 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-11eny801
2025-09-13 18:55:05,047 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,047 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-b5ap_j4a
2025-09-13 18:55:05,047 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-2b8p8tqi
2025-09-13 18:55:05,047 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-vh7n6rbf
2025-09-13 18:55:05,047 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-7he0mguh
2025-09-13 18:55:05,047 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-vgwjfl8w
2025-09-13 18:55:05,047 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,047 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-25o_nt4p
2025-09-13 18:55:05,047 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,047 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-my_0frgg
2025-09-13 18:55:05,047 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-p1h7i_3v
2025-09-13 18:55:05,047 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,047 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-zwlnil2f
2025-09-13 18:55:05,047 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-giu120kq
2025-09-13 18:55:05,047 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,047 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-nw4fv1zj
2025-09-13 18:55:05,047 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,047 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-0hnf19gt
2025-09-13 18:55:05,047 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,047 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,047 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,047 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,047 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,047 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,047 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-y7yxwsn2
2025-09-13 18:55:05,047 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,047 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,047 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,047 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,047 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,047 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,047 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,047 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,049 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:33021
2025-09-13 18:55:05,049 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:33021
2025-09-13 18:55:05,049 - distributed.worker - INFO -          dashboard at:           10.6.85.41:39517
2025-09-13 18:55:05,049 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,049 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,049 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,049 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,049 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-cm5frkkn
2025-09-13 18:55:05,049 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,071 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,071 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,071 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,072 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,072 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,073 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,073 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,074 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,074 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,074 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,075 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,075 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,076 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,077 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,077 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,078 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,078 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,078 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,078 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,079 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,079 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,080 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,080 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,081 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,081 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,082 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,082 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,082 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,083 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,083 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,083 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,084 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,084 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,085 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,085 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,087 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,087 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,088 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,088 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,088 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,089 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,089 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,090 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,090 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,091 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,091 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,091 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,091 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,092 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,092 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,092 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,092 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,092 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,093 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,093 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,093 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:39963
2025-09-13 18:55:05,093 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:39963
2025-09-13 18:55:05,093 - distributed.worker - INFO -          dashboard at:           10.6.85.41:42773
2025-09-13 18:55:05,093 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,093 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,093 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,094 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,094 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-ef57n4ju
2025-09-13 18:55:05,094 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,094 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,094 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,094 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,094 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,095 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,095 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,095 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,095 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,095 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,096 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,096 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,096 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,097 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,097 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,097 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,097 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,098 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,098 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,098 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,098 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,100 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,116 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,117 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,117 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,119 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,168 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:39743
2025-09-13 18:55:05,169 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:39743
2025-09-13 18:55:05,169 - distributed.worker - INFO -          dashboard at:           10.6.85.41:37605
2025-09-13 18:55:05,169 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,169 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,169 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,169 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,169 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-q2dewy1f
2025-09-13 18:55:05,169 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,169 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:44455
2025-09-13 18:55:05,169 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:44455
2025-09-13 18:55:05,169 - distributed.worker - INFO -          dashboard at:           10.6.85.41:46625
2025-09-13 18:55:05,169 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,169 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,169 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,169 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,169 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-g8o_nuzx
2025-09-13 18:55:05,169 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,191 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,192 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,192 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,194 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,196 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,197 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,198 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,199 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,202 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:44259
2025-09-13 18:55:05,202 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:44259
2025-09-13 18:55:05,202 - distributed.worker - INFO -          dashboard at:           10.6.85.41:44933
2025-09-13 18:55:05,202 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,202 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,202 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,202 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,202 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-poi3k0tj
2025-09-13 18:55:05,202 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,208 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:39807
2025-09-13 18:55:05,208 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:39807
2025-09-13 18:55:05,208 - distributed.worker - INFO -          dashboard at:           10.6.85.41:41645
2025-09-13 18:55:05,208 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,208 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,208 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,208 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,208 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-qeijtcza
2025-09-13 18:55:05,208 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,222 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:36585
2025-09-13 18:55:05,223 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:36585
2025-09-13 18:55:05,223 - distributed.worker - INFO -          dashboard at:           10.6.85.41:46723
2025-09-13 18:55:05,223 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,223 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,223 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,223 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,223 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-ci4zudhc
2025-09-13 18:55:05,223 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,225 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,226 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,226 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,227 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,229 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,230 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,230 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,231 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,241 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,241 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:34585
2025-09-13 18:55:05,241 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:34585
2025-09-13 18:55:05,241 - distributed.worker - INFO -          dashboard at:           10.6.85.41:43369
2025-09-13 18:55:05,241 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,241 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,241 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,241 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,241 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-3sj1izxm
2025-09-13 18:55:05,241 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,241 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,242 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,243 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,261 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,262 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,262 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,264 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,271 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:39071
2025-09-13 18:55:05,271 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:39071
2025-09-13 18:55:05,271 - distributed.worker - INFO -          dashboard at:           10.6.85.41:33277
2025-09-13 18:55:05,271 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,271 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,271 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,271 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,271 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-bhrur1pi
2025-09-13 18:55:05,271 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,274 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:37353
2025-09-13 18:55:05,274 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:37353
2025-09-13 18:55:05,274 - distributed.worker - INFO -          dashboard at:           10.6.85.41:44359
2025-09-13 18:55:05,274 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,274 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,274 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,275 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,275 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-7esh2xra
2025-09-13 18:55:05,275 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,275 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:46209
2025-09-13 18:55:05,275 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:46209
2025-09-13 18:55:05,275 - distributed.worker - INFO -          dashboard at:           10.6.85.41:44653
2025-09-13 18:55:05,275 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,275 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,275 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,275 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,275 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-svy6dgf9
2025-09-13 18:55:05,275 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,286 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:33045
2025-09-13 18:55:05,286 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:33045
2025-09-13 18:55:05,286 - distributed.worker - INFO -          dashboard at:           10.6.85.41:35367
2025-09-13 18:55:05,286 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,286 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,286 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,286 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,286 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-wk829uwl
2025-09-13 18:55:05,286 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,292 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,293 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,293 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,294 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,296 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,296 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,297 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,298 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,298 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,299 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,299 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,301 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,304 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,305 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,305 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,306 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,475 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:35251
2025-09-13 18:55:05,475 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:35251
2025-09-13 18:55:05,476 - distributed.worker - INFO -          dashboard at:           10.6.85.41:34241
2025-09-13 18:55:05,476 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,476 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,476 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,476 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,476 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-on1p0erc
2025-09-13 18:55:05,476 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,488 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,488 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,488 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,488 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,491 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:33539
2025-09-13 18:55:05,491 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:33539
2025-09-13 18:55:05,491 - distributed.worker - INFO -          dashboard at:           10.6.85.41:45541
2025-09-13 18:55:05,491 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,491 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,491 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,492 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,492 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-jvz2i3t9
2025-09-13 18:55:05,492 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,499 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:42077
2025-09-13 18:55:05,499 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:42077
2025-09-13 18:55:05,499 - distributed.worker - INFO -          dashboard at:           10.6.85.41:38885
2025-09-13 18:55:05,499 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,499 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,499 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,499 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,499 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-rmkn2u2u
2025-09-13 18:55:05,499 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,500 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:41425
2025-09-13 18:55:05,500 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:41425
2025-09-13 18:55:05,500 - distributed.worker - INFO -          dashboard at:           10.6.85.41:42331
2025-09-13 18:55:05,500 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,500 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,500 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,500 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,500 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-xn01pf18
2025-09-13 18:55:05,500 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,500 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:46141
2025-09-13 18:55:05,500 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:46141
2025-09-13 18:55:05,500 - distributed.worker - INFO -          dashboard at:           10.6.85.41:42661
2025-09-13 18:55:05,500 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,500 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,500 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,500 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,500 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-3md3effz
2025-09-13 18:55:05,500 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,501 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:38963
2025-09-13 18:55:05,501 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:38963
2025-09-13 18:55:05,501 - distributed.worker - INFO -          dashboard at:           10.6.85.41:46867
2025-09-13 18:55:05,501 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,501 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,501 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,501 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,501 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:33851
2025-09-13 18:55:05,501 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-uek8xa38
2025-09-13 18:55:05,501 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:39189
2025-09-13 18:55:05,501 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:33851
2025-09-13 18:55:05,501 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,501 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:39189
2025-09-13 18:55:05,501 - distributed.worker - INFO -          dashboard at:           10.6.85.41:33745
2025-09-13 18:55:05,501 - distributed.worker - INFO -          dashboard at:           10.6.85.41:36071
2025-09-13 18:55:05,501 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,501 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,501 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,501 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,501 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,501 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,501 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,501 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,501 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-4ff8t76f
2025-09-13 18:55:05,501 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-8e3cv48x
2025-09-13 18:55:05,501 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,501 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,502 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:43267
2025-09-13 18:55:05,502 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:43267
2025-09-13 18:55:05,502 - distributed.worker - INFO -          dashboard at:           10.6.85.41:39939
2025-09-13 18:55:05,502 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,502 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,502 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,502 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,502 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-rs5w87ze
2025-09-13 18:55:05,502 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,511 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:35931
2025-09-13 18:55:05,511 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:35931
2025-09-13 18:55:05,511 - distributed.worker - INFO -          dashboard at:           10.6.85.41:38997
2025-09-13 18:55:05,511 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,511 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,511 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,511 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,511 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-9jbqtkdz
2025-09-13 18:55:05,511 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,514 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:39603
2025-09-13 18:55:05,515 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:39603
2025-09-13 18:55:05,515 - distributed.worker - INFO -          dashboard at:           10.6.85.41:41525
2025-09-13 18:55:05,515 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,515 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,515 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,515 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,515 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-j02nwgoh
2025-09-13 18:55:05,515 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,516 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:43925
2025-09-13 18:55:05,516 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:43925
2025-09-13 18:55:05,516 - distributed.worker - INFO -          dashboard at:           10.6.85.41:36269
2025-09-13 18:55:05,516 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,516 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,516 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,516 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,517 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-svsi_hq9
2025-09-13 18:55:05,517 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,518 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:45245
2025-09-13 18:55:05,518 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:45245
2025-09-13 18:55:05,518 - distributed.worker - INFO -          dashboard at:           10.6.85.41:33805
2025-09-13 18:55:05,518 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,518 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,518 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,518 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,518 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-mlxy6hzq
2025-09-13 18:55:05,518 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,520 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:33695
2025-09-13 18:55:05,520 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:33695
2025-09-13 18:55:05,520 - distributed.worker - INFO -          dashboard at:           10.6.85.41:35751
2025-09-13 18:55:05,520 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,520 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,520 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,520 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,520 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-y6ao02ay
2025-09-13 18:55:05,520 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,521 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,521 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,521 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,521 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,522 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,522 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:37417
2025-09-13 18:55:05,522 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,522 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:37417
2025-09-13 18:55:05,522 - distributed.worker - INFO -          dashboard at:           10.6.85.41:40631
2025-09-13 18:55:05,522 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,522 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,522 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,522 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,522 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-mb_dq98t
2025-09-13 18:55:05,522 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,522 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,523 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:41929
2025-09-13 18:55:05,523 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:41929
2025-09-13 18:55:05,523 - distributed.worker - INFO -          dashboard at:           10.6.85.41:46839
2025-09-13 18:55:05,523 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,523 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,523 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,523 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,523 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-f609nnua
2025-09-13 18:55:05,523 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,524 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,525 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,526 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,526 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:39303
2025-09-13 18:55:05,526 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:39303
2025-09-13 18:55:05,526 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,526 - distributed.worker - INFO -          dashboard at:           10.6.85.41:32839
2025-09-13 18:55:05,527 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,527 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,527 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,527 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-4z6aw5x2
2025-09-13 18:55:05,527 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,527 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,528 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,528 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,528 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,529 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,529 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:39691
2025-09-13 18:55:05,529 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:39691
2025-09-13 18:55:05,529 - distributed.worker - INFO -          dashboard at:           10.6.85.41:46883
2025-09-13 18:55:05,529 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,529 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,529 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,529 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,529 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-sa60haye
2025-09-13 18:55:05,530 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,530 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,530 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,530 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,530 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,531 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,531 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,531 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,531 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,532 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,532 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,533 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,534 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:41145
2025-09-13 18:55:05,534 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,534 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:41145
2025-09-13 18:55:05,534 - distributed.worker - INFO -          dashboard at:           10.6.85.41:41115
2025-09-13 18:55:05,534 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,534 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,534 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,534 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,534 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-so6p8jaq
2025-09-13 18:55:05,534 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,534 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,535 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,535 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,536 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,539 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:42349
2025-09-13 18:55:05,539 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:42349
2025-09-13 18:55:05,539 - distributed.worker - INFO -          dashboard at:           10.6.85.41:38285
2025-09-13 18:55:05,539 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,539 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,539 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,539 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,539 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-krpmmdfe
2025-09-13 18:55:05,539 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,540 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:46127
2025-09-13 18:55:05,540 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:46127
2025-09-13 18:55:05,541 - distributed.worker - INFO -          dashboard at:           10.6.85.41:39153
2025-09-13 18:55:05,541 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,541 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,541 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,541 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,541 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-3cw7davd
2025-09-13 18:55:05,541 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,542 - distributed.worker - INFO -       Start worker at:     tcp://10.6.85.41:34803
2025-09-13 18:55:05,542 - distributed.worker - INFO -          Listening to:     tcp://10.6.85.41:34803
2025-09-13 18:55:05,542 - distributed.worker - INFO -          dashboard at:           10.6.85.41:43643
2025-09-13 18:55:05,542 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,542 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,542 - distributed.worker - INFO -               Threads:                          2
2025-09-13 18:55:05,542 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-13 18:55:05,542 - distributed.worker - INFO -       Local Directory: /jobfs/149618345.gadi-pbs/dask-scratch-space/worker-a2802xus
2025-09-13 18:55:05,542 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,554 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,555 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,555 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,555 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,555 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,556 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,556 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,556 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,556 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,557 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,557 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,558 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,558 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,558 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,558 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,558 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,559 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,559 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,559 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,560 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,560 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,560 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,560 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,561 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,561 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,561 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,561 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,562 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,562 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,562 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,562 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,562 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,562 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,562 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,562 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,562 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,562 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,562 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,563 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,563 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,563 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,563 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,564 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,564 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,565 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,565 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-13 18:55:05,566 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,566 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,566 - distributed.worker - INFO -         Registered to:      tcp://10.6.85.37:8784
2025-09-13 18:55:05,566 - distributed.worker - INFO - -------------------------------------------------
2025-09-13 18:55:05,567 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:05,568 - distributed.core - INFO - Starting established connection to tcp://10.6.85.37:8784
2025-09-13 18:55:11,291 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,291 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,291 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,291 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,291 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,291 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,292 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,292 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,292 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,292 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,292 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,292 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,292 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,292 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,293 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,293 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,293 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,293 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,293 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,293 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,293 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,293 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,293 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,294 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,294 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,294 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,294 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,294 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,294 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,294 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,294 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,294 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,294 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,294 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,295 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,295 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,294 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,295 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,295 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,295 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,295 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,295 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,295 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,295 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,295 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,295 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,295 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,295 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,295 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,295 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,295 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,295 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,295 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,295 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,295 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,296 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,296 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,296 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,296 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,296 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,296 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,296 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,296 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,295 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,296 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,296 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,296 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,296 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,296 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,296 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,296 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,296 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,296 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,296 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,297 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,297 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,297 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,297 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,297 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,297 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,297 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,297 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,297 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,298 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,298 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,298 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,298 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,298 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,298 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,297 - distributed.worker - INFO - Starting Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:55:11,298 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,298 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,295 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,299 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,299 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,299 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,300 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,300 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,300 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,300 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,300 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,302 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,305 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:11,310 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-13 18:55:13,868 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,868 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,868 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,869 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,869 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,869 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,869 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,869 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,869 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,869 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,869 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,869 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,870 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,870 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,870 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,870 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,871 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,870 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,870 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,871 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,871 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,870 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,870 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,870 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,871 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,871 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,871 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,871 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,871 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,871 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,871 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,871 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,872 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,872 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,871 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,872 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,871 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,872 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,872 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,871 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,871 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,872 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,872 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,872 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,872 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,872 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,872 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,872 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,872 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,872 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,872 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,872 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,873 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,873 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,873 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,873 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,872 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,872 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,873 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,873 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,873 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,873 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,873 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,873 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,873 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,873 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,874 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,874 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,873 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,874 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,873 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,873 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,874 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,873 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,874 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,874 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,874 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,874 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,874 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,874 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,874 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,874 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,874 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,874 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,875 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,875 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,875 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,875 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,875 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,875 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,876 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,876 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,876 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,876 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,876 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,876 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,876 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,876 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,876 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,874 - distributed.worker - INFO - Starting Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:55:13,877 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,877 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,878 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,878 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-13 18:55:13,936 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,936 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,936 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,936 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,937 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,937 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,937 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,937 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,937 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,937 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,937 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,938 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,938 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,938 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,938 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,938 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,938 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,938 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,938 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,938 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,939 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,939 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,939 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,939 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,939 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,939 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,939 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,939 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,940 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,940 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,940 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,940 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,940 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,940 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,940 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,940 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,940 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,940 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,940 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,940 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,941 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,941 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,941 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,941 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,941 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,941 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,941 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,941 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,941 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,941 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,941 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,941 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,941 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,941 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,942 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,941 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,941 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,941 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,941 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,942 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,942 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,942 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,942 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,942 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,942 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,942 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,942 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,942 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,942 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,943 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,943 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,943 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,943 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,943 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,942 - distributed.worker - INFO - Starting Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:55:13,943 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,943 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,943 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,943 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,943 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,943 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,943 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,944 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,944 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,944 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,944 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,944 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,944 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,944 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,944 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,944 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,944 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,945 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,945 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,945 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,945 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,945 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,945 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,945 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,946 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,946 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,946 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,946 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:13,952 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-13 18:55:14,036 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,036 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,036 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,036 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,036 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,036 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,036 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,037 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,037 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,037 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,037 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,037 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,037 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,037 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,037 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,037 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,037 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,037 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,038 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,038 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,038 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,038 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,038 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,038 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,038 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,038 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,038 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,039 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,038 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,039 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,039 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,039 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,039 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,039 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,039 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,039 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,039 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,039 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,039 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,039 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,039 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,039 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,039 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,039 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,039 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,039 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,039 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,039 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,039 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,039 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,039 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,040 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,040 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,040 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,040 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,040 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,040 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,040 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,040 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,040 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,040 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,040 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,040 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,040 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,040 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,040 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,040 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,040 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,041 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,041 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,041 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,041 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,041 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,041 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,041 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,041 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,041 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,041 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,041 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,041 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,041 - distributed.worker - INFO - Starting Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:55:14,042 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,041 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,042 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,042 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,042 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,042 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,042 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,043 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,043 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,043 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,043 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,043 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,043 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,043 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,043 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,043 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,043 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,044 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,044 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,044 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,044 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,044 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:55:14,045 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,228 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:45245. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,228 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:41145. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,228 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,228 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:33539. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,228 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,228 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,228 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,228 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,228 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:33431. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,229 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:44711. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,229 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:41425. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,229 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:35251. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,229 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:38813. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,224 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.85.41:34658 remote=tcp://10.6.85.37:8784>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,228 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,228 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,228 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,228 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,228 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,228 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,229 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:37417. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,229 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:43267. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,228 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,229 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:39965. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,229 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,229 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:41777. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,229 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:43003. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,230 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:46127. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,230 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:43925. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,230 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:34803. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,230 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:37931. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,230 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:34533. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,230 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:33257. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,230 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:42077. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,230 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,230 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:39815. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,230 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:33695. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,230 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:33045. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,230 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:39303. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,231 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:42349. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,231 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:36585. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,229 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,229 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,228 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,230 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,231 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:39963. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,229 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,231 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:41929. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,231 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:39691. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,231 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:39189. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,231 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:38963. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,232 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:37131. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,228 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,232 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:46209. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,232 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:34373. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,229 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,232 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:33851. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,232 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:39743. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,232 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:44085. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,232 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:39603. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,232 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:33021. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,228 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,230 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,228 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,233 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:43123. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,233 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:37967. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,233 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:35931. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,229 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 226, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 137, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.85.41:39410 remote=tcp://10.6.85.37:8784>: Stream is closed
2025-09-13 18:57:39,233 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:34585. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,233 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:41815. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,228 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.85.41:39398 remote=tcp://10.6.85.37:8784>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-13 18:57:39,229 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,227 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,230 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,234 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:39071. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,235 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:34101. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,234 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,235 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:44455. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,235 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:37353. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,231 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.85.41:39258 remote=tcp://10.6.85.37:8784>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-13 18:57:39,236 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,236 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:44259. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,232 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.85.41:34736 remote=tcp://10.6.85.37:8784>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-13 18:57:39,232 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.85.41:34732 remote=tcp://10.6.85.37:8784>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-13 18:57:39,229 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,232 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.85.41:34716 remote=tcp://10.6.85.37:8784>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-13 18:57:39,233 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.85.41:34740 remote=tcp://10.6.85.37:8784>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-13 18:57:39,233 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.85.41:34770 remote=tcp://10.6.85.37:8784>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-13 18:57:39,239 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:39807. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,232 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.85.41:34710 remote=tcp://10.6.85.37:8784>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-13 18:57:39,234 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.85.41:34692 remote=tcp://10.6.85.37:8784>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-13 18:57:39,234 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.85.41:34696 remote=tcp://10.6.85.37:8784>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-13 18:57:39,240 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:45849'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,234 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.85.41:34754 remote=tcp://10.6.85.37:8784>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-13 18:57:39,234 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.85.41:34784 remote=tcp://10.6.85.37:8784>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-13 18:57:39,243 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,243 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,243 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,244 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,244 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,244 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,235 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.85.41:34762 remote=tcp://10.6.85.37:8784>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-13 18:57:39,233 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.85.41:34792 remote=tcp://10.6.85.37:8784>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-13 18:57:39,235 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.85.41:39180 remote=tcp://10.6.85.37:8784>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-13 18:57:39,239 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.85.41:34800 remote=tcp://10.6.85.37:8784>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-13 18:57:39,233 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.85.41:34790 remote=tcp://10.6.85.37:8784>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-13 18:57:39,237 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.85.41:39232 remote=tcp://10.6.85.37:8784>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-13 18:57:39,232 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.85.41:34698 remote=tcp://10.6.85.37:8784>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-13 18:57:39,248 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,251 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:36361'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,252 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:35893'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,252 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 423, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,252 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:36493'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,253 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,253 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:43615'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,253 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,253 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 483, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,253 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 427, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,253 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 50, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,253 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,253 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,253 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,253 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:33697'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,253 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,253 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 579, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,253 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,253 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,253 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,253 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,253 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:45023'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,254 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 125, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,254 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:40395'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,254 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,254 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 368, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,254 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,254 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,254 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,254 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,254 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,254 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,254 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,254 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,255 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,254 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 149, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,255 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,255 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,255 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,255 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 117, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,255 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,255 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,255 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,255 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 711, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,255 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,255 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 647, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,255 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,256 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,256 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,256 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,256 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,256 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,256 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,256 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,256 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,235 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
                ^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.85.41:34758 remote=tcp://10.6.85.37:8784>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-13 18:57:39,259 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,266 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:36605'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,266 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:35719'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,267 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:33191'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,267 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:40317'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,267 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:45083'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,267 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,267 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:35813'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,267 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:37133'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,267 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,267 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 639, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,268 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:40767'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,268 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,268 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 455, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,268 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:36783'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,268 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 177, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,268 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,268 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 656, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,268 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:43009'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,268 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 261, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,268 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,268 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 588, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,268 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,268 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 15, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,268 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:38189'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,268 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,268 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 179, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,268 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 293, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,268 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:33669'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,268 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,268 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,269 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 419, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,269 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:44903'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,269 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,269 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:39893'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,269 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 287, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,269 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:32793'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,269 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 401, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,269 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 632, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,269 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:39003'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,269 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 100, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,269 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,269 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 172, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,270 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:43403'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,270 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 546, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,270 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 67, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,270 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 163, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,270 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:36233'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,270 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 454, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,270 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 664, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,270 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:35191'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,270 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,271 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:38955'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,271 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 447, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,271 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:46557'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,271 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 159, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,271 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:41897'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,271 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:42601'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,271 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,271 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,271 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,271 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:43327'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,272 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,272 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,272 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,272 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 340, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,272 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,272 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:35655'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,272 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,272 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,272 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 330, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,272 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 411, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,272 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 222, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,272 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,272 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:36809'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,272 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,272 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 525, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,272 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 237, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,272 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,272 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,272 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:36487'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,272 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,272 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,272 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:37779'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,273 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,273 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,272 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,273 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,273 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,273 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:46041'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,273 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,273 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,273 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,273 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 190, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,273 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,273 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,273 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,273 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:34335'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,273 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,273 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,273 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,273 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 250, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,273 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,273 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,273 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,273 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:40331'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,273 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,273 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,273 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 22, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,273 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,273 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:46133'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,273 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,273 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 64, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,273 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,273 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,273 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 658, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,273 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,273 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 478, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,274 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:33563'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,274 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,274 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 68, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,274 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 240, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,274 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 383, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,274 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,274 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 16, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,274 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:46459'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,274 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 645, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,274 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,274 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,274 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,274 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 462, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,274 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,274 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:39793'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,274 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,274 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,274 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 518, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,274 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,274 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,274 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,274 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,274 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 671, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,274 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:37643'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,274 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,274 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,274 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,274 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 366, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,274 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,275 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,275 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,275 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,275 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:37803'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,275 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,275 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,275 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,275 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,275 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,275 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,275 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,275 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,275 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,275 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:40311'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,275 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,275 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,275 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 438, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,275 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,275 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,275 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,275 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,275 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,275 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:46253'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,275 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 126, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,275 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,275 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,275 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,275 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 415, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,275 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,275 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,276 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,276 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,276 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 439, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,276 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,276 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,276 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,276 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,276 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,276 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,276 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,276 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,276 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,276 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,276 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,276 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,276 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,276 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,276 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,276 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,276 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,276 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,277 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,277 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,277 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,277 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,276 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,277 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,277 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,277 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,277 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,277 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,277 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:45243'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,277 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,277 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,278 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:38411'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,278 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,278 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:44905'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,278 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,278 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,279 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,279 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,279 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,279 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 506, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,279 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,280 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 198, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,280 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 29, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,280 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 490, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,280 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 367, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,280 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,280 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 708, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,280 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,281 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,281 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,281 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,281 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,281 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,281 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,281 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,281 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,281 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 466, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,281 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,281 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,281 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,281 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,281 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('rechunk-merge-rechunk-split-store-map-9ce927bd04a1446ed0ad90cd4e10f564', 622, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,282 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,282 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,282 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,282 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,282 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,282 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,282 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,282 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,282 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,282 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,282 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,283 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,283 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,278 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14706dd3aa50>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-13 18:57:39,286 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,286 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,273 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1489eeb8acd0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-13 18:57:39,289 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,283 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14c9c070da50>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-13 18:57:39,307 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,308 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,313 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,316 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,326 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,334 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,841 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,841 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:46141. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,846 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:33379'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,847 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,847 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,847 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,847 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,847 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,847 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,848 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1455f99ca710>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-13 18:57:39,853 - distributed.nanny - INFO - Worker closed
2025-09-13 18:57:39,859 - distributed.core - INFO - Connection to tcp://10.6.85.37:8784 has been closed.
2025-09-13 18:57:39,860 - distributed.worker - INFO - Stopping worker at tcp://10.6.85.41:45139. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,862 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.85.41:40929'. Reason: worker-handle-scheduler-connection-broken
2025-09-13 18:57:39,863 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.85.42:36161, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-13 18:57:39,863 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-13 18:57:39,863 - distributed.worker - INFO - Removing Worker plugin qme_utils.py6c57781b-1b0f-4281-a069-63e35d7bb45d
2025-09-13 18:57:39,863 - distributed.worker - INFO - Removing Worker plugin qme_vars.pye298bcae-3a9c-4489-b36c-818c2815bdbc
2025-09-13 18:57:39,863 - distributed.worker - INFO - Removing Worker plugin qme_train.py7b0ec03a-62dd-45f0-9c91-3c609e1acd47
2025-09-13 18:57:39,863 - distributed.worker - INFO - Removing Worker plugin qme_apply.py67a77df5-3bdf-4146-af0f-7b93384dcafa
2025-09-13 18:57:39,864 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1477b60c32d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-13 18:57:39,868 - distributed.nanny - INFO - Worker closed
