Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-03 10:31:48,992 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:41967'
2025-09-03 10:31:49,003 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:37701'
2025-09-03 10:31:49,010 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:35111'
2025-09-03 10:31:49,013 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:35849'
2025-09-03 10:31:49,020 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:43559'
2025-09-03 10:31:49,050 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:43457'
2025-09-03 10:31:49,055 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:35249'
2025-09-03 10:31:49,059 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:43013'
2025-09-03 10:31:49,064 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:46491'
2025-09-03 10:31:49,071 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:39315'
2025-09-03 10:31:49,074 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:36911'
2025-09-03 10:31:49,079 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:45743'
2025-09-03 10:31:49,083 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:33003'
2025-09-03 10:31:49,092 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:43339'
2025-09-03 10:31:49,096 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:37589'
2025-09-03 10:31:49,101 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:42565'
2025-09-03 10:31:49,106 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:35543'
2025-09-03 10:31:49,111 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:42787'
2025-09-03 10:31:49,117 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:34993'
2025-09-03 10:31:49,121 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:36841'
2025-09-03 10:31:49,126 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:35643'
2025-09-03 10:31:49,130 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:41783'
2025-09-03 10:31:49,134 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:40199'
2025-09-03 10:31:49,138 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:40201'
2025-09-03 10:31:49,143 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:46165'
2025-09-03 10:31:49,148 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:38137'
2025-09-03 10:31:49,153 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:39233'
2025-09-03 10:31:49,158 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:44973'
2025-09-03 10:31:49,163 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:38959'
2025-09-03 10:31:49,166 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:34193'
2025-09-03 10:31:49,170 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:34729'
2025-09-03 10:31:49,174 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:44255'
2025-09-03 10:31:49,177 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:38235'
2025-09-03 10:31:49,180 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:40141'
2025-09-03 10:31:49,184 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:38665'
2025-09-03 10:31:49,189 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:45291'
2025-09-03 10:31:49,193 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:42523'
2025-09-03 10:31:49,197 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:36437'
2025-09-03 10:31:49,201 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:35339'
2025-09-03 10:31:49,206 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:46795'
2025-09-03 10:31:49,211 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:40925'
2025-09-03 10:31:49,215 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:40435'
2025-09-03 10:31:49,290 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:34057'
2025-09-03 10:31:49,295 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:38135'
2025-09-03 10:31:49,300 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:35705'
2025-09-03 10:31:49,304 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:43863'
2025-09-03 10:31:49,308 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:34239'
2025-09-03 10:31:49,314 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:41393'
2025-09-03 10:31:49,317 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:45797'
2025-09-03 10:31:49,323 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:39141'
2025-09-03 10:31:49,326 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:40051'
2025-09-03 10:31:49,331 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:37419'
2025-09-03 10:31:49,338 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:36525'
2025-09-03 10:31:49,342 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:42541'
2025-09-03 10:31:49,347 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:40267'
2025-09-03 10:31:49,352 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:42173'
2025-09-03 10:31:49,356 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:36909'
2025-09-03 10:31:49,361 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:45629'
2025-09-03 10:31:49,366 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:34341'
2025-09-03 10:31:49,370 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:36459'
2025-09-03 10:31:49,372 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:38993'
2025-09-03 10:31:49,378 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:45337'
2025-09-03 10:31:49,382 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:44695'
2025-09-03 10:31:49,387 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:46043'
2025-09-03 10:31:49,390 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:34791'
2025-09-03 10:31:49,393 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:36439'
2025-09-03 10:31:49,398 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:36335'
2025-09-03 10:31:49,403 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:37655'
2025-09-03 10:31:49,409 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:32979'
2025-09-03 10:31:49,413 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:38097'
2025-09-03 10:31:49,418 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:43561'
2025-09-03 10:31:49,422 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:45367'
2025-09-03 10:31:49,427 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:37661'
2025-09-03 10:31:49,431 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:44583'
2025-09-03 10:31:49,436 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:41813'
2025-09-03 10:31:50,012 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:46077'
2025-09-03 10:31:50,016 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:34887'
2025-09-03 10:31:50,020 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:46047'
2025-09-03 10:31:50,023 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:45285'
2025-09-03 10:31:50,026 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:40131'
2025-09-03 10:31:50,031 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:44613'
2025-09-03 10:31:50,035 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:37617'
2025-09-03 10:31:50,038 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:37437'
2025-09-03 10:31:50,043 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:45781'
2025-09-03 10:31:50,055 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:45505'
2025-09-03 10:31:50,059 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:46005'
2025-09-03 10:31:50,064 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:43655'
2025-09-03 10:31:50,067 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:33355'
2025-09-03 10:31:50,074 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:37863'
2025-09-03 10:31:50,078 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:34649'
2025-09-03 10:31:50,085 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:38021'
2025-09-03 10:31:50,090 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:40921'
2025-09-03 10:31:50,095 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:36793'
2025-09-03 10:31:50,101 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:45175'
2025-09-03 10:31:50,105 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:36093'
2025-09-03 10:31:50,111 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:33133'
2025-09-03 10:31:50,117 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:42861'
2025-09-03 10:31:50,123 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:46393'
2025-09-03 10:31:50,964 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:41523
2025-09-03 10:31:50,964 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:41485
2025-09-03 10:31:50,964 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:41523
2025-09-03 10:31:50,964 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:46069
2025-09-03 10:31:50,964 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:45045
2025-09-03 10:31:50,964 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:37565
2025-09-03 10:31:50,964 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:41485
2025-09-03 10:31:50,964 - distributed.worker - INFO -          dashboard at:          10.6.102.27:40459
2025-09-03 10:31:50,964 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:45041
2025-09-03 10:31:50,964 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:46069
2025-09-03 10:31:50,964 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:45045
2025-09-03 10:31:50,964 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:40523
2025-09-03 10:31:50,964 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:37565
2025-09-03 10:31:50,964 - distributed.worker - INFO -          dashboard at:          10.6.102.27:43021
2025-09-03 10:31:50,964 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,964 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:45041
2025-09-03 10:31:50,964 - distributed.worker - INFO -          dashboard at:          10.6.102.27:39713
2025-09-03 10:31:50,964 - distributed.worker - INFO -          dashboard at:          10.6.102.27:32773
2025-09-03 10:31:50,964 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:40523
2025-09-03 10:31:50,964 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,964 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,964 - distributed.worker - INFO -          dashboard at:          10.6.102.27:45219
2025-09-03 10:31:50,964 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,964 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,964 - distributed.worker - INFO -          dashboard at:          10.6.102.27:34829
2025-09-03 10:31:50,964 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,964 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:50,964 - distributed.worker - INFO -          dashboard at:          10.6.102.27:40377
2025-09-03 10:31:50,964 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,964 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,964 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:50,964 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,964 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,964 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:50,964 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,964 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,964 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:50,964 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:50,964 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:50,964 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,964 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:50,964 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,964 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-g3dmr9dm
2025-09-03 10:31:50,964 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:50,964 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:50,964 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:50,964 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-3l1duml_
2025-09-03 10:31:50,964 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:50,964 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:50,964 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,964 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ererl06m
2025-09-03 10:31:50,964 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:50,964 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-btge6equ
2025-09-03 10:31:50,964 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,964 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:50,964 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-l3pmiu2v
2025-09-03 10:31:50,964 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,964 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-0c_1d_ie
2025-09-03 10:31:50,964 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,964 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-lzv5_xtr
2025-09-03 10:31:50,964 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,964 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,964 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,965 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:39337
2025-09-03 10:31:50,966 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:39337
2025-09-03 10:31:50,966 - distributed.worker - INFO -          dashboard at:          10.6.102.27:38035
2025-09-03 10:31:50,966 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,966 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,966 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:50,966 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:50,966 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-c32f3tzq
2025-09-03 10:31:50,966 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,967 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:38739
2025-09-03 10:31:50,967 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:38739
2025-09-03 10:31:50,967 - distributed.worker - INFO -          dashboard at:          10.6.102.27:33671
2025-09-03 10:31:50,967 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,967 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,968 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:50,968 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:50,968 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-us7vls9r
2025-09-03 10:31:50,968 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,968 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:33113
2025-09-03 10:31:50,968 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:33113
2025-09-03 10:31:50,968 - distributed.worker - INFO -          dashboard at:          10.6.102.27:36367
2025-09-03 10:31:50,968 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,968 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,968 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:50,968 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:50,968 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-00uxtxrd
2025-09-03 10:31:50,968 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,971 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:40089
2025-09-03 10:31:50,971 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:40089
2025-09-03 10:31:50,971 - distributed.worker - INFO -          dashboard at:          10.6.102.27:34231
2025-09-03 10:31:50,971 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:50,971 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:50,971 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:50,971 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:50,971 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-5c3y1ve6
2025-09-03 10:31:50,971 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,003 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:41235
2025-09-03 10:31:51,003 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:41235
2025-09-03 10:31:51,003 - distributed.worker - INFO -          dashboard at:          10.6.102.27:35753
2025-09-03 10:31:51,003 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,003 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,003 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,003 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,003 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-0jnsrxzr
2025-09-03 10:31:51,003 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,013 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:45201
2025-09-03 10:31:51,014 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:45201
2025-09-03 10:31:51,014 - distributed.worker - INFO -          dashboard at:          10.6.102.27:40179
2025-09-03 10:31:51,014 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,014 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,014 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,014 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,014 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-lyp3ivpo
2025-09-03 10:31:51,014 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,020 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:43369
2025-09-03 10:31:51,020 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:43369
2025-09-03 10:31:51,020 - distributed.worker - INFO -          dashboard at:          10.6.102.27:44725
2025-09-03 10:31:51,020 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,020 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,020 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,020 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,020 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-y67sb_aa
2025-09-03 10:31:51,020 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,025 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:43195
2025-09-03 10:31:51,026 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:43195
2025-09-03 10:31:51,026 - distributed.worker - INFO -          dashboard at:          10.6.102.27:36559
2025-09-03 10:31:51,026 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,026 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,026 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,026 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,026 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-sk83prz3
2025-09-03 10:31:51,026 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,038 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:37533
2025-09-03 10:31:51,038 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:37533
2025-09-03 10:31:51,038 - distributed.worker - INFO -          dashboard at:          10.6.102.27:45479
2025-09-03 10:31:51,038 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,038 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,038 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,038 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,038 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-5599b_m7
2025-09-03 10:31:51,038 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,040 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:36197
2025-09-03 10:31:51,041 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:36197
2025-09-03 10:31:51,041 - distributed.worker - INFO -          dashboard at:          10.6.102.27:34345
2025-09-03 10:31:51,041 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,041 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,041 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,041 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,041 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-anj6v4o5
2025-09-03 10:31:51,041 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,046 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:37373
2025-09-03 10:31:51,046 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:37373
2025-09-03 10:31:51,046 - distributed.worker - INFO -          dashboard at:          10.6.102.27:32899
2025-09-03 10:31:51,046 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,047 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,047 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,047 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,047 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-5h4fakmr
2025-09-03 10:31:51,047 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,083 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:38861
2025-09-03 10:31:51,083 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:38861
2025-09-03 10:31:51,083 - distributed.worker - INFO -          dashboard at:          10.6.102.27:38057
2025-09-03 10:31:51,083 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,083 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,083 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,083 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,083 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-7a80uflz
2025-09-03 10:31:51,083 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,087 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:44899
2025-09-03 10:31:51,087 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:44899
2025-09-03 10:31:51,087 - distributed.worker - INFO -          dashboard at:          10.6.102.27:35333
2025-09-03 10:31:51,087 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,087 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,087 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,087 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,087 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-e2vlq0go
2025-09-03 10:31:51,087 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,197 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:40999
2025-09-03 10:31:51,197 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:40999
2025-09-03 10:31:51,197 - distributed.worker - INFO -          dashboard at:          10.6.102.27:43157
2025-09-03 10:31:51,197 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,197 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,197 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,197 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,197 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-_qx9jfmp
2025-09-03 10:31:51,197 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,240 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:35733
2025-09-03 10:31:51,240 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:35733
2025-09-03 10:31:51,240 - distributed.worker - INFO -          dashboard at:          10.6.102.27:45381
2025-09-03 10:31:51,240 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,240 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,240 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,240 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,240 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-5eaw03u5
2025-09-03 10:31:51,240 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,243 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:44737
2025-09-03 10:31:51,243 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:44737
2025-09-03 10:31:51,243 - distributed.worker - INFO -          dashboard at:          10.6.102.27:43223
2025-09-03 10:31:51,243 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,243 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,243 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,243 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,243 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-2kpwfq06
2025-09-03 10:31:51,243 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,306 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:36501
2025-09-03 10:31:51,306 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:36501
2025-09-03 10:31:51,306 - distributed.worker - INFO -          dashboard at:          10.6.102.27:39879
2025-09-03 10:31:51,306 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,306 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,306 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,306 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,306 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-6_1f0g53
2025-09-03 10:31:51,306 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,346 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:36891
2025-09-03 10:31:51,346 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:36891
2025-09-03 10:31:51,346 - distributed.worker - INFO -          dashboard at:          10.6.102.27:42515
2025-09-03 10:31:51,346 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,346 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,346 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,346 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,346 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-c8e1z931
2025-09-03 10:31:51,346 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,384 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:44925
2025-09-03 10:31:51,384 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:44925
2025-09-03 10:31:51,384 - distributed.worker - INFO -          dashboard at:          10.6.102.27:33681
2025-09-03 10:31:51,384 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,384 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,384 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,384 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,384 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-dff6iy3z
2025-09-03 10:31:51,384 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,392 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:51,393 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,395 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:51,403 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:37821
2025-09-03 10:31:51,403 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:37821
2025-09-03 10:31:51,404 - distributed.worker - INFO -          dashboard at:          10.6.102.27:46681
2025-09-03 10:31:51,404 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,404 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,404 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,404 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,404 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-4fvmepsz
2025-09-03 10:31:51,404 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,404 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:51,406 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,406 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,407 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:51,411 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:33159
2025-09-03 10:31:51,411 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:33159
2025-09-03 10:31:51,411 - distributed.worker - INFO -          dashboard at:          10.6.102.27:38953
2025-09-03 10:31:51,411 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,411 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,411 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,411 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,411 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-5vu48n6w
2025-09-03 10:31:51,411 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,417 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:43083
2025-09-03 10:31:51,417 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:43083
2025-09-03 10:31:51,417 - distributed.worker - INFO -          dashboard at:          10.6.102.27:41743
2025-09-03 10:31:51,417 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,417 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,417 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,417 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,417 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-wjlsq4lh
2025-09-03 10:31:51,417 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,420 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:51,422 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,422 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,425 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:51,431 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:51,432 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,432 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,434 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:51,434 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:39407
2025-09-03 10:31:51,434 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:39407
2025-09-03 10:31:51,434 - distributed.worker - INFO -          dashboard at:          10.6.102.27:37009
2025-09-03 10:31:51,434 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,434 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,434 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,434 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,434 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-zk3wx4j2
2025-09-03 10:31:51,434 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,437 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:44453
2025-09-03 10:31:51,437 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:44453
2025-09-03 10:31:51,437 - distributed.worker - INFO -          dashboard at:          10.6.102.27:40149
2025-09-03 10:31:51,437 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,437 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,437 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,437 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,437 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-uyu2bgm8
2025-09-03 10:31:51,437 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,443 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:40359
2025-09-03 10:31:51,443 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:40359
2025-09-03 10:31:51,443 - distributed.worker - INFO -          dashboard at:          10.6.102.27:34759
2025-09-03 10:31:51,443 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,443 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,443 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,443 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,443 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-z8yhxcbz
2025-09-03 10:31:51,443 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,447 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:41313
2025-09-03 10:31:51,447 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:41313
2025-09-03 10:31:51,448 - distributed.worker - INFO -          dashboard at:          10.6.102.27:46787
2025-09-03 10:31:51,448 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,448 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,448 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,448 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,448 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-58lp8bu7
2025-09-03 10:31:51,448 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,457 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:35595
2025-09-03 10:31:51,457 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:35595
2025-09-03 10:31:51,457 - distributed.worker - INFO -          dashboard at:          10.6.102.27:45195
2025-09-03 10:31:51,458 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,458 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,458 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,458 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,458 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-s5k7s8nw
2025-09-03 10:31:51,458 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,460 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:44189
2025-09-03 10:31:51,461 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:44189
2025-09-03 10:31:51,461 - distributed.worker - INFO -          dashboard at:          10.6.102.27:33479
2025-09-03 10:31:51,461 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,461 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,461 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,461 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,461 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-m3t4g5kk
2025-09-03 10:31:51,461 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,489 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:46573
2025-09-03 10:31:51,489 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:46573
2025-09-03 10:31:51,489 - distributed.worker - INFO -          dashboard at:          10.6.102.27:32825
2025-09-03 10:31:51,489 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,489 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,489 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,489 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,489 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-lyse67mn
2025-09-03 10:31:51,489 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,504 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:51,506 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,506 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,507 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:51,544 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:43595
2025-09-03 10:31:51,544 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:43595
2025-09-03 10:31:51,544 - distributed.worker - INFO -          dashboard at:          10.6.102.27:45497
2025-09-03 10:31:51,544 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,544 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,544 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,544 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,544 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-b74ydkgh
2025-09-03 10:31:51,544 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,727 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:38315
2025-09-03 10:31:51,727 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:38315
2025-09-03 10:31:51,727 - distributed.worker - INFO -          dashboard at:          10.6.102.27:36765
2025-09-03 10:31:51,727 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,727 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,727 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,727 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,727 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-72_1u0mx
2025-09-03 10:31:51,727 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,749 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:35215
2025-09-03 10:31:51,749 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:35215
2025-09-03 10:31:51,750 - distributed.worker - INFO -          dashboard at:          10.6.102.27:43353
2025-09-03 10:31:51,750 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,750 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,750 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,750 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,750 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-5sckmnq1
2025-09-03 10:31:51,750 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,753 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:40311
2025-09-03 10:31:51,753 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:40311
2025-09-03 10:31:51,753 - distributed.worker - INFO -          dashboard at:          10.6.102.27:45471
2025-09-03 10:31:51,753 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,753 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,753 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,753 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,753 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-4lj9ietq
2025-09-03 10:31:51,753 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,771 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:39389
2025-09-03 10:31:51,771 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:39389
2025-09-03 10:31:51,771 - distributed.worker - INFO -          dashboard at:          10.6.102.27:46273
2025-09-03 10:31:51,771 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,771 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,771 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,771 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,771 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-upfp2q7t
2025-09-03 10:31:51,771 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,773 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:42309
2025-09-03 10:31:51,774 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:42309
2025-09-03 10:31:51,774 - distributed.worker - INFO -          dashboard at:          10.6.102.27:40683
2025-09-03 10:31:51,774 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,774 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,774 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,774 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,774 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-guhb3o6v
2025-09-03 10:31:51,774 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,828 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:37297
2025-09-03 10:31:51,828 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:37297
2025-09-03 10:31:51,828 - distributed.worker - INFO -          dashboard at:          10.6.102.27:36455
2025-09-03 10:31:51,828 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,828 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,828 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,828 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,828 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-o3of9sgj
2025-09-03 10:31:51,828 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,872 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:32977
2025-09-03 10:31:51,872 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:32977
2025-09-03 10:31:51,872 - distributed.worker - INFO -          dashboard at:          10.6.102.27:34419
2025-09-03 10:31:51,872 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,873 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,873 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,873 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,873 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-tua9791x
2025-09-03 10:31:51,873 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,881 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:41681
2025-09-03 10:31:51,881 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:41681
2025-09-03 10:31:51,881 - distributed.worker - INFO -          dashboard at:          10.6.102.27:34767
2025-09-03 10:31:51,881 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,881 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,881 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,881 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,881 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-cj_zsylm
2025-09-03 10:31:51,881 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,911 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:42555
2025-09-03 10:31:51,911 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:42555
2025-09-03 10:31:51,911 - distributed.worker - INFO -          dashboard at:          10.6.102.27:39725
2025-09-03 10:31:51,911 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,911 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,911 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,912 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,912 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-oe454ot0
2025-09-03 10:31:51,912 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,927 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:42009
2025-09-03 10:31:51,927 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:42009
2025-09-03 10:31:51,927 - distributed.worker - INFO -          dashboard at:          10.6.102.27:41683
2025-09-03 10:31:51,927 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,927 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,927 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,927 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,927 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-_vh9zdlo
2025-09-03 10:31:51,927 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,947 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:38193
2025-09-03 10:31:51,947 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:38193
2025-09-03 10:31:51,947 - distributed.worker - INFO -          dashboard at:          10.6.102.27:38637
2025-09-03 10:31:51,947 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,947 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,947 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,947 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,947 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-737fyop_
2025-09-03 10:31:51,947 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,949 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:32931
2025-09-03 10:31:51,950 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:32931
2025-09-03 10:31:51,950 - distributed.worker - INFO -          dashboard at:          10.6.102.27:40857
2025-09-03 10:31:51,950 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,950 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,950 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,950 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,950 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-0s0lrkw3
2025-09-03 10:31:51,950 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,982 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:40269
2025-09-03 10:31:51,982 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:40269
2025-09-03 10:31:51,982 - distributed.worker - INFO -          dashboard at:          10.6.102.27:39399
2025-09-03 10:31:51,982 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,982 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,982 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,982 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,982 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-t9y2laku
2025-09-03 10:31:51,982 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,995 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:32775
2025-09-03 10:31:51,995 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:32775
2025-09-03 10:31:51,995 - distributed.worker - INFO -          dashboard at:          10.6.102.27:42531
2025-09-03 10:31:51,995 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,995 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,995 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,995 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,995 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-0o4m0f66
2025-09-03 10:31:51,995 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,998 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:37831
2025-09-03 10:31:51,998 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:37831
2025-09-03 10:31:51,998 - distributed.worker - INFO -          dashboard at:          10.6.102.27:45199
2025-09-03 10:31:51,998 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:51,998 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:51,998 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:51,998 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:51,998 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-mvbt0thq
2025-09-03 10:31:51,998 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,000 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:40599
2025-09-03 10:31:52,000 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:40599
2025-09-03 10:31:52,000 - distributed.worker - INFO -          dashboard at:          10.6.102.27:36833
2025-09-03 10:31:52,000 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,000 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,000 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,000 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,000 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-mlup45nh
2025-09-03 10:31:52,000 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,003 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:45465
2025-09-03 10:31:52,003 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:45465
2025-09-03 10:31:52,003 - distributed.worker - INFO -          dashboard at:          10.6.102.27:33165
2025-09-03 10:31:52,003 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,003 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,003 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,003 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,003 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-g0vul27u
2025-09-03 10:31:52,003 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,006 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:34151
2025-09-03 10:31:52,006 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:34151
2025-09-03 10:31:52,006 - distributed.worker - INFO -          dashboard at:          10.6.102.27:36221
2025-09-03 10:31:52,006 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,006 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,006 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,006 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,006 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-uz9n_q37
2025-09-03 10:31:52,007 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,008 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:38823
2025-09-03 10:31:52,008 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:38823
2025-09-03 10:31:52,008 - distributed.worker - INFO -          dashboard at:          10.6.102.27:38365
2025-09-03 10:31:52,008 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,008 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,008 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,008 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,008 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-3t1s0v0j
2025-09-03 10:31:52,008 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,013 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:34613
2025-09-03 10:31:52,013 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:34613
2025-09-03 10:31:52,013 - distributed.worker - INFO -          dashboard at:          10.6.102.27:36423
2025-09-03 10:31:52,013 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,013 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,013 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,013 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,013 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-64uwi_1g
2025-09-03 10:31:52,013 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,124 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:42415
2025-09-03 10:31:52,124 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:42415
2025-09-03 10:31:52,124 - distributed.worker - INFO -          dashboard at:          10.6.102.27:35815
2025-09-03 10:31:52,124 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,124 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,124 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,124 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,124 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-yugkhns7
2025-09-03 10:31:52,124 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,136 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:34031
2025-09-03 10:31:52,136 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:34031
2025-09-03 10:31:52,136 - distributed.worker - INFO -          dashboard at:          10.6.102.27:37515
2025-09-03 10:31:52,136 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,136 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,136 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,136 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,136 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-6569iw2m
2025-09-03 10:31:52,136 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,143 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:44365
2025-09-03 10:31:52,143 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:44365
2025-09-03 10:31:52,143 - distributed.worker - INFO -          dashboard at:          10.6.102.27:45811
2025-09-03 10:31:52,143 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,143 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,143 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,143 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,143 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-lw8kuj42
2025-09-03 10:31:52,143 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,157 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:34131
2025-09-03 10:31:52,157 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:34131
2025-09-03 10:31:52,157 - distributed.worker - INFO -          dashboard at:          10.6.102.27:36509
2025-09-03 10:31:52,157 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,157 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,157 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,157 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,157 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-1a_ll8mz
2025-09-03 10:31:52,157 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,209 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:42971
2025-09-03 10:31:52,209 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:42971
2025-09-03 10:31:52,209 - distributed.worker - INFO -          dashboard at:          10.6.102.27:35863
2025-09-03 10:31:52,209 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,209 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,209 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,209 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,209 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-x2uo4nnr
2025-09-03 10:31:52,209 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,223 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:41939
2025-09-03 10:31:52,223 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:41939
2025-09-03 10:31:52,223 - distributed.worker - INFO -          dashboard at:          10.6.102.27:37811
2025-09-03 10:31:52,223 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,223 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,223 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,223 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,223 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-tjjta26f
2025-09-03 10:31:52,223 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,260 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:34361
2025-09-03 10:31:52,260 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:34361
2025-09-03 10:31:52,260 - distributed.worker - INFO -          dashboard at:          10.6.102.27:46801
2025-09-03 10:31:52,260 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,260 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,260 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,260 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,260 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-_ssfsaxy
2025-09-03 10:31:52,260 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,354 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:43233
2025-09-03 10:31:52,354 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:43233
2025-09-03 10:31:52,354 - distributed.worker - INFO -          dashboard at:          10.6.102.27:36859
2025-09-03 10:31:52,354 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,354 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,354 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,354 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,354 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-2f0qlx04
2025-09-03 10:31:52,354 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,365 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:44901
2025-09-03 10:31:52,366 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:44901
2025-09-03 10:31:52,366 - distributed.worker - INFO -          dashboard at:          10.6.102.27:36031
2025-09-03 10:31:52,366 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,366 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,366 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,366 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,366 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-pkqm2o85
2025-09-03 10:31:52,366 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,368 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:44633
2025-09-03 10:31:52,368 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:44633
2025-09-03 10:31:52,368 - distributed.worker - INFO -          dashboard at:          10.6.102.27:39703
2025-09-03 10:31:52,368 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,368 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,368 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,368 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,368 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-riva1w4l
2025-09-03 10:31:52,368 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,370 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:32981
2025-09-03 10:31:52,370 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:32981
2025-09-03 10:31:52,370 - distributed.worker - INFO -          dashboard at:          10.6.102.27:44081
2025-09-03 10:31:52,370 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,370 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,370 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,370 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,370 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-1o8u6sqb
2025-09-03 10:31:52,371 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,374 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:34553
2025-09-03 10:31:52,374 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:34553
2025-09-03 10:31:52,374 - distributed.worker - INFO -          dashboard at:          10.6.102.27:43203
2025-09-03 10:31:52,374 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,374 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,374 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,374 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,374 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-uaxqcv2d
2025-09-03 10:31:52,374 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,378 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:35465
2025-09-03 10:31:52,378 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:35465
2025-09-03 10:31:52,378 - distributed.worker - INFO -          dashboard at:          10.6.102.27:45627
2025-09-03 10:31:52,378 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,378 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,378 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,378 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,378 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-rxedmna4
2025-09-03 10:31:52,378 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,380 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:40963
2025-09-03 10:31:52,380 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:40963
2025-09-03 10:31:52,380 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:46299
2025-09-03 10:31:52,380 - distributed.worker - INFO -          dashboard at:          10.6.102.27:41943
2025-09-03 10:31:52,380 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:46299
2025-09-03 10:31:52,380 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,380 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,380 - distributed.worker - INFO -          dashboard at:          10.6.102.27:40231
2025-09-03 10:31:52,380 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,380 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,380 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,380 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,380 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,380 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-lpidnrju
2025-09-03 10:31:52,380 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,380 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,380 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-_i3o8436
2025-09-03 10:31:52,380 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,380 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:42775
2025-09-03 10:31:52,380 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:42775
2025-09-03 10:31:52,380 - distributed.worker - INFO -          dashboard at:          10.6.102.27:38465
2025-09-03 10:31:52,380 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,380 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,380 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,380 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,380 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-qo9v7h0h
2025-09-03 10:31:52,380 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,393 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:34283
2025-09-03 10:31:52,393 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:34283
2025-09-03 10:31:52,393 - distributed.worker - INFO -          dashboard at:          10.6.102.27:45147
2025-09-03 10:31:52,393 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,393 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,393 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,393 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-gtoljynv
2025-09-03 10:31:52,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,397 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:37483
2025-09-03 10:31:52,397 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:37483
2025-09-03 10:31:52,397 - distributed.worker - INFO -          dashboard at:          10.6.102.27:38249
2025-09-03 10:31:52,397 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,397 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,397 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,397 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,397 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-gk_gu_tz
2025-09-03 10:31:52,397 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,402 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:39335
2025-09-03 10:31:52,402 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:39335
2025-09-03 10:31:52,402 - distributed.worker - INFO -          dashboard at:          10.6.102.27:42625
2025-09-03 10:31:52,402 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,402 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,402 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,402 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,402 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-a3fusqrq
2025-09-03 10:31:52,402 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,405 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:38095
2025-09-03 10:31:52,405 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:38095
2025-09-03 10:31:52,405 - distributed.worker - INFO -          dashboard at:          10.6.102.27:34955
2025-09-03 10:31:52,405 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,405 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,405 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,405 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,405 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-or8tv03r
2025-09-03 10:31:52,405 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,408 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:36745
2025-09-03 10:31:52,408 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:36745
2025-09-03 10:31:52,408 - distributed.worker - INFO -          dashboard at:          10.6.102.27:43247
2025-09-03 10:31:52,408 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,408 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,408 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,408 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-sd56apr_
2025-09-03 10:31:52,409 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,419 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:41467
2025-09-03 10:31:52,419 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:41467
2025-09-03 10:31:52,419 - distributed.worker - INFO -          dashboard at:          10.6.102.27:43823
2025-09-03 10:31:52,419 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,419 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,419 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,419 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,419 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-oj8k8l3f
2025-09-03 10:31:52,419 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,420 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:39501
2025-09-03 10:31:52,420 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:39501
2025-09-03 10:31:52,420 - distributed.worker - INFO -          dashboard at:          10.6.102.27:37185
2025-09-03 10:31:52,420 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,420 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,420 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,420 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,420 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-4yi4ot10
2025-09-03 10:31:52,420 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,422 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:43185
2025-09-03 10:31:52,422 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:43185
2025-09-03 10:31:52,422 - distributed.worker - INFO -          dashboard at:          10.6.102.27:34101
2025-09-03 10:31:52,423 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,423 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,423 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,423 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,423 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-bmllvysf
2025-09-03 10:31:52,423 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,439 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:45133
2025-09-03 10:31:52,439 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:45133
2025-09-03 10:31:52,439 - distributed.worker - INFO -          dashboard at:          10.6.102.27:33403
2025-09-03 10:31:52,439 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,439 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,439 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,439 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,439 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-5x07za15
2025-09-03 10:31:52,439 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,485 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:35775
2025-09-03 10:31:52,485 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:35775
2025-09-03 10:31:52,485 - distributed.worker - INFO -          dashboard at:          10.6.102.27:44307
2025-09-03 10:31:52,485 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,485 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,485 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,485 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,485 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-3wiyw3qa
2025-09-03 10:31:52,485 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,534 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:42655
2025-09-03 10:31:52,534 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:42655
2025-09-03 10:31:52,534 - distributed.worker - INFO -          dashboard at:          10.6.102.27:46859
2025-09-03 10:31:52,534 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,534 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,534 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,534 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,534 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-rnuvp1eq
2025-09-03 10:31:52,534 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,539 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:46317
2025-09-03 10:31:52,539 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:46317
2025-09-03 10:31:52,539 - distributed.worker - INFO -          dashboard at:          10.6.102.27:34433
2025-09-03 10:31:52,539 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,539 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,539 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,540 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,540 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-htu0cdki
2025-09-03 10:31:52,540 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,639 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:33297
2025-09-03 10:31:52,639 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:33297
2025-09-03 10:31:52,639 - distributed.worker - INFO -          dashboard at:          10.6.102.27:46537
2025-09-03 10:31:52,639 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,639 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,639 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,639 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,639 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-5ayiliqb
2025-09-03 10:31:52,639 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,641 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:46119
2025-09-03 10:31:52,641 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:46119
2025-09-03 10:31:52,641 - distributed.worker - INFO -          dashboard at:          10.6.102.27:44829
2025-09-03 10:31:52,641 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,641 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,641 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,641 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,641 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-hbpi1s0e
2025-09-03 10:31:52,641 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,641 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:45839
2025-09-03 10:31:52,642 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:45839
2025-09-03 10:31:52,642 - distributed.worker - INFO -          dashboard at:          10.6.102.27:41781
2025-09-03 10:31:52,642 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,642 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,642 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,642 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,642 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-xish87tk
2025-09-03 10:31:52,642 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,673 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:52,674 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,674 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,675 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:52,686 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:52,687 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,687 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,689 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:52,699 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:52,700 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,701 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,702 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:52,734 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:39629
2025-09-03 10:31:52,734 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:39629
2025-09-03 10:31:52,734 - distributed.worker - INFO -          dashboard at:          10.6.102.27:42631
2025-09-03 10:31:52,734 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,734 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,734 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,734 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,734 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-1qvumjpn
2025-09-03 10:31:52,734 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,743 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:41929
2025-09-03 10:31:52,743 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:41929
2025-09-03 10:31:52,743 - distributed.worker - INFO -          dashboard at:          10.6.102.27:46435
2025-09-03 10:31:52,743 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,743 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,743 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,743 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,743 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-f9qf4i5g
2025-09-03 10:31:52,743 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,744 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:39433
2025-09-03 10:31:52,744 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:39433
2025-09-03 10:31:52,744 - distributed.worker - INFO -          dashboard at:          10.6.102.27:39851
2025-09-03 10:31:52,744 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,744 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,744 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,744 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,744 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-7q1e3zwp
2025-09-03 10:31:52,745 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,748 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:36807
2025-09-03 10:31:52,749 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:36807
2025-09-03 10:31:52,749 - distributed.worker - INFO -          dashboard at:          10.6.102.27:35469
2025-09-03 10:31:52,749 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,749 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,749 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,749 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,749 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-ehk14yyl
2025-09-03 10:31:52,749 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,749 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:43433
2025-09-03 10:31:52,750 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:43433
2025-09-03 10:31:52,750 - distributed.worker - INFO -          dashboard at:          10.6.102.27:38679
2025-09-03 10:31:52,750 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,750 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,750 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,750 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,750 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-xff5tgby
2025-09-03 10:31:52,750 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,750 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:43063
2025-09-03 10:31:52,751 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:43063
2025-09-03 10:31:52,751 - distributed.worker - INFO -          dashboard at:          10.6.102.27:41023
2025-09-03 10:31:52,751 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,751 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,751 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,751 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,751 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-3k82xz2q
2025-09-03 10:31:52,751 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,752 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:41337
2025-09-03 10:31:52,752 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:41337
2025-09-03 10:31:52,752 - distributed.worker - INFO -          dashboard at:          10.6.102.27:36979
2025-09-03 10:31:52,752 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,752 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,752 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,752 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,752 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-rdsia1hw
2025-09-03 10:31:52,752 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,762 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:45725
2025-09-03 10:31:52,762 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:45725
2025-09-03 10:31:52,762 - distributed.worker - INFO -          dashboard at:          10.6.102.27:40109
2025-09-03 10:31:52,762 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,762 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,762 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,762 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,762 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-d2q5ob7t
2025-09-03 10:31:52,762 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,769 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:33001
2025-09-03 10:31:52,769 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:33001
2025-09-03 10:31:52,769 - distributed.worker - INFO -          dashboard at:          10.6.102.27:37435
2025-09-03 10:31:52,769 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,769 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,769 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,769 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,769 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-_uapvtsh
2025-09-03 10:31:52,769 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,773 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:44097
2025-09-03 10:31:52,773 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:44097
2025-09-03 10:31:52,773 - distributed.worker - INFO -          dashboard at:          10.6.102.27:33453
2025-09-03 10:31:52,773 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,773 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,773 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:52,773 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:52,773 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-xmjbnfep
2025-09-03 10:31:52,773 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,863 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:52,864 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,864 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,865 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:52,876 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:52,877 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,877 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,879 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:52,889 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:52,891 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,891 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,892 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:52,903 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:52,904 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,904 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,906 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:52,955 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:52,956 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,956 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,958 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:52,969 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:52,970 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:52,970 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:52,972 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:53,051 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:53,052 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,053 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,054 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:53,161 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:53,162 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,162 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,163 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:53,311 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:53,312 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,312 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,314 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:53,325 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:53,327 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,327 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,328 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:53,340 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:53,341 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,341 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,342 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:53,353 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:53,354 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,354 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,356 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:53,422 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:53,423 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,424 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,425 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:53,441 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:53,442 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,442 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,444 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:53,455 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:53,456 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,456 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,458 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:53,468 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:53,469 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,470 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,471 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:53,537 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:53,538 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,538 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,540 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:53,591 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:53,592 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,592 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,594 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:53,645 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:34529'
2025-09-03 10:31:53,650 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:35999'
2025-09-03 10:31:53,654 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:33919'
2025-09-03 10:31:53,661 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:39387'
2025-09-03 10:31:53,677 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:39677'
2025-09-03 10:31:53,683 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.102.27:34643'
2025-09-03 10:31:53,783 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:53,784 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,784 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,786 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:53,968 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:53,969 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,969 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,970 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:53,981 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:53,982 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,983 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,984 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:53,997 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:53,998 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:53,998 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:53,999 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,417 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:33071
2025-09-03 10:31:54,417 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:33071
2025-09-03 10:31:54,417 - distributed.worker - INFO -          dashboard at:          10.6.102.27:39659
2025-09-03 10:31:54,417 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,417 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,417 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,417 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,417 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-s695uidm
2025-09-03 10:31:54,417 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,418 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:40659
2025-09-03 10:31:54,418 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:40659
2025-09-03 10:31:54,418 - distributed.worker - INFO -          dashboard at:          10.6.102.27:42865
2025-09-03 10:31:54,418 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,418 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,418 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,419 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,419 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-68aoa62p
2025-09-03 10:31:54,419 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,427 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,428 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,428 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,430 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,439 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:45837
2025-09-03 10:31:54,440 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:45837
2025-09-03 10:31:54,440 - distributed.worker - INFO -          dashboard at:          10.6.102.27:35349
2025-09-03 10:31:54,440 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,440 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,440 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,440 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,440 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-vlulehvv
2025-09-03 10:31:54,440 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,445 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:42263
2025-09-03 10:31:54,445 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:42263
2025-09-03 10:31:54,445 - distributed.worker - INFO -          dashboard at:          10.6.102.27:36487
2025-09-03 10:31:54,445 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,445 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,445 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,445 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,445 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-18q97f31
2025-09-03 10:31:54,446 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,446 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:40633
2025-09-03 10:31:54,446 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:40633
2025-09-03 10:31:54,446 - distributed.worker - INFO -          dashboard at:          10.6.102.27:41921
2025-09-03 10:31:54,446 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,446 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,446 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,446 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,446 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-8dn53mms
2025-09-03 10:31:54,446 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,464 - distributed.worker - INFO -       Start worker at:    tcp://10.6.102.27:34575
2025-09-03 10:31:54,464 - distributed.worker - INFO -          Listening to:    tcp://10.6.102.27:34575
2025-09-03 10:31:54,464 - distributed.worker - INFO -          dashboard at:          10.6.102.27:35319
2025-09-03 10:31:54,464 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,464 - distributed.worker - INFO -               Threads:                          1
2025-09-03 10:31:54,464 - distributed.worker - INFO -                Memory:                   4.81 GiB
2025-09-03 10:31:54,464 - distributed.worker - INFO -       Local Directory: /jobfs/148606947.gadi-pbs/dask-scratch-space/worker-0rsnn_d5
2025-09-03 10:31:54,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,717 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,718 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,718 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,720 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,788 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,789 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,789 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,791 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,802 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,803 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,803 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,805 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,816 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,817 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,817 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,819 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,830 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,831 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,831 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,833 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,844 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,845 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,845 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,847 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:54,859 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:54,860 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:54,860 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:54,862 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:55,556 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:55,558 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:55,558 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:55,560 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,176 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,180 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,180 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,186 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,190 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,194 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,194 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,200 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,202 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,203 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,204 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,206 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,218 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,222 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,223 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,227 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,232 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,236 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,236 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,242 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,244 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,245 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,245 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,247 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,258 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,259 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,259 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,261 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,272 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,274 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,274 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,277 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,335 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,336 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,336 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,338 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,362 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,363 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,364 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,365 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,377 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,378 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,378 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,380 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,391 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,393 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,393 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,394 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,405 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,406 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,407 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,408 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,419 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,420 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,421 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,423 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,434 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,435 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,435 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,437 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,448 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,449 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,450 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,451 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,462 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,464 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,464 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,466 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:56,476 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:56,478 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:56,478 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:56,480 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,695 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,697 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,697 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,699 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,786 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,788 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,788 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,790 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,887 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,888 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,889 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,890 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,902 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,904 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,904 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,905 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:58,961 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:58,962 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:58,963 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:58,964 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:59,290 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:59,291 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,292 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,293 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:59,319 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:59,321 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,321 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,323 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:59,349 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:59,350 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,351 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,352 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:59,627 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:59,629 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,629 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,631 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:59,642 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:59,644 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,644 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,646 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:59,657 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:59,658 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,659 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,660 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:59,671 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:59,673 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,673 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,675 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:59,686 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:59,687 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,687 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,688 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:31:59,756 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:31:59,757 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:31:59,757 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:31:59,759 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:00,762 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:00,764 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,764 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,765 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:00,777 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:00,779 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,779 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,780 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:00,792 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:00,793 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,794 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,795 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:00,806 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:00,807 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,808 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,809 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:00,821 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:00,822 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,822 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,824 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:00,835 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:00,837 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,837 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,839 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:00,850 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:00,852 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:00,852 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:00,854 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,516 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,518 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,518 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,520 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,530 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,532 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,532 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,534 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,546 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,547 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,548 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,549 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,561 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,563 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,563 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,564 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,576 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,578 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,578 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,580 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,958 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,960 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,960 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,962 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,974 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,975 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,975 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,977 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:02,989 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:02,990 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:02,990 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:02,992 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:03,004 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:03,005 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:03,005 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:03,007 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:21,015 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:21,022 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:21,027 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:21,039 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:21,042 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:21,048 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:21,085 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:21,088 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:21,829 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:21,874 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:21,883 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:22,125 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:22,137 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:22,144 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:22,486 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:22,510 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:22,536 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:22,541 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:23,821 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:23,825 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:23,825 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:23,831 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:23,868 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:23,880 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:23,895 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:23,908 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:23,962 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:23,975 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:24,166 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:24,700 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:24,701 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:24,701 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:24,703 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:24,716 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:24,718 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:24,718 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:24,720 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:27,184 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:27,200 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:27,208 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:27,227 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:27,241 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:27,250 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:27,266 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:27,279 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:27,516 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:27,518 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,518 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:27,520 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:27,533 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:27,535 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:27,535 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:27,536 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:29,793 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:30,296 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:30,761 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:32,220 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:32,222 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:32,222 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:32,223 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:33,964 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:33,980 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:37,965 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:37,966 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:37,966 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:37,968 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:51,123 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,129 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,140 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,144 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,150 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,187 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,189 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,931 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,976 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:51,984 - distributed.worker - INFO - Waiting to connect to:      tcp://10.6.82.69:8735
2025-09-03 10:32:53,074 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:53,075 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:53,076 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:53,077 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:53,198 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:53,199 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:53,199 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:53,201 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:53,216 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:53,217 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:53,217 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:53,218 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:54,882 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:54,894 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:54,908 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:54,961 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:54,974 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:55,398 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:55,399 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:55,399 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:55,401 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:55,452 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:55,453 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:55,454 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:55,455 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:57,288 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:57,289 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:57,289 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:57,291 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:57,312 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:57,313 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:57,313 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:57,315 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:57,333 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:57,334 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:57,334 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:57,336 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:57,353 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:57,354 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:57,354 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:57,356 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:57,372 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-03 10:32:57,373 - distributed.worker - INFO -         Registered to:      tcp://10.6.82.69:8735
2025-09-03 10:32:57,373 - distributed.worker - INFO - -------------------------------------------------
2025-09-03 10:32:57,374 - distributed.core - INFO - Starting established connection to tcp://10.6.82.69:8735
2025-09-03 10:32:58,521 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:32:58,538 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:33:01,295 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 547, in connect
    stream = await self.client.connect(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/tcpclient.py", line 279, in connect
    af, addr, stream = await connector.start(connect_timeout=timeout)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1922, in wait_for
    async with asyncio.timeout(timeout):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 368, in connect
    raise OSError(
OSError: Timed out trying to connect to tcp://10.6.82.69:8735 after 30 s
2025-09-03 10:35:51,671 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,673 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:51,882 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:51,883 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,046 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,047 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,051 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,053 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,053 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,054 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,055 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,060 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,098 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,100 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,118 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,119 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:52,119 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:52,124 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,009 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,014 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,059 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,060 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,065 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,072 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,081 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,083 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,089 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,090 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,091 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,092 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,120 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,121 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,121 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,122 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,317 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,320 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,322 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,327 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,400 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,405 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,414 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,415 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,446 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,450 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,462 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,469 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,474 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,481 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,969 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,973 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:53,974 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:53,975 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,054 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,055 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,060 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,065 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,249 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,252 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,252 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,257 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,273 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,274 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,317 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,319 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,357 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,360 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,373 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,381 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,480 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,482 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,813 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,819 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,903 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,905 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:54,935 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:54,939 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,090 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,093 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,150 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,150 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,154 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,155 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,174 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,175 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,228 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,229 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,315 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,317 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,401 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,402 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,410 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,412 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,437 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,442 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,445 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,447 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,673 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,674 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,785 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,791 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,801 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,803 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,816 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,822 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,843 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,845 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,876 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,877 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,941 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,942 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:55,963 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:55,969 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,038 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,040 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,062 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,065 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,065 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,071 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,093 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,095 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,106 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,111 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,146 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,149 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,167 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,168 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,167 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,173 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,292 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,295 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,298 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,299 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,566 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,568 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,656 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,659 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,709 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,714 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,727 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,728 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,743 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,746 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,813 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,820 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,893 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,899 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,921 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,922 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,955 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,957 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:56,979 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:56,980 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,228 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,230 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,234 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,236 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:57,750 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:57,756 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,337 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,339 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,358 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,359 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:58,823 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:58,832 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,104 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,109 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,230 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,231 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,236 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,237 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,524 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,526 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:35:59,537 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:35:59,540 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,008 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,011 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,129 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,135 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:00,170 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:00,175 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,361 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,363 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,416 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,420 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:01,547 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:01,551 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,010 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,013 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,161 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,162 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,518 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,521 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,580 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,583 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,644 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,646 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:02,807 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:02,809 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:03,170 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:03,175 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:06,771 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:06,773 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:09,731 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:09,733 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:11,745 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:11,752 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:12,829 - distributed.worker - INFO - Starting Worker plugin qme_utils.pyb42042ae-8929-4a91-b24e-81cc14c9ec9c
2025-09-03 10:36:12,832 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-03 10:36:22,904 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,906 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,907 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,909 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,910 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,912 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,915 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,916 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,917 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,917 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,918 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,919 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,933 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,935 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,936 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,938 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,938 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,940 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:22,955 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:22,957 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,387 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,387 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,389 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,389 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,388 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,391 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,394 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,396 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,500 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,501 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,500 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,502 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,503 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,505 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,509 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,510 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,511 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,512 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,527 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,529 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,539 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,541 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,544 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,546 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,545 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,547 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,564 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,567 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,568 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,569 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,569 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,570 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,782 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,784 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,793 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,795 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,796 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,798 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,803 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,806 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,837 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,839 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,843 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,845 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,847 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,848 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,852 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,854 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,882 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,884 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,888 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,890 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,895 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,896 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,897 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,896 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:23,898 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:23,898 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,032 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,034 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,036 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,039 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,064 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,066 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,076 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,078 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,082 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,083 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,084 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,085 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,091 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,096 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,101 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,106 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,125 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,127 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,130 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,135 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,235 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,237 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,335 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,337 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,384 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,386 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,388 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,389 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,386 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,391 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,391 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,393 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,401 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,404 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,403 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,409 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,410 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,412 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,432 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,434 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,452 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,454 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,455 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,457 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,458 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,459 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,504 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,504 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,506 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,506 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,586 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,591 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,591 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,593 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,626 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,631 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,632 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,634 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,638 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,639 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,641 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,640 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,642 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,645 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,663 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,665 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,666 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,667 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,672 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,674 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,682 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,682 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,684 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,684 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,746 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,748 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,750 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,752 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,855 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,858 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,899 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,900 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,920 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,922 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,935 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,937 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,939 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,941 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,944 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,952 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,985 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,989 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:24,993 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:24,994 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,015 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,018 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,018 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,022 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,041 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,043 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,064 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,069 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,135 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,138 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,220 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,222 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,445 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,447 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,492 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,494 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,628 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,630 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,672 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,674 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,722 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,724 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,894 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,899 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:25,938 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:25,941 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,091 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,093 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,149 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,152 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,556 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,558 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:26,870 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:26,872 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-03 10:36:28,371 - distributed.worker - INFO - Starting Worker plugin qme_vars.pyec4441cf-c514-4464-8e81-e2d1e307d220
2025-09-03 10:36:28,373 - distributed.utils - INFO - Reload module qme_vars from .py file
