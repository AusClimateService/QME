Loading singularity
Loading conda/analysis3-25.06
Loading gadi_jupyterlab/23.02
2025-09-05 09:11:30,467 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:36339'
2025-09-05 09:11:30,476 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:45711'
2025-09-05 09:11:30,480 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:34513'
2025-09-05 09:11:30,484 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:42615'
2025-09-05 09:11:30,488 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:36309'
2025-09-05 09:11:30,492 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:42825'
2025-09-05 09:11:30,495 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:36995'
2025-09-05 09:11:30,500 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:38379'
2025-09-05 09:11:30,503 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:35075'
2025-09-05 09:11:30,508 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:40049'
2025-09-05 09:11:30,514 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:43897'
2025-09-05 09:11:30,518 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:42725'
2025-09-05 09:11:30,522 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:43615'
2025-09-05 09:11:30,527 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:37917'
2025-09-05 09:11:30,531 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:38977'
2025-09-05 09:11:30,535 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:33183'
2025-09-05 09:11:30,540 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:45187'
2025-09-05 09:11:30,545 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:39043'
2025-09-05 09:11:30,549 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:35201'
2025-09-05 09:11:30,553 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:45183'
2025-09-05 09:11:30,621 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:40333'
2025-09-05 09:11:30,626 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:38965'
2025-09-05 09:11:30,630 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:40017'
2025-09-05 09:11:30,634 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:40101'
2025-09-05 09:11:30,637 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:43239'
2025-09-05 09:11:30,642 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:40387'
2025-09-05 09:11:30,647 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:43135'
2025-09-05 09:11:30,651 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:36269'
2025-09-05 09:11:30,656 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:43941'
2025-09-05 09:11:30,660 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:40377'
2025-09-05 09:11:30,665 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:41917'
2025-09-05 09:11:30,669 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:33463'
2025-09-05 09:11:30,674 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:39221'
2025-09-05 09:11:30,679 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:36759'
2025-09-05 09:11:30,684 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:39219'
2025-09-05 09:11:30,690 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:32803'
2025-09-05 09:11:30,694 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:36105'
2025-09-05 09:11:30,699 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:42355'
2025-09-05 09:11:30,703 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:38159'
2025-09-05 09:11:30,708 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:35721'
2025-09-05 09:11:30,712 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:33653'
2025-09-05 09:11:30,716 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:41599'
2025-09-05 09:11:30,720 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:33829'
2025-09-05 09:11:30,726 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:36875'
2025-09-05 09:11:30,730 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:36651'
2025-09-05 09:11:30,734 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:37537'
2025-09-05 09:11:30,738 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:39681'
2025-09-05 09:11:30,744 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:46221'
2025-09-05 09:11:30,748 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:45361'
2025-09-05 09:11:30,753 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:42377'
2025-09-05 09:11:30,758 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:37859'
2025-09-05 09:11:30,762 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.6.101.40:41663'
2025-09-05 09:11:31,877 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:36831
2025-09-05 09:11:31,877 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:37407
2025-09-05 09:11:31,877 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:42281
2025-09-05 09:11:31,877 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:44565
2025-09-05 09:11:31,877 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:34695
2025-09-05 09:11:31,877 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:36831
2025-09-05 09:11:31,877 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:38177
2025-09-05 09:11:31,877 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:37407
2025-09-05 09:11:31,877 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:34743
2025-09-05 09:11:31,877 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:34127
2025-09-05 09:11:31,877 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:42281
2025-09-05 09:11:31,877 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:44565
2025-09-05 09:11:31,877 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:34695
2025-09-05 09:11:31,877 - distributed.worker - INFO -          dashboard at:          10.6.101.40:44319
2025-09-05 09:11:31,877 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:38177
2025-09-05 09:11:31,877 - distributed.worker - INFO -          dashboard at:          10.6.101.40:42157
2025-09-05 09:11:31,877 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:34743
2025-09-05 09:11:31,877 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:34127
2025-09-05 09:11:31,877 - distributed.worker - INFO -          dashboard at:          10.6.101.40:37435
2025-09-05 09:11:31,877 - distributed.worker - INFO -          dashboard at:          10.6.101.40:35999
2025-09-05 09:11:31,877 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,877 - distributed.worker - INFO -          dashboard at:          10.6.101.40:39981
2025-09-05 09:11:31,877 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,877 - distributed.worker - INFO -          dashboard at:          10.6.101.40:39503
2025-09-05 09:11:31,877 - distributed.worker - INFO -          dashboard at:          10.6.101.40:44065
2025-09-05 09:11:31,877 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,877 - distributed.worker - INFO -          dashboard at:          10.6.101.40:39205
2025-09-05 09:11:31,877 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,877 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,877 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,877 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,877 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,877 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,877 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,877 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,877 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,877 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,877 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,877 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,877 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,877 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,877 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,877 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,877 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,877 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,877 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,877 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,877 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,877 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,877 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,877 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,877 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-5tddbqbe
2025-09-05 09:11:31,877 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,877 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,877 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-9nbelmer
2025-09-05 09:11:31,877 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,877 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,877 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-e300t4i3
2025-09-05 09:11:31,877 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,877 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,877 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-4rrbyagw
2025-09-05 09:11:31,877 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-b_ftqgrk
2025-09-05 09:11:31,877 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,877 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-6qdemdqx
2025-09-05 09:11:31,877 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-x4b2m_2e
2025-09-05 09:11:31,877 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,877 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-snen85oq
2025-09-05 09:11:31,877 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,877 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,877 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,877 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,877 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,880 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:39587
2025-09-05 09:11:31,880 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:39587
2025-09-05 09:11:31,880 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:42653
2025-09-05 09:11:31,880 - distributed.worker - INFO -          dashboard at:          10.6.101.40:37119
2025-09-05 09:11:31,880 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,880 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:42653
2025-09-05 09:11:31,880 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,880 - distributed.worker - INFO -          dashboard at:          10.6.101.40:44665
2025-09-05 09:11:31,880 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,880 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,880 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,880 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,880 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,880 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-91vlq7t9
2025-09-05 09:11:31,880 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,880 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,880 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-n13fn5zi
2025-09-05 09:11:31,880 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,881 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:33925
2025-09-05 09:11:31,881 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:33925
2025-09-05 09:11:31,881 - distributed.worker - INFO -          dashboard at:          10.6.101.40:45697
2025-09-05 09:11:31,881 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,881 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,881 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,881 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,881 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-j2fpsrs8
2025-09-05 09:11:31,881 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,882 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:36155
2025-09-05 09:11:31,882 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:36155
2025-09-05 09:11:31,882 - distributed.worker - INFO -          dashboard at:          10.6.101.40:41255
2025-09-05 09:11:31,882 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,882 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,882 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,882 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,882 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-qp6ycr38
2025-09-05 09:11:31,882 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,882 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:34575
2025-09-05 09:11:31,882 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:34575
2025-09-05 09:11:31,882 - distributed.worker - INFO -          dashboard at:          10.6.101.40:35189
2025-09-05 09:11:31,883 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,883 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,883 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,883 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,883 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-0zgr5o5i
2025-09-05 09:11:31,883 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,886 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:39075
2025-09-05 09:11:31,887 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:39075
2025-09-05 09:11:31,887 - distributed.worker - INFO -          dashboard at:          10.6.101.40:36983
2025-09-05 09:11:31,887 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,887 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,887 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,887 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,887 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-776gkiv8
2025-09-05 09:11:31,887 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,887 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:39947
2025-09-05 09:11:31,887 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:39947
2025-09-05 09:11:31,887 - distributed.worker - INFO -          dashboard at:          10.6.101.40:33633
2025-09-05 09:11:31,887 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,887 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,887 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,887 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,887 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-zwrmeju_
2025-09-05 09:11:31,887 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,887 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:34189
2025-09-05 09:11:31,887 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:34189
2025-09-05 09:11:31,887 - distributed.worker - INFO -          dashboard at:          10.6.101.40:40383
2025-09-05 09:11:31,887 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,887 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,888 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,888 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,888 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-adxve6_r
2025-09-05 09:11:31,888 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:43237
2025-09-05 09:11:31,888 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,888 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:43237
2025-09-05 09:11:31,888 - distributed.worker - INFO -          dashboard at:          10.6.101.40:46383
2025-09-05 09:11:31,888 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,888 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,888 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,888 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,888 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-d0cbxk68
2025-09-05 09:11:31,888 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,890 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:45399
2025-09-05 09:11:31,890 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:45399
2025-09-05 09:11:31,890 - distributed.worker - INFO -          dashboard at:          10.6.101.40:35839
2025-09-05 09:11:31,890 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,890 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,890 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,890 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,891 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-cyzidt6r
2025-09-05 09:11:31,891 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,892 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:44985
2025-09-05 09:11:31,892 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:44985
2025-09-05 09:11:31,893 - distributed.worker - INFO -          dashboard at:          10.6.101.40:35651
2025-09-05 09:11:31,893 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,893 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,893 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,893 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,893 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-edl9pczx
2025-09-05 09:11:31,893 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,898 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,898 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,898 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,899 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,899 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,899 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,899 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,900 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,900 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,901 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,901 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,901 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,902 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,902 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,902 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,903 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,903 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,904 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,904 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,905 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,905 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,906 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,906 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,907 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,907 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,908 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,908 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:41709
2025-09-05 09:11:31,908 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,908 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:41709
2025-09-05 09:11:31,908 - distributed.worker - INFO -          dashboard at:          10.6.101.40:34415
2025-09-05 09:11:31,908 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,908 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,908 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,908 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,908 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,908 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-vp_m8cdp
2025-09-05 09:11:31,908 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,909 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,909 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,909 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,910 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,910 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,910 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,911 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,911 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,911 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,912 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,912 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,913 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,913 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,913 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,914 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,914 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,914 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,914 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,915 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,916 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,916 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,916 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,916 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,916 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,917 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,917 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,917 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,918 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,918 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,919 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,919 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,919 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,920 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,920 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,920 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,920 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,921 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,921 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,921 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,922 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,922 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,922 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,922 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,923 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,923 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,923 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,924 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,925 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,940 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,941 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,942 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,943 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:31,951 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:40229
2025-09-05 09:11:31,952 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:40229
2025-09-05 09:11:31,952 - distributed.worker - INFO -          dashboard at:          10.6.101.40:46213
2025-09-05 09:11:31,952 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,952 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,952 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,952 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,952 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-_v9hpthj
2025-09-05 09:11:31,952 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,993 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:31,993 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:40325
2025-09-05 09:11:31,993 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:40325
2025-09-05 09:11:31,993 - distributed.worker - INFO -          dashboard at:          10.6.101.40:39441
2025-09-05 09:11:31,993 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,993 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,993 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,993 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,993 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-mzsfwkk3
2025-09-05 09:11:31,993 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,994 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,994 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,995 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:37043
2025-09-05 09:11:31,995 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:37043
2025-09-05 09:11:31,995 - distributed.worker - INFO -          dashboard at:          10.6.101.40:43887
2025-09-05 09:11:31,995 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,995 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,995 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,995 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,995 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-56meh4i7
2025-09-05 09:11:31,995 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,995 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:38439
2025-09-05 09:11:31,995 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:38439
2025-09-05 09:11:31,995 - distributed.worker - INFO -          dashboard at:          10.6.101.40:37429
2025-09-05 09:11:31,995 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:31,995 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,995 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:31,995 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:31,995 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-36p_d56v
2025-09-05 09:11:31,996 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:31,996 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,015 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:40619
2025-09-05 09:11:32,015 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:40619
2025-09-05 09:11:32,015 - distributed.worker - INFO -          dashboard at:          10.6.101.40:38437
2025-09-05 09:11:32,015 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,016 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,016 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:32,016 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:32,016 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-42x1_s3w
2025-09-05 09:11:32,016 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,023 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,023 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:43935
2025-09-05 09:11:32,023 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:43935
2025-09-05 09:11:32,023 - distributed.worker - INFO -          dashboard at:          10.6.101.40:43055
2025-09-05 09:11:32,023 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,023 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,023 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:32,023 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:32,023 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-4aak590s
2025-09-05 09:11:32,023 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,023 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,024 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,025 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,026 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,026 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,026 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,027 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,028 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,029 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,029 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,030 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,031 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:35777
2025-09-05 09:11:32,031 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:35777
2025-09-05 09:11:32,031 - distributed.worker - INFO -          dashboard at:          10.6.101.40:32979
2025-09-05 09:11:32,031 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,031 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,031 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:32,031 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:32,031 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-uov1r27a
2025-09-05 09:11:32,032 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,033 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:33007
2025-09-05 09:11:32,033 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:33007
2025-09-05 09:11:32,033 - distributed.worker - INFO -          dashboard at:          10.6.101.40:38327
2025-09-05 09:11:32,033 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,033 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,033 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:32,033 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:32,033 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-qq99o33f
2025-09-05 09:11:32,033 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,038 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,039 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,039 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,040 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,042 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,042 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,042 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,043 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,046 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:44465
2025-09-05 09:11:32,046 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:44465
2025-09-05 09:11:32,046 - distributed.worker - INFO -          dashboard at:          10.6.101.40:40989
2025-09-05 09:11:32,046 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,046 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,046 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:32,046 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:32,046 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-tevgquaw
2025-09-05 09:11:32,046 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,047 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,048 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,048 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,050 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,054 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,055 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,055 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,057 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,070 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,071 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,072 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,073 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,089 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:33403
2025-09-05 09:11:32,089 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:33403
2025-09-05 09:11:32,089 - distributed.worker - INFO -          dashboard at:          10.6.101.40:45055
2025-09-05 09:11:32,089 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,089 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,089 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:32,089 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:32,089 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-8sfiri_5
2025-09-05 09:11:32,089 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,097 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:41729
2025-09-05 09:11:32,097 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:41729
2025-09-05 09:11:32,097 - distributed.worker - INFO -          dashboard at:          10.6.101.40:43885
2025-09-05 09:11:32,097 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,097 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,097 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:32,097 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:32,097 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-bbuj4q74
2025-09-05 09:11:32,097 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,109 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,110 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,110 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,111 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,112 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:35417
2025-09-05 09:11:32,112 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:35417
2025-09-05 09:11:32,112 - distributed.worker - INFO -          dashboard at:          10.6.101.40:37697
2025-09-05 09:11:32,112 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,112 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,112 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:32,112 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:32,112 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-9uuah1y_
2025-09-05 09:11:32,112 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,113 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:38947
2025-09-05 09:11:32,113 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:38947
2025-09-05 09:11:32,113 - distributed.worker - INFO -          dashboard at:          10.6.101.40:37981
2025-09-05 09:11:32,113 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,114 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,114 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:32,114 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:32,114 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-e4umv2po
2025-09-05 09:11:32,114 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,119 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,119 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,120 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,121 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,129 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:41013
2025-09-05 09:11:32,129 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:41013
2025-09-05 09:11:32,129 - distributed.worker - INFO -          dashboard at:          10.6.101.40:43967
2025-09-05 09:11:32,129 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,129 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,129 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:32,129 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:32,129 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-b1t9oxhz
2025-09-05 09:11:32,129 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,130 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:37611
2025-09-05 09:11:32,131 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:37611
2025-09-05 09:11:32,131 - distributed.worker - INFO -          dashboard at:          10.6.101.40:43423
2025-09-05 09:11:32,131 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,131 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,131 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:32,131 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:32,131 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-h_an8fwm
2025-09-05 09:11:32,131 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,134 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,135 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,136 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,137 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,138 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,139 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,139 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,141 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,154 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,155 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,155 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,157 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,157 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,158 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,158 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,160 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,171 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:40039
2025-09-05 09:11:32,171 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:40039
2025-09-05 09:11:32,171 - distributed.worker - INFO -          dashboard at:          10.6.101.40:34881
2025-09-05 09:11:32,171 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,171 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,171 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:32,171 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:32,172 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-ixf_akr0
2025-09-05 09:11:32,172 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,194 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,194 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,195 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,196 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,290 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:46567
2025-09-05 09:11:32,290 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:46567
2025-09-05 09:11:32,290 - distributed.worker - INFO -          dashboard at:          10.6.101.40:38981
2025-09-05 09:11:32,290 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,290 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,290 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:32,290 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:32,290 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-m2wkjrjy
2025-09-05 09:11:32,290 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,290 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:35153
2025-09-05 09:11:32,291 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:35153
2025-09-05 09:11:32,291 - distributed.worker - INFO -          dashboard at:          10.6.101.40:46739
2025-09-05 09:11:32,291 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,291 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,291 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:32,291 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:32,291 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-5fxir1ut
2025-09-05 09:11:32,291 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,297 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:36295
2025-09-05 09:11:32,297 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:36295
2025-09-05 09:11:32,297 - distributed.worker - INFO -          dashboard at:          10.6.101.40:43937
2025-09-05 09:11:32,297 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,297 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,297 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:32,297 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:32,297 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-nsoscslw
2025-09-05 09:11:32,297 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,297 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:40267
2025-09-05 09:11:32,298 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:40267
2025-09-05 09:11:32,298 - distributed.worker - INFO -          dashboard at:          10.6.101.40:45445
2025-09-05 09:11:32,298 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,298 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,298 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:32,298 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:32,298 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-7a0wz9k2
2025-09-05 09:11:32,298 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,298 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:45371
2025-09-05 09:11:32,299 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:45371
2025-09-05 09:11:32,299 - distributed.worker - INFO -          dashboard at:          10.6.101.40:42243
2025-09-05 09:11:32,299 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,299 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,299 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:32,299 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:32,299 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-jvf6y4g8
2025-09-05 09:11:32,299 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,302 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,303 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,303 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,303 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,308 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,308 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,308 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:35275
2025-09-05 09:11:32,308 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:35275
2025-09-05 09:11:32,308 - distributed.worker - INFO -          dashboard at:          10.6.101.40:33733
2025-09-05 09:11:32,308 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,308 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:32,308 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:32,308 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-w02oydfn
2025-09-05 09:11:32,308 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,308 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,312 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,312 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,313 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,314 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,319 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,320 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,320 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,320 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:45585
2025-09-05 09:11:32,320 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:45585
2025-09-05 09:11:32,320 - distributed.worker - INFO -          dashboard at:          10.6.101.40:35077
2025-09-05 09:11:32,321 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,321 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,321 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:32,321 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:32,321 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-rsr3y56o
2025-09-05 09:11:32,321 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,321 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,322 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,323 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:41707
2025-09-05 09:11:32,323 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:41707
2025-09-05 09:11:32,323 - distributed.worker - INFO -          dashboard at:          10.6.101.40:44331
2025-09-05 09:11:32,323 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,323 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,323 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:32,323 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:32,323 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-6qvjartg
2025-09-05 09:11:32,323 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,323 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:46409
2025-09-05 09:11:32,323 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,324 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:46409
2025-09-05 09:11:32,324 - distributed.worker - INFO -          dashboard at:          10.6.101.40:38551
2025-09-05 09:11:32,324 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,324 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,324 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,324 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:32,324 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:32,324 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-ys93amqt
2025-09-05 09:11:32,324 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,325 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,331 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:41109
2025-09-05 09:11:32,331 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:41109
2025-09-05 09:11:32,331 - distributed.worker - INFO -          dashboard at:          10.6.101.40:41299
2025-09-05 09:11:32,331 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,331 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:36317
2025-09-05 09:11:32,331 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,331 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:32,331 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:36317
2025-09-05 09:11:32,331 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:32,331 - distributed.worker - INFO -          dashboard at:          10.6.101.40:38505
2025-09-05 09:11:32,331 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-zjp_ln1h
2025-09-05 09:11:32,331 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,331 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,331 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,331 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:32,331 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:32,331 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-xt8r4old
2025-09-05 09:11:32,331 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,336 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:38707
2025-09-05 09:11:32,337 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:38707
2025-09-05 09:11:32,337 - distributed.worker - INFO -          dashboard at:          10.6.101.40:45363
2025-09-05 09:11:32,337 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,337 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,337 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:32,337 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:32,337 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-5ww82wp3
2025-09-05 09:11:32,337 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,343 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:34003
2025-09-05 09:11:32,343 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:34003
2025-09-05 09:11:32,343 - distributed.worker - INFO -          dashboard at:          10.6.101.40:35589
2025-09-05 09:11:32,343 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,343 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,343 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:32,343 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:32,343 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-viy7c4o4
2025-09-05 09:11:32,343 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,344 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:43351
2025-09-05 09:11:32,344 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:43351
2025-09-05 09:11:32,344 - distributed.worker - INFO -          dashboard at:          10.6.101.40:42049
2025-09-05 09:11:32,344 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,344 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,344 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:32,344 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:32,344 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-0angjva3
2025-09-05 09:11:32,344 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,350 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:39757
2025-09-05 09:11:32,350 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:39757
2025-09-05 09:11:32,350 - distributed.worker - INFO -          dashboard at:          10.6.101.40:36439
2025-09-05 09:11:32,350 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,350 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,350 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:32,350 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:32,350 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-fs9__0tu
2025-09-05 09:11:32,351 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,359 - distributed.worker - INFO -       Start worker at:    tcp://10.6.101.40:33697
2025-09-05 09:11:32,360 - distributed.worker - INFO -          Listening to:    tcp://10.6.101.40:33697
2025-09-05 09:11:32,360 - distributed.worker - INFO -          dashboard at:          10.6.101.40:42861
2025-09-05 09:11:32,360 - distributed.worker - INFO - Waiting to connect to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,360 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,360 - distributed.worker - INFO -               Threads:                          2
2025-09-05 09:11:32,360 - distributed.worker - INFO -                Memory:                   9.62 GiB
2025-09-05 09:11:32,360 - distributed.worker - INFO -       Local Directory: /jobfs/148852475.gadi-pbs/dask-scratch-space/worker-9extf4mu
2025-09-05 09:11:32,360 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,400 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,401 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,401 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,403 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,405 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,406 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,406 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,406 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,407 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,407 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,408 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,408 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,408 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,409 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,409 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,410 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,410 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,410 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,410 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,411 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,411 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,411 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,412 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,413 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,413 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,413 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,413 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,414 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,414 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,414 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,415 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,416 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,416 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,416 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,416 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,417 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,417 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-09-05 09:11:32,417 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,418 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,418 - distributed.worker - INFO -         Registered to:     tcp://10.6.101.37:8753
2025-09-05 09:11:32,419 - distributed.worker - INFO - -------------------------------------------------
2025-09-05 09:11:32,419 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:11:32,420 - distributed.core - INFO - Starting established connection to tcp://10.6.101.37:8753
2025-09-05 09:12:06,593 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,594 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,594 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,594 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,594 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,594 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,594 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,594 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,595 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,595 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,595 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,595 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,595 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,595 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,595 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,596 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,596 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,596 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,596 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,596 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,596 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,596 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,596 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,596 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,597 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,597 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,597 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,597 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,597 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,597 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,597 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,597 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,597 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,597 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,597 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,597 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,597 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,597 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,598 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,598 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,598 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,598 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,598 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,598 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,596 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,598 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,598 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,598 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,598 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,598 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,599 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,598 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,599 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,599 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,599 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,599 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,599 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,599 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,599 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,600 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,600 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,600 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,598 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,598 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,600 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,601 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,599 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,601 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,599 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,601 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,601 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,601 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,601 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,602 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,600 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,602 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,600 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,600 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,600 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,601 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,602 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,602 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,602 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,602 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,602 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,604 - distributed.worker - INFO - Starting Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:12:06,607 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,609 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,610 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,610 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,611 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,611 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,611 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,612 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,612 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,612 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,614 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,614 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,614 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,615 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,616 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,613 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,615 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:06,624 - distributed.utils - INFO - Reload module qme_utils from .py file
2025-09-05 09:12:09,393 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,393 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,393 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,393 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,393 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,394 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,394 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,394 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,394 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,394 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,395 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,395 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,393 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,394 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,395 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,394 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,396 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,396 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,396 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,396 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,395 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,395 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,396 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,395 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,396 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,395 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,397 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,397 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,397 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,397 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,396 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,395 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,397 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,397 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,396 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,396 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,397 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,397 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,397 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,396 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,395 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,398 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,397 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,398 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,397 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,395 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,397 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,398 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,398 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,398 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,397 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,398 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,398 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,399 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,398 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,399 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,399 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,398 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,398 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,399 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,397 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,398 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,399 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,398 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,399 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,398 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,399 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,399 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,399 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,398 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,398 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,399 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,400 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,400 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,400 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,400 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,398 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,401 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,399 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,402 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,398 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,400 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,400 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,402 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,398 - distributed.worker - INFO - Starting Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:12:09,402 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,401 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,402 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,401 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,402 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,401 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,402 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,401 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,402 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,403 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,403 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,403 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,403 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,403 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,403 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,404 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,404 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,404 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,404 - distributed.utils - INFO - Reload module qme_vars from .py file
2025-09-05 09:12:09,772 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,772 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,773 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,773 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,773 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,773 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,773 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,773 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,773 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,773 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,773 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,774 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,774 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,774 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,774 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,774 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,774 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,774 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,774 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,774 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,774 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,774 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,775 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,775 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,775 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,775 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,775 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,775 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,775 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,775 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,776 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,776 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,776 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,776 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,776 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,776 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,776 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,776 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,776 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,776 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,776 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,777 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,777 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,777 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,777 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,777 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,777 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,777 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,777 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,777 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,777 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,777 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,777 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,778 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,778 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,778 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,778 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,778 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,778 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,778 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,778 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,778 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,778 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,778 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,779 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,779 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,779 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,779 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,779 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,779 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,779 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,779 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,780 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,780 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,780 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,780 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,780 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,780 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,780 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,780 - distributed.worker - INFO - Starting Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:12:09,780 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,780 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,780 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,781 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,781 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,781 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,781 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,781 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,781 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,781 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,781 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,781 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,781 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,781 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,781 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,781 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,781 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,782 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,782 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,782 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,782 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,782 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,783 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:09,785 - distributed.utils - INFO - Reload module qme_train from .py file
2025-09-05 09:12:10,234 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,234 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,234 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,234 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,234 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,234 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,234 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,234 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,234 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,235 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,235 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,235 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,235 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,235 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,235 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,235 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,235 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,235 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,236 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,236 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,236 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,236 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,236 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,236 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,236 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,236 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,236 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,236 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,236 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,236 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,236 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,236 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,236 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,237 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,237 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,237 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,237 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,237 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,237 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,237 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,237 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,237 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,237 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,237 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,237 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,237 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,237 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,237 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,237 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,237 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,237 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,237 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,238 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,238 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,238 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,238 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,238 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,238 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,238 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,238 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,238 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,238 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,238 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,238 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,238 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,238 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,238 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,238 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,239 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,239 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,239 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,239 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,239 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,239 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,239 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,239 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,239 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,239 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,239 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,239 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,239 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,239 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,239 - distributed.worker - INFO - Starting Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:12:10,240 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,240 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,240 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,240 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,241 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,241 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,241 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,241 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,241 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,241 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,241 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,241 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,242 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,242 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,242 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,242 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,242 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,243 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,243 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,243 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:12:10,252 - distributed.utils - INFO - Reload module qme_apply from .py file
2025-09-05 09:16:17,348 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,348 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,349 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,354 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,415 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,427 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,427 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,428 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,428 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,437 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,438 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,438 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,438 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,439 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,443 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,445 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,456 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,462 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,470 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,490 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:17,555 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:48,725 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:49,191 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:49,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:50,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:50,883 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,446 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,446 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,855 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:52,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:55,071 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:55,840 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:55,843 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:55,846 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:56,260 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:57,498 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,302 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,303 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,305 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:58,829 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:59,046 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:59,652 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:16:59,657 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:00,059 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:00,368 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:00,370 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:01,126 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:01,783 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:02,484 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:02,689 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:02,697 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:02,705 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:03,025 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:04,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:05,149 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:05,150 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:08,395 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:08,503 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:10,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:10,249 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:11,411 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:11,425 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:11,427 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:13,763 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:14,711 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:14,960 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:14,961 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:14,962 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,120 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,124 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,384 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,454 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,457 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,546 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,985 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:15,993 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,050 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,111 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,201 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,205 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,205 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,352 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:16,651 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,204 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,420 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,422 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:17,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:18,166 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:20,545 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:20,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:21,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:22,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:22,906 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:23,395 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:25,145 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:25,228 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:29,764 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:30,289 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:30,292 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:30,295 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:30,296 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:30,465 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:30,845 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:31,589 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,384 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,399 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,785 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,786 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:34,787 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,178 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,890 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,892 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,910 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:35,916 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,450 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:36,940 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:37,434 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,126 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,127 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,128 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,167 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,560 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:38,563 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:39,709 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:39,716 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:39,720 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:42,461 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:42,879 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:42,881 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:42,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:44,021 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:44,021 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:47,224 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:50,366 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:50,878 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:51,662 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:51,739 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,404 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:52,593 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,266 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,271 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:53,415 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,125 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,128 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,347 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,445 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,448 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,553 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,557 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:54,801 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:56,270 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:56,401 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:56,434 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:56,587 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:57,900 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:57,901 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:57,914 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:57,920 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:57,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:58,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:17:58,340 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:00,817 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:00,817 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:01,629 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:01,952 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:06,477 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:09,484 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:14,401 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:14,406 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:14,410 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:14,560 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,426 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,764 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,987 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,988 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:15,994 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,179 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,181 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,481 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,532 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,653 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,709 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:16,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,287 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,329 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,379 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,741 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,742 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,825 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:17,950 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:18,175 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:18,176 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,019 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,028 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,383 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,432 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,467 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,589 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,806 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,992 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:33,998 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,318 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,408 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,573 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,631 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,701 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,702 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:34,702 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,367 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,752 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:35,759 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,272 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,572 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:36,613 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,163 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:37,226 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:39,092 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:42,227 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:42,831 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,782 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:44,854 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:45,200 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,785 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,789 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,997 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:46,997 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,254 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,338 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,822 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,823 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,824 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:47,887 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,171 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,320 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,446 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,511 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,666 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,668 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,668 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,672 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,768 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,777 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,781 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:48,827 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:49,155 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:50,444 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:51,246 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:52,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:52,585 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:52,587 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:54,237 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:54,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:18:59,664 - distributed.core - INFO - Event loop was unresponsive in Worker for 20.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:06,500 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:07,182 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:07,617 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:08,027 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:08,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:08,512 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:08,516 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:08,519 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,314 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,387 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:09,582 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:10,128 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:12,768 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:12,769 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:12,771 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:12,858 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:12,990 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:12,994 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,005 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,008 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,009 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,014 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,386 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,386 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:13,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,599 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,599 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,600 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,944 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,946 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:14,952 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:15,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:15,893 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:19,549 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:20,315 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:20,777 - distributed.core - INFO - Event loop was unresponsive in Worker for 16.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:31,183 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:31,183 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:31,185 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:31,186 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:31,196 - distributed.core - INFO - Event loop was unresponsive in Worker for 27.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:34,322 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:34,912 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:36,359 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:36,775 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:36,776 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:36,778 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:37,505 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:37,688 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,461 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,472 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,568 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,568 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,586 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:38,731 - distributed.core - INFO - Event loop was unresponsive in Worker for 7.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,065 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:39,267 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,112 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,271 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,465 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:40,951 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,039 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,040 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,045 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:41,385 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,010 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,130 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,273 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:42,323 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,173 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,520 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,524 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,525 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,525 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,533 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,566 - distributed.core - INFO - Event loop was unresponsive in Worker for 12.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,708 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,723 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:43,913 - distributed.core - INFO - Event loop was unresponsive in Worker for 11.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:46,444 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:46,540 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:47,154 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:47,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,627 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,635 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,690 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,690 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:48,772 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,952 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:49,954 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,020 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,023 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,343 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,346 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,348 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:50,351 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:51,720 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:51,726 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:51,758 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:51,774 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:51,793 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:51,998 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:53,625 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:53,864 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:53,903 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:19:55,468 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:00,372 - distributed.core - INFO - Event loop was unresponsive in Worker for 13.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:03,390 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:04,418 - distributed.core - INFO - Event loop was unresponsive in Worker for 17.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:04,972 - distributed.core - INFO - Event loop was unresponsive in Worker for 19.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:05,274 - distributed.core - INFO - Event loop was unresponsive in Worker for 18.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:07,496 - distributed.core - INFO - Event loop was unresponsive in Worker for 22.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:11,286 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,286 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:36295. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,286 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,286 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,286 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,286 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,286 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,286 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,286 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,289 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:45371. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,286 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,289 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:36317. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,289 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:43351. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,286 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,286 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,289 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:46409. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,289 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:45585. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,287 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,289 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:39757. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,287 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,289 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:34189. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,289 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:41109. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,287 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,290 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:38707. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,287 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,290 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:41707. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,290 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:34003. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,290 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:40267. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,290 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:35153. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,290 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:33403. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,290 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:40039. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,291 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:39587. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,291 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:41013. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,285 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.101.40:43498 remote=tcp://10.6.101.37:8753>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:20:11,291 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:42281. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,291 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:41729. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,291 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:38947. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,291 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:42653. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,286 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,291 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:35275. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,291 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:45399. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,292 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:41709. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,292 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:39075. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,288 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,292 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:37611. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,292 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:39947. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,292 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:44985. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,292 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:33925. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,292 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:40229. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,292 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:36831. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,293 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:34743. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,293 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:35417. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,293 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:37407. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,293 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:43237. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,293 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,285 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.101.40:43514 remote=tcp://10.6.101.37:8753>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:20:11,294 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:33697. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,285 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.101.40:43550 remote=tcp://10.6.101.37:8753>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:20:11,285 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.101.40:43532 remote=tcp://10.6.101.37:8753>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:20:11,285 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.101.40:43534 remote=tcp://10.6.101.37:8753>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:20:11,294 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:38177. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,294 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:38439. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,285 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.101.40:43518 remote=tcp://10.6.101.37:8753>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:20:11,294 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:43135'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,285 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.101.40:43510 remote=tcp://10.6.101.37:8753>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:20:11,285 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 861, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1113, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1259, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1018, in send_recv
    response = await comm.read(deserializers=deserializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.6.101.40:43496 remote=tcp://10.6.101.37:8753>: ConnectionResetError: [Errno 104] Connection reset by peer
2025-09-05 09:20:11,296 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2901, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,296 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,295 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,295 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,295 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,295 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,297 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:44465. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,297 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:40619. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,297 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:33007. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,295 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,297 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:43935. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,297 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,298 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:35777. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,295 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,296 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:11,298 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:37043. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,298 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:40325. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,304 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:40377'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,305 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:33653'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,305 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:41663'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,305 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1923, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,306 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:40101'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,306 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 876, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,306 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 2533, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,306 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,306 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:43941'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,307 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:36759'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,307 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2907, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,307 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1236, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,307 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2918, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,307 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,307 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2908, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,307 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:38379'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,308 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:41599'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,308 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:42355'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,308 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,308 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2235, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,308 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1727, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,309 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:38159'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,308 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.37:32841, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,309 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2246, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,309 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1734, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,309 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3779, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,309 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 591, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,309 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,309 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1632, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,309 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 7954, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,310 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1505, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,310 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1924, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,310 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2316, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,310 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,311 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,312 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,320 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:42377'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,321 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:46221'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,321 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:32803'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,321 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:36105'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,321 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:36269'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,322 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:38977'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,322 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2827, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,322 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:37859'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,322 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2098, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,322 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 2607, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,322 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:34513'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,322 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1766, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,322 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3679, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,322 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:43239'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,322 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1783, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,323 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,323 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:36875'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,323 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 6249, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,323 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 8595, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,323 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,323 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:37917'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,323 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3523, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,323 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,323 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,323 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:40333'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,323 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,323 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1147, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,323 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,323 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,323 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2317, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,323 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:39043'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,323 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,323 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,323 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 6136, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,323 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,324 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 5446, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,324 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 5423, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,324 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:35201'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,324 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3522, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,324 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2779, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,324 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:42825'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,324 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 8333, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,324 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2633, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,324 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:45183'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,324 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 8327, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,324 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,325 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:36995'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,325 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2821, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,325 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:40017'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,325 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.37:33007, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,325 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1467, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,325 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.41:35887, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,325 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1380, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,325 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:45187'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,325 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.38:46577, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,325 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,325 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1898, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,326 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.44:37189, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,326 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3101, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,326 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.55:34755, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,326 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:43615'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,326 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,326 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,326 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,326 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,326 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1921, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,326 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 4114, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,326 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,326 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,326 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.46:37477, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,326 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3337, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,326 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,326 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,326 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:36309'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,326 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 4039, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,326 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2234, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,326 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.55:45653, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,326 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,326 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,326 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,326 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,327 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:37537'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,327 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2790, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,327 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.39:34503, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,327 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.48:35429, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,327 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:43897'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,327 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1767, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,327 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1138, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,327 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1188, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,327 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:42615'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,327 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.56:40647, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,327 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,327 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1364, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,328 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:41917'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,328 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1360, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,328 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:39221'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,328 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 6998, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,328 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2321, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,328 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:45361'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,328 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2075, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,328 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2523, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,328 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1180, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,329 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2078, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,328 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,329 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:45711'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,329 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 86, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,329 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,329 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1784, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,330 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 5511, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,330 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,330 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1479, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,331 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,331 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,331 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,331 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,331 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,332 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,332 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 8588, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,332 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,332 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 8332, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,333 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,339 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,340 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:40387'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,340 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,340 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:39219'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,340 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,340 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:36651'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,340 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,341 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:35721'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,341 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,341 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:39681'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,341 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:38965'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,341 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:33829'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:11,341 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1958, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,341 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1781, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,341 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 1761, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,342 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2315, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,342 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 5468, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,342 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2117, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,342 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,342 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 5503, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,342 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-91ba88a1e7a9909410f2cbe9c4a420de', 1383, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,342 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.37:44793, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,342 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,342 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,342 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,342 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 4127, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,343 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,343 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='gather_dep(tcp://10.6.101.41:36871, {...})' coro=<Worker.gather_dep() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,343 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,343 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,343 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,343 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,343 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,343 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 865, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,343 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,343 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,343 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,343 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,343 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,343 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,343 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3842, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,343 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,343 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,343 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,343 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,343 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 2270, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:11,343 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,343 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,344 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,344 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,344 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,344 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,344 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,344 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:11,344 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,344 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,345 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,345 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:11,345 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:11,345 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:11,345 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:11,332 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14a9fc3cdd10>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:11,352 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:11,498 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:11,800 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:11,943 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,359 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:12,361 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,361 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:34695. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:12,372 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:12,378 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.40:42818 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:12,383 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,384 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:35075'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:12,385 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:12,385 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:12,385 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:12,385 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:12,385 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:12,389 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14a52ddc5210>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:12,397 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,438 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,499 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:12,499 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:44565. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:12,501 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,501 - distributed.worker - ERROR - failed during get data with tcp://10.6.101.40:44565 -> tcp://10.6.101.63:35721
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 962, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/iostream.py", line 1121, in write_to_fd
    return self.socket.send(data)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1795, in get_data
    response = await comm.read(deserializers=serializers)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://10.6.101.40:44565 remote=tcp://10.6.101.63:49054>: BrokenPipeError: [Errno 32] Broken pipe
2025-09-05 09:20:12,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:12,506 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.40:42810 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:12,508 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:42725'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:12,508 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('mean_chunk-4bd2b8670bc0f309eefca402291e2c5c', 3886, 0, 0))" coro=<Worker.execute() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker_state_machine.py:3607>> ended with CancelledError
2025-09-05 09:20:12,508 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:12,508 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:12,508 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:12,509 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:12,509 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:12,512 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1548b50df710>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:12,516 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:12,538 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:13,355 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:13,502 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:13,804 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:13,895 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:40333'. Reason: nanny-close-gracefully
2025-09-05 09:20:13,897 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:40333' closed.
2025-09-05 09:20:13,907 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:38965'. Reason: nanny-close-gracefully
2025-09-05 09:20:13,908 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:38965' closed.
2025-09-05 09:20:13,908 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:13,948 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,164 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:14,167 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:34575. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:14,168 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:14,177 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:14,183 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.40:42878 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:14,187 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:40049'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:14,188 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:14,189 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:14,189 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:14,189 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:14,189 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:14,193 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14e5356aebd0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:14,197 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:42825'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,198 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:14,199 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:42825' closed.
2025-09-05 09:20:14,321 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:36269'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,322 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:36269' closed.
2025-09-05 09:20:14,365 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,388 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,400 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,442 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,505 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,518 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,542 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:14,757 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:40377'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,759 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:40377' closed.
2025-09-05 09:20:14,775 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:36309'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,776 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:36309' closed.
2025-09-05 09:20:14,819 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:35075'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,821 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:35075' closed.
2025-09-05 09:20:14,881 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:45361'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,882 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:45361' closed.
2025-09-05 09:20:14,890 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:42725'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,890 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:14,891 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:42725' closed.
2025-09-05 09:20:14,899 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:14,900 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:34127. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:14,910 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:14,916 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.40:42834 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:14,920 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:36339'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:14,921 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:14,921 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:14,922 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:14,922 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:14,922 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:14,924 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x1552fba64090>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:14,930 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:14,938 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:37917'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,943 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:37917' closed.
2025-09-05 09:20:14,976 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:42615'. Reason: nanny-close-gracefully
2025-09-05 09:20:14,983 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:42615' closed.
2025-09-05 09:20:15,133 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,135 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,246 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,542 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:15,912 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:16,173 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:16,201 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:16,265 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,297 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:36875'. Reason: nanny-close-gracefully
2025-09-05 09:20:16,298 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:36875' closed.
2025-09-05 09:20:16,397 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,614 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:40049'. Reason: nanny-close-gracefully
2025-09-05 09:20:16,615 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:40049' closed.
2025-09-05 09:20:16,635 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:42377'. Reason: nanny-close-gracefully
2025-09-05 09:20:16,636 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:42377' closed.
2025-09-05 09:20:16,738 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,889 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,890 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,890 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,891 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:16,894 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:16,933 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,065 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,066 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,137 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,139 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,250 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,286 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:45183'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,287 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:45183' closed.
2025-09-05 09:20:17,300 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:36339'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,301 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:36339' closed.
2025-09-05 09:20:17,516 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:43239'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,517 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:43239' closed.
2025-09-05 09:20:17,527 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:40017'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,528 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:40017' closed.
2025-09-05 09:20:17,546 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:17,636 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:41663'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,637 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:41663' closed.
2025-09-05 09:20:17,679 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,881 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:17,935 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:39043'. Reason: nanny-close-gracefully
2025-09-05 09:20:17,936 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:39043' closed.
2025-09-05 09:20:18,006 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,006 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,078 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,078 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,079 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,141 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,271 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,329 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,400 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,498 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,649 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:41599'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,650 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:41599' closed.
2025-09-05 09:20:18,670 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:18,742 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,757 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:38379'. Reason: nanny-close-gracefully
2025-09-05 09:20:18,759 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:38379' closed.
2025-09-05 09:20:18,893 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,894 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,894 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:18,895 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,014 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,069 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,070 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,120 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:19,123 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:36155. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,125 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:45187'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,127 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:45187' closed.
2025-09-05 09:20:19,133 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:19,139 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.40:42862 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:19,143 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:33183'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,144 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:19,144 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:19,144 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:19,144 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:19,144 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:19,146 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x152637083550>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:19,151 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,266 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:41917'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,267 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:41917' closed.
2025-09-05 09:20:19,273 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,273 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,305 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,334 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:38977'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,336 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:38977' closed.
2025-09-05 09:20:19,365 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:35201'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,366 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:35201' closed.
2025-09-05 09:20:19,381 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,381 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:35721'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,382 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:35721' closed.
2025-09-05 09:20:19,454 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:34513'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,455 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:34513' closed.
2025-09-05 09:20:19,496 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:42355'. Reason: nanny-close-gracefully
2025-09-05 09:20:19,498 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:42355' closed.
2025-09-05 09:20:19,683 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,727 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,728 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,728 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,842 - distributed.core - INFO - Connection to tcp://10.6.101.37:8753 has been closed.
2025-09-05 09:20:19,845 - distributed.worker - INFO - Stopping worker at tcp://10.6.101.40:46567. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,857 - distributed.core - INFO - Event loop was unresponsive in Worker for 10.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-09-05 09:20:19,862 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.6.101.40:43112 remote=tcp://10.6.101.37:8753>
Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
             ^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
            ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2025-09-05 09:20:19,866 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.6.101.40:33463'. Reason: worker-handle-scheduler-connection-broken
2025-09-05 09:20:19,867 - distributed.worker - INFO - Removing Worker plugin shuffle
2025-09-05 09:20:19,867 - distributed.worker - INFO - Removing Worker plugin qme_utils.py58b91349-375a-4f80-bc64-63a3345a5cc3
2025-09-05 09:20:19,867 - distributed.worker - INFO - Removing Worker plugin qme_vars.py3d9d9dae-18a4-4d2b-8fe5-6399b8b58b13
2025-09-05 09:20:19,868 - distributed.worker - INFO - Removing Worker plugin qme_train.pya33d068a-7082-4018-b81a-ff9e770713f8
2025-09-05 09:20:19,868 - distributed.worker - INFO - Removing Worker plugin qme_apply.py255bb687-8c19-490c-a9b0-654072a80cf5
2025-09-05 09:20:19,869 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils.py", line 1923, in wait_for
    return await fut
           ^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x14b7c1a35910>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1485, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1429, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1476, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/worker.py", line 1267, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 416, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/utils_comm.py", line 395, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1256, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.06/lib/python3.11/site-packages/distributed/core.py", line 1497, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-09-05 09:20:19,874 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,885 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:19,970 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:19,970 - distributed.nanny - INFO - Worker closed
2025-09-05 09:20:20,010 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,010 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,074 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:36759'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,075 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:36759' closed.
2025-09-05 09:20:20,082 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,082 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,083 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,145 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,260 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:38159'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,261 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:38159' closed.
2025-09-05 09:20:20,333 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,392 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:33829'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,394 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:33829' closed.
2025-09-05 09:20:20,421 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:40101'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,421 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:40101' closed.
2025-09-05 09:20:20,502 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,508 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:39221'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,509 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:39221' closed.
2025-09-05 09:20:20,551 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:43615'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,554 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:43615' closed.
2025-09-05 09:20:20,555 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:39219'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,558 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:39219' closed.
2025-09-05 09:20:20,596 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:36105'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,597 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:36105' closed.
2025-09-05 09:20:20,674 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:20,724 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:36651'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,725 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:36651' closed.
2025-09-05 09:20:20,900 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:40387'. Reason: nanny-close-gracefully
2025-09-05 09:20:20,901 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:40387' closed.
2025-09-05 09:20:21,018 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,067 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:43897'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,067 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:43897' closed.
2025-09-05 09:20:21,154 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,277 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,277 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,309 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,385 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,423 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:37537'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,424 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:37537' closed.
2025-09-05 09:20:21,562 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:33183'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,563 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:33183' closed.
2025-09-05 09:20:21,672 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:43941'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,673 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:43941' closed.
2025-09-05 09:20:21,732 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,732 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,738 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,745 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:43135'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,746 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:43135' closed.
2025-09-05 09:20:21,763 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:36995'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,764 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:36995' closed.
2025-09-05 09:20:21,780 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:37859'. Reason: nanny-close-gracefully
2025-09-05 09:20:21,781 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:37859' closed.
2025-09-05 09:20:21,877 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,974 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:21,974 - distributed.nanny - ERROR - Worker process died unexpectedly
2025-09-05 09:20:22,120 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:46221'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,121 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:46221' closed.
2025-09-05 09:20:22,156 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:32803'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,158 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:32803' closed.
2025-09-05 09:20:22,236 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:45711'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,237 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:45711' closed.
2025-09-05 09:20:22,273 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:33463'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,274 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:33463' closed.
2025-09-05 09:20:22,362 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:33653'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,362 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:33653' closed.
2025-09-05 09:20:22,382 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.6.101.40:39681'. Reason: nanny-close-gracefully
2025-09-05 09:20:22,382 - distributed.nanny - INFO - Nanny at 'tcp://10.6.101.40:39681' closed.
2025-09-05 09:20:22,384 - distributed.dask_worker - INFO - End worker
